{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c52b617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA is available! GPU will be used for training.\n",
      "PyTorch CUDA Version: 12.1\n",
      "GPU Name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ CUDA is available! GPU will be used for training.\")\n",
    "    print(f\"PyTorch CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"❌ CUDA not found. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ec4f5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.utils import softmax\n",
    "from rdkit import Chem\n",
    "from Bio import SeqIO\n",
    "import random\n",
    "\n",
    "# --- Configuration (UPDATE THESE PATHS & ID) ---\n",
    "CHEMPL_DB_PATH = 'DL_ENDSEM__DATASET\\chembl_35\\chembl_35_sqlite\\chembl_35.db'\n",
    "BLAST_FASTA_PATH = 'DL_ENDSEM__DATASET\\chembl_35_blast.fa.gz'\n",
    "TARGET_UNIPROT_ID = \"P00533\" # Example: EGFR Kinase - CHANGE THIS TO YOUR TARGET'S UNIPROT ID\n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "Z_DIM = 100         # Latent noise dimension\n",
    "ATOM_FEAT_DIM = 9   # Atom feature size\n",
    "BOND_FEAT_DIM = 4   # Bond feature size (Single, Double, Triple, Aromatic)\n",
    "EMBED_DIM = 128     # Hidden dimension for the Graph Transformer\n",
    "T_EMBED_DIM = 768   # Target embedding dimension (Mocked)\n",
    "LAMBDA_GP = 10.0    # Gradient Penalty weight\n",
    "MAX_NODES = 30      # Max atoms in generated molecules (for Generator tensor shape)\n",
    "N_CRITIC = 5        # Discriminator training steps per Generator step\n",
    "EPOCHS = 100        # Total epochs\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Mock Target Embedding (Replace with actual protein sequence embedding for T_EMBED_DIM)\n",
    "TARGET_EMBED = torch.randn(T_EMBED_DIM).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "084a2559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3989 potent inhibitors for UniProt ID P00533.\n",
      "Prepared 1334 real graph samples for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\AppData\\Local\\Temp\\ipykernel_944\\1119537532.py:101: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  real_loader = DataLoader(real_data_list, batch_size=BATCH_SIZE, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "def extract_potent_inhibitors(db_path, uniprot_id, potency_cutoff_nM=100):\n",
    "    \"\"\"\n",
    "    Connects to ChEMBL DB and extracts SMILES for potent inhibitors of a given Uniprot ID.\n",
    "    This version uses the most robust join by routing through target_components and\n",
    "    component_sequences, where the accession (UniProt ID) is guaranteed to be found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        # The FIX: Joining multiple tables to hit the accession column in cseq\n",
    "        sql_query = f\"\"\"\n",
    "        SELECT DISTINCT\n",
    "            cs.canonical_smiles\n",
    "        FROM\n",
    "            activities acts\n",
    "        JOIN assays a ON acts.assay_id = a.assay_id\n",
    "        JOIN target_dictionary td ON a.tid = td.tid\n",
    "        \n",
    "        -- Joining to link the target (td) to its components (tc)\n",
    "        JOIN target_components tc ON td.tid = tc.tid\n",
    "        -- Joining to get the sequence information where the accession is stored\n",
    "        JOIN component_sequences cseq ON tc.component_id = cseq.component_id\n",
    "        \n",
    "        JOIN compound_structures cs ON acts.molregno = cs.molregno\n",
    "        WHERE\n",
    "            cseq.accession = '{uniprot_id}' AND  -- **UniProt ID is stored here**\n",
    "            acts.standard_type = 'IC50' AND\n",
    "            acts.standard_units = 'nM' AND\n",
    "            acts.standard_relation = '=' AND\n",
    "            acts.standard_value <= {potency_cutoff_nM}\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(sql_query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"Found {len(df)} potent inhibitors for UniProt ID {uniprot_id}.\")\n",
    "        return df['canonical_smiles'].unique().tolist()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during database query. This is likely due to missing tables or a critical file path issue.\")\n",
    "        raise RuntimeError(f\"Database Error: {e}. Please ensure the file is the full ChEMBL SQLite dump.\") from e\n",
    "\n",
    "# --- 2.2. Protein Sequence Loading (Just for context) ---\n",
    "def load_target_sequence(fasta_path, uniprot_id):\n",
    "    \"\"\"Loads a protein sequence from a FASTA file.\"\"\"\n",
    "    for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "        if uniprot_id in record.id or uniprot_id in record.description:\n",
    "            return str(record.seq)\n",
    "    return None\n",
    "\n",
    "# --- 2.3. Graph Featurization (Double Bond Adjacency) ---\n",
    "def get_atom_features(atom):\n",
    "    \"\"\"Creates the 9-dimensional atom feature vector.\"\"\"\n",
    "    return [\n",
    "        atom.GetAtomicNum(), atom.GetDegree(), atom.GetFormalCharge(),\n",
    "        int(atom.GetIsAromatic()),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP2),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP3),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP3D),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP3D2)\n",
    "    ]\n",
    "\n",
    "def smiles_to_graph(smiles, target_embed):\n",
    "    \"\"\"Converts SMILES to a PyG Data object.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol: return None\n",
    "    if mol.GetNumAtoms() > MAX_NODES: return None\n",
    "\n",
    "    x = torch.tensor([get_atom_features(atom) for atom in mol.GetAtoms()], dtype=torch.float).to(DEVICE)\n",
    "    \n",
    "    edge_indices, edge_attrs = [], []\n",
    "    bond_types = [Chem.BondType.SINGLE, Chem.BondType.DOUBLE, Chem.BondType.TRIPLE, Chem.BondType.AROMATIC]\n",
    "    \n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        \n",
    "        # Bond Feature One-Hot Encoding (BOND_FEAT_DIM = 4)\n",
    "        bond_type_oh = [int(bond.GetBondType() == t) for t in bond_types]\n",
    "        \n",
    "        edge_indices.extend([[i, j], [j, i]])\n",
    "        edge_attrs.extend([bond_type_oh, bond_type_oh])\n",
    "\n",
    "    if not edge_indices: return None\n",
    "    \n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous().to(DEVICE)\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, \n",
    "                target_embed=target_embed.unsqueeze(0))\n",
    "    return data\n",
    "\n",
    "# --- Data Pipeline Execution ---\n",
    "inhibitor_smiles = extract_potent_inhibitors(CHEMPL_DB_PATH, TARGET_UNIPROT_ID)\n",
    "real_data_list = [smiles_to_graph(s, TARGET_EMBED) for s in inhibitor_smiles]\n",
    "real_data_list = [d for d in real_data_list if d is not None]\n",
    "\n",
    "if not real_data_list:\n",
    "    print(\"Warning: No valid inhibitor data found. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "real_loader = DataLoader(real_data_list, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(f\"Prepared {len(real_data_list)} real graph samples for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e4fc931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3989 potent inhibitors for Uniprot ID P00533.\n",
      "Prepared 1334 real graph samples for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\AppData\\Local\\Temp\\ipykernel_944\\3819942704.py:91: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  real_loader = DataLoader(real_data_list, batch_size=BATCH_SIZE, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "# --- 2.1. ChEMBL Data Extraction (FIXED SQL QUERY) ---\n",
    "def extract_potent_inhibitors(db_path, uniprot_id, potency_cutoff_nM=100):\n",
    "    \"\"\"\n",
    "    Connects to ChEMBL DB and extracts SMILES for potent inhibitors of a given Uniprot ID.\n",
    "    Uses the robust join through target_components and component_sequences to find the accession.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    # The fix: Join through target_components and component_sequences tables\n",
    "    sql_query = f\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        cs.canonical_smiles\n",
    "    FROM\n",
    "        activities acts\n",
    "    JOIN assays a ON acts.assay_id = a.assay_id\n",
    "    JOIN target_dictionary td ON a.tid = td.tid\n",
    "    JOIN target_components tc ON td.tid = tc.tid\n",
    "    JOIN component_sequences cseq ON tc.component_id = cseq.component_id\n",
    "    JOIN compound_structures cs ON acts.molregno = cs.molregno\n",
    "    WHERE\n",
    "        cseq.accession = '{uniprot_id}' AND  -- The accession is correctly found here\n",
    "        acts.standard_type = 'IC50' AND\n",
    "        acts.standard_units = 'nM' AND\n",
    "        acts.standard_relation = '=' AND\n",
    "        acts.standard_value <= {potency_cutoff_nM}\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(sql_query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Found {len(df)} potent inhibitors for Uniprot ID {uniprot_id}.\")\n",
    "    return df['canonical_smiles'].unique().tolist()\n",
    "\n",
    "# --- 2.2. Protein Sequence Loading (Just for context) ---\n",
    "def load_target_sequence(fasta_path, uniprot_id):\n",
    "    \"\"\"Loads a protein sequence from a FASTA file.\"\"\"\n",
    "    for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "        if uniprot_id in record.id or uniprot_id in record.description:\n",
    "            return str(record.seq)\n",
    "    return None\n",
    "\n",
    "# --- 2.3. Graph Featurization (Double Bond Adjacency) ---\n",
    "def get_atom_features(atom):\n",
    "    \"\"\"Creates the 9-dimensional atom feature vector.\"\"\"\n",
    "    return [\n",
    "        atom.GetAtomicNum(), atom.GetDegree(), atom.GetFormalCharge(),\n",
    "        int(atom.GetIsAromatic()),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP2),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP3),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP3D),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP3D2)\n",
    "    ]\n",
    "\n",
    "def smiles_to_graph(smiles, target_embed):\n",
    "    \"\"\"Converts SMILES to a PyG Data object.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol: return None\n",
    "    if mol.GetNumAtoms() > MAX_NODES: return None\n",
    "\n",
    "    x = torch.tensor([get_atom_features(atom) for atom in mol.GetAtoms()], dtype=torch.float).to(DEVICE)\n",
    "    \n",
    "    edge_indices, edge_attrs = [], []\n",
    "    bond_types = [Chem.BondType.SINGLE, Chem.BondType.DOUBLE, Chem.BondType.TRIPLE, Chem.BondType.AROMATIC]\n",
    "    \n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        \n",
    "        # Bond Feature One-Hot Encoding (BOND_FEAT_DIM = 4)\n",
    "        bond_type_oh = [int(bond.GetBondType() == t) for t in bond_types]\n",
    "        \n",
    "        edge_indices.extend([[i, j], [j, i]])\n",
    "        edge_attrs.extend([bond_type_oh, bond_type_oh])\n",
    "\n",
    "    if not edge_indices: return None\n",
    "    \n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous().to(DEVICE)\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, \n",
    "                target_embed=target_embed.unsqueeze(0))\n",
    "    return data\n",
    "\n",
    "# --- Data Pipeline Execution ---\n",
    "inhibitor_smiles = extract_potent_inhibitors(CHEMPL_DB_PATH, TARGET_UNIPROT_ID)\n",
    "real_data_list = [smiles_to_graph(s, TARGET_EMBED) for s in inhibitor_smiles]\n",
    "real_data_list = [d for d in real_data_list if d is not None]\n",
    "\n",
    "if not real_data_list:\n",
    "    print(\"Warning: No valid inhibitor data found. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "real_loader = DataLoader(real_data_list, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(f\"Prepared {len(real_data_list)} real graph samples for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "285eb77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.1. Relational Graph Transformer Layer (Core GNN Component) ---\n",
    "class RelationalGraphTransformerLayer(MessagePassing):\n",
    "    \"\"\"Graph Transformer Layer with explicit edge/bond feature integration.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, edge_dim, heads=4, dropout=0.1, **kwargs):\n",
    "        super().__init__(aggr='add', node_dim=0, **kwargs)\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        \n",
    "        self.lin_q = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_k = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_v = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_edge = nn.Linear(edge_dim, heads * out_channels, bias=False) \n",
    "        self.att_coeff = nn.Parameter(torch.Tensor(1, heads, 2 * out_channels)) \n",
    "        self.lin_out = nn.Linear(heads * out_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        Q = self.lin_q(x).view(-1, self.heads, self.out_channels)\n",
    "        K = self.lin_k(x).view(-1, self.heads, self.out_channels)\n",
    "        V = self.lin_v(x).view(-1, self.heads, self.out_channels)\n",
    "        E = self.lin_edge(edge_attr).view(-1, self.heads, self.out_channels)\n",
    "\n",
    "        out = self.propagate(edge_index, Q=Q, K=K, V=V, E=E)\n",
    "        out = out.view(-1, self.heads * self.out_channels)\n",
    "        out = self.lin_out(out)\n",
    "        return out\n",
    "\n",
    "    def message(self, Q_i, K_j, V_j, E_k, index):\n",
    "        \n",
    "        # 1. The FINAL Robust Guard Clause (using try-except to catch the '_empty' object)\n",
    "        try:\n",
    "            # Attempt to use the tensor method (.size) to check for emptiness\n",
    "            is_empty = (E_k.size(0) == 0)\n",
    "        except AttributeError:\n",
    "            # If AttributeError is raised, E_k is the problematic '_empty' object.\n",
    "            is_empty = True\n",
    "        \n",
    "        if is_empty:\n",
    "            # If empty (no messages/edges for this step), set the edge bias contribution to zero.\n",
    "            # We must infer the correct shape for the zero tensor using Q_i and heads.\n",
    "            E_bias = torch.zeros(Q_i.size(0), self.heads, 1, device=self.lin_q.weight.device)\n",
    "        else:\n",
    "            # If not empty, calculate the bias contribution as intended.\n",
    "            E_bias = E_k.mean(dim=-1, keepdim=True) \n",
    "\n",
    "        # Compute raw attention score e_ij: \n",
    "        QK_cat = torch.cat([Q_i, K_j], dim=-1) # (E, H, 2*out)\n",
    "        e_ij = (QK_cat * self.att_coeff).sum(dim=-1, keepdim=True) # (E, H, 1)\n",
    "\n",
    "        # 2. Add Bond Feature Bias (Use the safe E_bias)\n",
    "        e_ij = e_ij + E_bias\n",
    "        \n",
    "        e_ij = torch.nn.functional.leaky_relu(e_ij)\n",
    "        alpha = softmax(e_ij, index)\n",
    "        self.dropout(alpha)\n",
    "        \n",
    "        return V_j * alpha.view(-1, self.heads, 1)\n",
    "\n",
    "# --- 3.2. Discriminator (Graph Encoder) ---\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, node_features, edge_dim, t_embed_dim, embed_dim, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_d = node_features if i == 0 else embed_dim\n",
    "            self.layers.append(RelationalGraphTransformerLayer(in_d, embed_dim, edge_dim))\n",
    "        self.lin_final = nn.Linear(embed_dim + t_embed_dim, 1)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        t_embed = data.target_embed\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index, edge_attr)\n",
    "            x = torch.relu(x)\n",
    "        \n",
    "        graph_embed = global_mean_pool(x, batch)\n",
    "        final_input = torch.cat([graph_embed, t_embed.squeeze(1)], dim=1)\n",
    "        \n",
    "        return self.lin_final(final_input).squeeze(1)\n",
    "\n",
    "# --- 3.3. Generator (Graph Decoder - MOCK) ---\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, t_embed_dim, node_features, bond_features, max_nodes=MAX_NODES):\n",
    "        super().__init__()\n",
    "        self.max_nodes = max_nodes\n",
    "        self.node_features = node_features\n",
    "        self.bond_features = bond_features\n",
    "        \n",
    "        self.lin_x = nn.Sequential(\n",
    "            nn.Linear(z_dim + t_embed_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, max_nodes * node_features)\n",
    "        )\n",
    "        self.lin_adj = nn.Sequential(\n",
    "            nn.Linear(z_dim + t_embed_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, max_nodes * max_nodes * bond_features) \n",
    "        )\n",
    "\n",
    "    def forward(self, z, t_embed):\n",
    "        zt = torch.cat([z, t_embed], dim=1)\n",
    "        x_fake = self.lin_x(zt).view(-1, self.max_nodes, self.node_features)\n",
    "        adj_fake = self.lin_adj(zt).view(-1, self.max_nodes, self.max_nodes, self.bond_features)\n",
    "        adj_fake = torch.softmax(adj_fake, dim=-1)\n",
    "        \n",
    "        return x_fake, adj_fake\n",
    "\n",
    "# --- Model Initialization ---\n",
    "generator = Generator(Z_DIM, T_EMBED_DIM, ATOM_FEAT_DIM, BOND_FEAT_DIM).to(DEVICE)\n",
    "discriminator = Discriminator(ATOM_FEAT_DIM, BOND_FEAT_DIM, T_EMBED_DIM, EMBED_DIM).to(DEVICE)\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=1e-4)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f6fc9f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Code is complete and ready to run. Ensure your file paths are correct.\n"
     ]
    }
   ],
   "source": [
    "# --- 4.1. Mock Graph Conversion for Discriminator ---\n",
    "def convert_fake_to_data(x_fake_tensor, adj_fake_tensor, t_embed_batch, device):\n",
    "    \"\"\"Mocks the conversion of Generator tensor output into a PyG Data list for the Discriminator.\"\"\"\n",
    "    batch_size = x_fake_tensor.size(0)\n",
    "    data_list = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        num_nodes = MAX_NODES\n",
    "        x_i = x_fake_tensor[i, :, :]\n",
    "        \n",
    "        # Create full graph edge index (for simplicity in GP calculation)\n",
    "        edge_indices = [[r, c] for r in range(num_nodes) for c in range(num_nodes) if r != c]\n",
    "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous().to(device)\n",
    "        \n",
    "        # Use the most probable bond type for the edge feature\n",
    "        adj_i = adj_fake_tensor[i, :, :, :]\n",
    "        edge_attr_indices = [adj_i[r, c].argmax().item() for r, c in edge_indices]\n",
    "        edge_attr = nn.functional.one_hot(torch.tensor(edge_attr_indices), \n",
    "                                          num_classes=BOND_FEAT_DIM).float().to(device)\n",
    "\n",
    "        data_list.append(Data(x=x_i, edge_index=edge_index, edge_attr=edge_attr, \n",
    "                              target_embed=t_embed_batch[i].unsqueeze(0)))\n",
    "    \n",
    "    return DataLoader(data_list, batch_size=batch_size).dataset\n",
    "\n",
    "# --- 4.2. WGAN-GP Gradient Penalty Calculation (Simplified) ---\n",
    "def calculate_gradient_penalty(discriminator, real_data, fake_data, lambda_gp, device):\n",
    "    \"\"\"Calculates the Gradient Penalty on interpolated node features (X).\"\"\"\n",
    "    \n",
    "    # --- FIX: Match fake_data.x size to real_data.x size for interpolation ---\n",
    "    real_x_size = real_data.x.size(0)\n",
    "    fake_x = fake_data.x.detach()\n",
    "    \n",
    "    if fake_x.size(0) > real_x_size:\n",
    "        # Truncate fake nodes if the generated batch is larger\n",
    "        fake_x = fake_x[:real_x_size]\n",
    "    elif fake_x.size(0) < real_x_size:\n",
    "        # Pad fake nodes if the generated batch is smaller\n",
    "        # Pad with zeros to match the number of nodes in the real batch\n",
    "        padding = torch.zeros(real_x_size - fake_x.size(0), fake_x.size(1), device=device)\n",
    "        fake_x = torch.cat([fake_x, padding], dim=0)\n",
    "\n",
    "    # 1. Linear Interpolation (Now guaranteed to work as both tensors are the same size)\n",
    "    alpha = torch.rand(real_x_size, 1).to(device) \n",
    "    interpolated_x = (alpha * real_data.x.detach()) + ((1 - alpha) * fake_x)\n",
    "    interpolated_x.requires_grad_(True)\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    # Create interpolated Data object (using real graph structure for batch context)\n",
    "    interpolated_data = Data(x=interpolated_x, \n",
    "                             edge_index=real_data.edge_index, \n",
    "                             edge_attr=real_data.edge_attr, \n",
    "                             batch=real_data.batch, \n",
    "                             target_embed=real_data.target_embed)\n",
    "\n",
    "    disc_interpolates = discriminator(interpolated_data)\n",
    "    \n",
    "    # Compute Gradients\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=disc_interpolates, inputs=interpolated_x,\n",
    "        grad_outputs=torch.ones_like(disc_interpolates),\n",
    "        create_graph=True, retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # Calculate Penalty\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = lambda_gp * ((gradient_norm - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty\n",
    "\n",
    "# --- 4.3. Main Training Loop ---\n",
    "def run_wgan_gp_training(generator, discriminator, data_loader, epochs, n_critic):\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        g_loss_sum, d_loss_sum = 0, 0\n",
    "        \n",
    "        for real_data in data_loader:\n",
    "            real_data.to(DEVICE)\n",
    "            batch_size = real_data.num_graphs\n",
    "            target_embed_batch = real_data.target_embed.squeeze(1)\n",
    "\n",
    "            # 1. Train Discriminator (n_critic steps)\n",
    "            for _ in range(n_critic):\n",
    "                optimizer_D.zero_grad()\n",
    "                \n",
    "                # Real Loss\n",
    "                d_real = discriminator(real_data).mean()\n",
    "                \n",
    "                # Fake Loss\n",
    "                z = torch.randn(batch_size, Z_DIM).to(DEVICE)\n",
    "                x_fake, adj_fake = generator(z, target_embed_batch)\n",
    "                \n",
    "                # Convert generated tensors to PyG Data\n",
    "                fake_data_list = convert_fake_to_data(x_fake.detach(), adj_fake.detach(), target_embed_batch, DEVICE)\n",
    "                fake_data = next(iter(DataLoader(fake_data_list, batch_size=batch_size))).to(DEVICE)\n",
    "                \n",
    "                d_fake = discriminator(fake_data).mean()\n",
    "                \n",
    "                # Gradient Penalty\n",
    "                gp = calculate_gradient_penalty(discriminator, real_data, fake_data, LAMBDA_GP, DEVICE)\n",
    "                \n",
    "                # Discriminator Loss\n",
    "                d_loss = - (d_real - d_fake) + gp\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "                d_loss_sum += d_loss.item()\n",
    "            \n",
    "            # 2. Train Generator (1 step)\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size, Z_DIM).to(DEVICE)\n",
    "            x_fake, adj_fake = generator(z, target_embed_batch)\n",
    "            \n",
    "            fake_data_list = convert_fake_to_data(x_fake, adj_fake, target_embed_batch, DEVICE)\n",
    "            fake_data = next(iter(DataLoader(fake_data_list, batch_size=batch_size))).to(DEVICE)\n",
    "            \n",
    "            # Generator Loss\n",
    "            g_loss = - discriminator(fake_data).mean()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            g_loss_sum += g_loss.item()\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            avg_d_loss = d_loss_sum / len(data_loader) / n_critic\n",
    "            avg_g_loss = g_loss_sum / len(data_loader)\n",
    "            print(f\"Epoch {epoch}/{EPOCHS} | D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f}\")\n",
    "\n",
    "# --- Execute Training ---\n",
    "# UNCOMMENT THE LINE BELOW TO START TRAINING\n",
    "# run_wgan_gp_training(generator, discriminator, real_loader, EPOCHS, N_CRITIC) \n",
    "print(\"\\nCode is complete and ready to run. Ensure your file paths are correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba8e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting WGAN-GP Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\AppData\\Local\\Temp\\ipykernel_944\\2941334568.py:24: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  return DataLoader(data_list, batch_size=batch_size).dataset\n",
      "C:\\Users\\nikhi\\AppData\\Local\\Temp\\ipykernel_944\\2941334568.py:99: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  fake_data = next(iter(DataLoader(fake_data_list, batch_size=batch_size))).to(DEVICE)\n",
      "C:\\Users\\nikhi\\AppData\\Local\\Temp\\ipykernel_944\\2941334568.py:119: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  fake_data = next(iter(DataLoader(fake_data_list, batch_size=batch_size))).to(DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | D Loss: -52.3648 | G Loss: 78.6632\n",
      "Epoch 2/100 | D Loss: -121.5836 | G Loss: 7.7575\n",
      "Epoch 3/100 | D Loss: -156.0709 | G Loss: -3.7328\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Execute Training ---\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Starting WGAN-GP Training ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mrun_wgan_gp_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_CRITIC\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining completed.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mrun_wgan_gp_training\u001b[39m\u001b[34m(generator, discriminator, data_loader, epochs, n_critic)\u001b[39m\n\u001b[32m     95\u001b[39m x_fake, adj_fake = generator(z, target_embed_batch)\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Convert generated tensors to PyG Data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m fake_data_list = \u001b[43mconvert_fake_to_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_fake\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_fake\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_embed_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m fake_data = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(DataLoader(fake_data_list, batch_size=batch_size))).to(DEVICE)\n\u001b[32m    101\u001b[39m d_fake = discriminator(fake_data).mean()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mconvert_fake_to_data\u001b[39m\u001b[34m(x_fake_tensor, adj_fake_tensor, t_embed_batch, device)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Use the most probable bond type for the edge feature\u001b[39;00m\n\u001b[32m     16\u001b[39m adj_i = adj_fake_tensor[i, :, :, :]\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m edge_attr_indices = \u001b[43m[\u001b[49m\u001b[43madj_i\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43medge_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     18\u001b[39m edge_attr = nn.functional.one_hot(torch.tensor(edge_attr_indices), \n\u001b[32m     19\u001b[39m                                   num_classes=BOND_FEAT_DIM).float().to(device)\n\u001b[32m     21\u001b[39m data_list.append(Data(x=x_i, edge_index=edge_index, edge_attr=edge_attr, \n\u001b[32m     22\u001b[39m                       target_embed=t_embed_batch[i].unsqueeze(\u001b[32m0\u001b[39m)))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Use the most probable bond type for the edge feature\u001b[39;00m\n\u001b[32m     16\u001b[39m adj_i = adj_fake_tensor[i, :, :, :]\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m edge_attr_indices = [\u001b[43madj_i\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r, c \u001b[38;5;129;01min\u001b[39;00m edge_indices]\n\u001b[32m     18\u001b[39m edge_attr = nn.functional.one_hot(torch.tensor(edge_attr_indices), \n\u001b[32m     19\u001b[39m                                   num_classes=BOND_FEAT_DIM).float().to(device)\n\u001b[32m     21\u001b[39m data_list.append(Data(x=x_i, edge_index=edge_index, edge_attr=edge_attr, \n\u001b[32m     22\u001b[39m                       target_embed=t_embed_batch[i].unsqueeze(\u001b[32m0\u001b[39m)))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Execute Training ---\n",
    "print(\"\\n--- Starting WGAN-GP Training ---\")\n",
    "run_wgan_gp_training(generator, discriminator, real_loader, EPOCHS, N_CRITIC) \n",
    "print(\"\\nTraining completed.\")\n",
    "\"There was a mode collapse we need to fix that\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9557d153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3989 potent inhibitors for Uniprot ID P00533.\n",
      "Prepared 1334 real graph samples for training.\n",
      "The final dataset size is: 1334 molecules.\n"
     ]
    }
   ],
   "source": [
    "# --- Data Pipeline Execution ---\n",
    "inhibitor_smiles = extract_potent_inhibitors(CHEMPL_DB_PATH, TARGET_UNIPROT_ID)\n",
    "real_data_list = [smiles_to_graph(s, TARGET_EMBED) for s in inhibitor_smiles]\n",
    "real_data_list = [d for d in real_data_list if d is not None]\n",
    "\n",
    "# This is the line that reports the dataset size:\n",
    "print(f\"Prepared {len(real_data_list)} real graph samples for training.\")\n",
    "\n",
    "# Your dataset size is:\n",
    "dataset_size = len(real_data_list)\n",
    "print(f\"The final dataset size is: {dataset_size} molecules.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
