{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6537655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA is available! GPU will be used for training.\n",
      "PyTorch CUDA Version: 12.1\n",
      "GPU Name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Using device: cuda\n",
      "Opening gzipped FASTA file: DL_ENDSEM__DATASET/chembl_35_blast.fa.gz\n",
      "Loading ProtT5 model... (This may take a moment)\n",
      "Generated protein embedding of shape: torch.Size([1024])\n",
      "Found 6727 potent inhibitors for UniProt ID P00533.\n",
      "Prepared 2612 real graph samples for training.\n",
      "Initializing models...\n",
      "\n",
      "--- Starting WGAN-GP Training (v3.1 - Sparse & Stabilized) ---\n",
      "Warning: Checkpoint file not found, starting from scratch: checkpoints/P00533_epoch_39.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 41/41 [03:00<00:00,  4.39s/it, D_Fake=0.0785, D_Loss=6.3134, D_Real=0.1290, G_Loss=422.3418]\n",
      "Epoch 2/100: 100%|██████████| 41/41 [02:52<00:00,  4.22s/it, D_Fake=0.0750, D_Loss=6.2448, D_Real=0.1615, G_Loss=420.2953]\n",
      "Epoch 3/100: 100%|██████████| 41/41 [03:00<00:00,  4.40s/it, D_Fake=0.0736, D_Loss=6.1762, D_Real=0.1961, G_Loss=417.7917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint to checkpoints\\P00533_epoch_3.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 41/41 [02:56<00:00,  4.29s/it, D_Fake=0.0738, D_Loss=6.1063, D_Real=0.2337, G_Loss=415.3689]\n",
      "Epoch 5/100: 100%|██████████| 41/41 [02:56<00:00,  4.31s/it, D_Fake=0.0738, D_Loss=6.0334, D_Real=0.2740, G_Loss=412.4175]\n",
      "Epoch 6/100: 100%|██████████| 41/41 [03:02<00:00,  4.46s/it, D_Fake=0.0707, D_Loss=5.9543, D_Real=0.3174, G_Loss=409.2957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint to checkpoints\\P00533_epoch_6.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 41/41 [03:00<00:00,  4.39s/it, D_Fake=0.0634, D_Loss=5.8679, D_Real=0.3639, G_Loss=405.4732]\n",
      "Epoch 8/100: 100%|██████████| 41/41 [02:59<00:00,  4.37s/it, D_Fake=0.0528, D_Loss=5.7743, D_Real=0.4143, G_Loss=401.0146]\n",
      "Epoch 9/100: 100%|██████████| 41/41 [02:58<00:00,  4.36s/it, D_Fake=0.0366, D_Loss=5.6705, D_Real=0.4694, G_Loss=396.2563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint to checkpoints\\P00533_epoch_9.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 41/41 [02:58<00:00,  4.36s/it, D_Fake=0.0090, D_Loss=5.5496, D_Real=0.5302, G_Loss=391.2940]\n",
      "Epoch 11/100: 100%|██████████| 41/41 [02:56<00:00,  4.30s/it, D_Fake=-0.0193, D_Loss=5.4220, D_Real=0.5969, G_Loss=385.5860]\n",
      "Epoch 12/100: 100%|██████████| 41/41 [03:26<00:00,  5.05s/it, D_Fake=-0.0584, D_Loss=5.2756, D_Real=0.6717, G_Loss=379.4465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint to checkpoints\\P00533_epoch_12.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 41/41 [04:23<00:00,  6.44s/it, D_Fake=-0.1063, D_Loss=5.1100, D_Real=0.7568, G_Loss=372.7449]\n",
      "Epoch 14/100: 100%|██████████| 41/41 [04:25<00:00,  6.48s/it, D_Fake=-0.1587, D_Loss=4.9298, D_Real=0.8522, G_Loss=365.3006]\n",
      "Epoch 15/100: 100%|██████████| 41/41 [04:30<00:00,  6.59s/it, D_Fake=-0.2215, D_Loss=4.7254, D_Real=0.9614, G_Loss=357.7542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint to checkpoints\\P00533_epoch_15.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|██████████| 41/41 [04:32<00:00,  6.65s/it, D_Fake=-0.2899, D_Loss=4.5031, D_Real=1.0829, G_Loss=349.8709]\n",
      "Epoch 17/100: 100%|██████████| 41/41 [04:26<00:00,  6.51s/it, D_Fake=-0.3700, D_Loss=4.2543, D_Real=1.2192, G_Loss=340.6096]\n",
      "Epoch 18/100: 100%|██████████| 41/41 [04:23<00:00,  6.43s/it, D_Fake=-0.4641, D_Loss=3.9753, D_Real=1.3718, G_Loss=331.6261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint to checkpoints\\P00533_epoch_18.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|██████████| 41/41 [04:22<00:00,  6.39s/it, D_Fake=-0.5653, D_Loss=3.6684, D_Real=1.5452, G_Loss=321.8462]\n",
      "Epoch 20/100: 100%|██████████| 41/41 [04:34<00:00,  6.70s/it, D_Fake=-0.6721, D_Loss=3.3369, D_Real=1.7376, G_Loss=311.6981]\n",
      "Epoch 21/100: 100%|██████████| 41/41 [04:22<00:00,  6.41s/it, D_Fake=-0.7978, D_Loss=2.9639, D_Real=1.9526, G_Loss=301.4414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint to checkpoints\\P00533_epoch_21.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|██████████| 41/41 [04:16<00:00,  6.26s/it, D_Fake=-0.9233, D_Loss=2.5713, D_Real=2.1876, G_Loss=290.2246]\n",
      "Epoch 23/100: 100%|██████████| 41/41 [04:23<00:00,  6.43s/it, D_Fake=-1.0758, D_Loss=2.1222, D_Real=2.4520, G_Loss=279.1951]\n",
      "Epoch 24/100: 100%|██████████| 41/41 [04:19<00:00,  6.33s/it, D_Fake=-1.2311, D_Loss=1.6402, D_Real=2.7466, G_Loss=267.6787]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint to checkpoints\\P00533_epoch_24.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|██████████| 41/41 [04:09<00:00,  6.07s/it, D_Fake=-1.4153, D_Loss=1.0994, D_Real=3.0712, G_Loss=255.7196]\n",
      "Epoch 26/100: 100%|██████████| 41/41 [04:12<00:00,  6.16s/it, D_Fake=-1.6114, D_Loss=0.5098, D_Real=3.4327, G_Loss=243.7436]\n",
      "Epoch 27/100: 100%|██████████| 41/41 [04:08<00:00,  6.05s/it, D_Fake=-1.8269, D_Loss=-0.1352, D_Real=3.8302, G_Loss=231.5846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint to checkpoints\\P00533_epoch_27.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: 100%|██████████| 41/41 [03:53<00:00,  5.70s/it, D_Fake=-2.0838, D_Loss=-0.8640, D_Real=4.2702, G_Loss=218.7500]\n",
      "Epoch 29/100: 100%|██████████| 41/41 [03:46<00:00,  5.54s/it, D_Fake=-2.3540, D_Loss=-1.6545, D_Real=4.7586, G_Loss=205.6269]\n",
      "Epoch 30/100: 100%|██████████| 41/41 [03:46<00:00,  5.54s/it, D_Fake=-2.6489, D_Loss=-2.5252, D_Real=5.3026, G_Loss=192.4258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint to checkpoints\\P00533_epoch_30.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: 100%|██████████| 41/41 [03:41<00:00,  5.39s/it, D_Fake=-2.9247, D_Loss=-3.4320, D_Real=5.9020, G_Loss=179.0131]\n",
      "Epoch 32/100: 100%|██████████| 41/41 [03:38<00:00,  5.32s/it, D_Fake=-3.1658, D_Loss=-4.3609, D_Real=6.5580, G_Loss=165.7755]\n",
      "Epoch 33/100: 100%|██████████| 41/41 [03:42<00:00,  5.42s/it, D_Fake=-3.2347, D_Loss=-5.1959, D_Real=7.2925, G_Loss=151.6629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint to checkpoints\\P00533_epoch_33.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100: 100%|██████████| 41/41 [03:38<00:00,  5.33s/it, D_Fake=-3.0599, D_Loss=-5.8637, D_Real=8.1036, G_Loss=138.6071]\n",
      "Epoch 35/100: 100%|██████████| 41/41 [03:32<00:00,  5.19s/it, D_Fake=-2.6266, D_Loss=-6.3577, D_Real=8.9996, G_Loss=124.5354]\n",
      "Epoch 36/100: 100%|██████████| 41/41 [03:19<00:00,  4.86s/it, D_Fake=-2.0827, D_Loss=-6.8263, D_Real=9.9810, G_Loss=111.3535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint to checkpoints\\P00533_epoch_36.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100: 100%|██████████| 41/41 [03:20<00:00,  4.90s/it, D_Fake=-1.5849, D_Loss=-7.4504, D_Real=11.0719, G_Loss=98.7935] \n",
      "Epoch 38/100: 100%|██████████| 41/41 [03:25<00:00,  5.01s/it, D_Fake=-1.1493, D_Loss=-8.2244, D_Real=12.2506, G_Loss=86.0722]\n",
      "Epoch 39/100: 100%|██████████| 41/41 [03:21<00:00,  4.91s/it, D_Fake=-0.7783, D_Loss=-9.2058, D_Real=13.5722, G_Loss=73.9501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint to checkpoints\\P00533_epoch_39.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100: 100%|██████████| 41/41 [03:25<00:00,  5.02s/it, D_Fake=-0.4598, D_Loss=-10.3516, D_Real=15.0059, G_Loss=62.4002]\n",
      "Epoch 41/100: 100%|██████████| 41/41 [03:19<00:00,  4.87s/it, D_Fake=-0.2108, D_Loss=-11.6765, D_Real=16.5491, G_Loss=51.2625]\n",
      "Epoch 42/100: 100%|██████████| 41/41 [03:20<00:00,  4.89s/it, D_Fake=-0.0311, D_Loss=-13.2513, D_Real=18.2731, G_Loss=41.4048]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint to checkpoints\\P00533_epoch_42.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/100: 100%|██████████| 41/41 [03:18<00:00,  4.83s/it, D_Fake=0.0555, D_Loss=-15.0720, D_Real=20.1496, G_Loss=32.0571]\n",
      "Epoch 44/100: 100%|██████████| 41/41 [03:26<00:00,  5.03s/it, D_Fake=0.0782, D_Loss=-17.1005, D_Real=22.1700, G_Loss=24.2391]\n",
      "Epoch 45/100: 100%|██████████| 41/41 [03:26<00:00,  5.04s/it, D_Fake=0.0812, D_Loss=-19.3529, D_Real=24.3948, G_Loss=17.9597]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint to checkpoints\\P00533_epoch_45.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/100: 100%|██████████| 41/41 [03:11<00:00,  4.66s/it, D_Fake=0.0818, D_Loss=-21.7847, D_Real=26.7966, G_Loss=13.0526]\n",
      "Epoch 47/100: 100%|██████████| 41/41 [02:51<00:00,  4.19s/it, D_Fake=0.0823, D_Loss=-24.4242, D_Real=29.4061, G_Loss=9.4442] \n",
      "Epoch 48/100: 100%|██████████| 41/41 [02:30<00:00,  3.66s/it, D_Fake=0.0828, D_Loss=-27.2789, D_Real=32.2307, G_Loss=7.1539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint to checkpoints\\P00533_epoch_48.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/100:   0%|          | 0/41 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 934\u001b[39m\n\u001b[32m    932\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Starting WGAN-GP Training (v3.1 - Sparse & Stabilized) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# --- MODIFIED: Capture the returned histories & pass resume path ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m934\u001b[39m d_loss_hist, g_loss_hist, d_real_hist, d_fake_hist = \u001b[43mrun_wgan_gp_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreal_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mN_CRITIC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mTARGET_UNIPROT_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# <-- Pass the ID for checkpoint filenames\u001b[39;49;00m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRESUME_CHECKPOINT_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# <-- Pass the resume path\u001b[39;49;00m\n\u001b[32m    942\u001b[39m \u001b[43m)\u001b[49m \n\u001b[32m    943\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining completed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# --- ADDED: Call the new plotting function ---\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 563\u001b[39m, in \u001b[36mrun_wgan_gp_training\u001b[39m\u001b[34m(generator, discriminator, data_loader, epochs, n_critic, target_id, resume_from_checkpoint)\u001b[39m\n\u001b[32m    560\u001b[39m progress_bar = tqdm(data_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# --- THIS IS THE BATCH LOOP ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreal_data\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreal_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_graphs\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Need at least 2 for GP\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nikhi\\OneDrive\\Attachments\\Desktop\\Git_Repos\\NNDL_endsem\\venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nikhi\\OneDrive\\Attachments\\Desktop\\Git_Repos\\NNDL_endsem\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:439\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    437\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nikhi\\OneDrive\\Attachments\\Desktop\\Git_Repos\\NNDL_endsem\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:387\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nikhi\\OneDrive\\Attachments\\Desktop\\Git_Repos\\NNDL_endsem\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1040\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1033\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1034\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1036\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1037\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1039\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1040\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1042\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\context.py:336\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    335\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     94\u001b[39m     reduction.dump(prep_data, to_child)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[43mreduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     97\u001b[39m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\reduction.py:60\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, file, protocol)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdump\u001b[39m(obj, file, protocol=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     59\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader \n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.utils import softmax\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import Descriptors, QED, AllChem\n",
    "from Bio import SeqIO\n",
    "import random\n",
    "import re\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os \n",
    "\n",
    "# --- 1. System & Configuration ---\n",
    "\n",
    "# --- Configuration (UPDATE THESE PATHS & ID) ---\n",
    "CHEMPL_DB_PATH = 'DL_ENDSEM__DATASET/chembl_35/chembl_35_sqlite/chembl_35.db'\n",
    "BLAST_FASTA_PATH = 'DL_ENDSEM__DATASET/chembl_35_blast.fa.gz'\n",
    "TARGET_UNIPROT_ID = \"P00533\" # Example: EGFR Kinase\n",
    "\n",
    "# --- Atom Definitions ---\n",
    "ATOM_CLASSES = [6, 7, 8, 9, 15, 16, 17, 35, 53] # C, N, O, F, P, S, Cl, Br, I\n",
    "ATOM_CLASSES_MAP = {num: i for i, num in enumerate(ATOM_CLASSES)}\n",
    "ATOM_FEAT_DIM = len(ATOM_CLASSES) # Now 9\n",
    "# Define valencies for our ATOM_CLASSES: [C, N, O, F, P, S, Cl, Br, I]\n",
    "VALENCIES = [4, 3, 2, 1, 5, 6, 1, 1, 1]\n",
    "\n",
    "# --- Bond Definitions ---\n",
    "# Define bond types RDKit knows\n",
    "BOND_CLASSES_RDKIT = [Chem.BondType.SINGLE, Chem.BondType.DOUBLE, Chem.BondType.TRIPLE, Chem.BondType.AROMATIC]\n",
    "# Add a 5th \"No Bond\" class for the Generator\n",
    "BOND_FEAT_DIM_GENERATOR = len(BOND_CLASSES_RDKIT) + 1 # Now 5\n",
    "NO_BOND_IDX = len(BOND_CLASSES_RDKIT) # Index 4\n",
    "# Discriminator only sees 4 bond types\n",
    "BOND_FEAT_DIM_DISCRIMINATOR = len(BOND_CLASSES_RDKIT) # Back to 4\n",
    "# Define bond orders for our 5 Generator bond types: [S, D, T, A, None]\n",
    "BOND_ORDERS = [1.0, 2.0, 3.0, 1.5, 0.0]\n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "Z_DIM = 100          # Latent noise dimension\n",
    "EMBED_DIM = 128      # Hidden dimension for the Graph Transformer\n",
    "T_EMBED_DIM = 1024   # Target embedding dimension (from ProtT5)\n",
    "LAMBDA_GP = 10.0     # Gradient Penalty weight\n",
    "LAMBDA_VALENCY = 1.0 # Valency loss weight\n",
    "MAX_NODES = 30       # Max atoms in generated molecules\n",
    "N_CRITIC = 1         # Discriminator training steps per Generator step (1:1)\n",
    "EPOCHS = 100         # Run for 100 epochs\n",
    "BATCH_SIZE = 64      # Your increased batch size\n",
    "CPU_WORKERS = 4      \n",
    "\n",
    "# --- CUDA Check ---\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ CUDA is available! GPU will be used for training.\")\n",
    "    print(f\"PyTorch CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    print(\"❌ CUDA not found. Running on CPU.\")\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Tensors (Define *after* DEVICE is set) ---\n",
    "VALENCY_TENSOR = torch.tensor(VALENCIES, dtype=torch.float, device=DEVICE)\n",
    "BOND_ORDER_TENSOR = torch.tensor(BOND_ORDERS, dtype=torch.float, device=DEVICE)\n",
    "\n",
    "\n",
    "# --- 2. Real Protein Embedding Generation (No changes) ---\n",
    "# ... (rest of your script follows) ..\n",
    "\n",
    "# --- 2. Real Protein Embedding Generation (No changes) ---\n",
    "\n",
    "def load_target_sequence(fasta_path, uniprot_id):\n",
    "    # (No changes to this function)\n",
    "    print(f\"Opening gzipped FASTA file: {fasta_path}\")\n",
    "    try:\n",
    "        with gzip.open(fasta_path, \"rt\") as handle:\n",
    "            for record in SeqIO.parse(handle, \"fasta\"):\n",
    "                if uniprot_id in record.id or uniprot_id in record.description:\n",
    "                    return str(record.seq)\n",
    "            print(f\"Warning: Could not find sequence for {uniprot_id} in {fasta_path}\")\n",
    "            return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FATAL ERROR: FASTA file not found at {fasta_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL ERROR: Could not read FASTA file. Error: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_protein_embedding(sequence, device):\n",
    "    # (No changes to this function)\n",
    "    print(\"Loading ProtT5 model... (This may take a moment)\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "    model = T5EncoderModel.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc').to(device)\n",
    "    model.eval() \n",
    "    sequence_preprocessed = \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))\n",
    "    inputs = tokenizer(sequence_preprocessed, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = model(**inputs).last_hidden_state\n",
    "    protein_vec = embedding.mean(dim=1).squeeze(0)\n",
    "    print(f\"Generated protein embedding of shape: {protein_vec.shape}\")\n",
    "    return protein_vec\n",
    "\n",
    "# --- Generate the REAL Target Embedding ---\n",
    "target_seq = load_target_sequence(BLAST_FASTA_PATH, TARGET_UNIPROT_ID)\n",
    "if target_seq is None:\n",
    "    raise ValueError(f\"Target sequence for {TARGET_UNIPROT_ID} not found. Exiting.\")\n",
    "TARGET_EMBED = get_protein_embedding(target_seq, DEVICE)\n",
    "\n",
    "\n",
    "# --- 3. Data Pipeline (Molecules -> Graphs) (v3 Logic) ---\n",
    "\n",
    "def extract_potent_inhibitors(db_path, uniprot_id, potency_cutoff_nM=100):\n",
    "    # (No changes to this function)\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        sql_query = f\"\"\"\n",
    "        SELECT DISTINCT cs.canonical_smiles\n",
    "        FROM activities acts\n",
    "        JOIN assays a ON acts.assay_id = a.assay_id\n",
    "        JOIN target_dictionary td ON a.tid = td.tid\n",
    "        JOIN target_components tc ON td.tid = tc.tid\n",
    "        JOIN component_sequences cseq ON tc.component_id = cseq.component_id\n",
    "        JOIN compound_structures cs ON acts.molregno = cs.molregno\n",
    "        WHERE\n",
    "            cseq.accession = '{uniprot_id}' AND\n",
    "            acts.standard_type = 'IC50' AND\n",
    "            acts.standard_units = 'nM' AND\n",
    "            acts.standard_relation = '=' AND\n",
    "            acts.standard_value <= {potency_cutoff_nM}\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(sql_query, conn)\n",
    "        conn.close()\n",
    "        print(f\"Found {len(df)} potent inhibitors for UniProt ID {uniprot_id}.\")\n",
    "        return df['canonical_smiles'].unique().tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during database query: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_atom_features(atom):\n",
    "    \"\"\"Creates a one-hot vector for the atom type.\"\"\"\n",
    "    atom_num = atom.GetAtomicNum()\n",
    "    if atom_num not in ATOM_CLASSES_MAP:\n",
    "        return None # Atom is not in our allowed list\n",
    "        \n",
    "    atom_index = ATOM_CLASSES_MAP[atom_num]\n",
    "    atom_one_hot = torch.zeros(ATOM_FEAT_DIM, dtype=torch.float)\n",
    "    atom_one_hot[atom_index] = 1.0\n",
    "    return atom_one_hot\n",
    "\n",
    "def smiles_to_graph(smiles, target_embed):\n",
    "    \"\"\"Converts SMILES to a PyG Data object.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol: return None\n",
    "    if mol.GetNumAtoms() > MAX_NODES: return None\n",
    "\n",
    "    atom_features_list = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        features = get_atom_features(atom)\n",
    "        if features is None: # Skip molecule if it contains an invalid atom\n",
    "            return None\n",
    "        atom_features_list.append(features)\n",
    "\n",
    "    if not atom_features_list:\n",
    "        return None\n",
    "    x = torch.stack(atom_features_list)\n",
    "    \n",
    "    edge_indices, edge_attrs = [], []\n",
    "    # --- Use the 4-class bond list ---\n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        # Create a 4-dim one-hot vector\n",
    "        bond_type_oh = [int(bond.GetBondType() == t) for t in BOND_CLASSES_RDKIT]\n",
    "        \n",
    "        # --- Ensure bond type is one we recognize ---\n",
    "        if sum(bond_type_oh) == 1: #i.e., it's S, D, T, or Aromatic\n",
    "            edge_indices.extend([[i, j], [j, i]])\n",
    "            edge_attrs.extend([bond_type_oh, bond_type_oh])\n",
    "\n",
    "    # --- Return None if molecule has no *recognized* bonds ---\n",
    "    if not edge_indices and mol.GetNumAtoms() > 1: # Only allow single-atom \"graphs\"\n",
    "        return None\n",
    "    \n",
    "    if not edge_indices:\n",
    "        # Handle single-atom molecule (e.g., [O-2])\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        edge_attr = torch.empty((0, BOND_FEAT_DIM_DISCRIMINATOR), dtype=torch.float)\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attrs, dtype=torch.float) # Shape: [num_bonds, 4]\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, \n",
    "                target_embed=target_embed.unsqueeze(0))\n",
    "    return data\n",
    "\n",
    "# --- Data Pipeline Execution ---\n",
    "\n",
    "inhibitor_smiles = extract_potent_inhibitors(CHEMPL_DB_PATH, TARGET_UNIPROT_ID, potency_cutoff_nM=5000)\n",
    "real_data_list = [smiles_to_graph(s, TARGET_EMBED.cpu()) for s in inhibitor_smiles]\n",
    "real_data_list = [d for d in real_data_list if d is not None]\n",
    "\n",
    "if not real_data_list:\n",
    "    print(f\"FATAL: No valid inhibitor data found (or all were filtered out). Check ATOM_CLASSES and BOND_CLASSES.\")\n",
    "    exit()\n",
    "\n",
    "# --- USE THE STANDARD PyG DATALOADER ---\n",
    "real_loader = PyGDataLoader(\n",
    "    real_data_list, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=CPU_WORKERS, \n",
    "    pin_memory=True \n",
    ")\n",
    "print(f\"Prepared {len(real_data_list)} real graph samples for training.\")\n",
    "\n",
    "\n",
    "# --- 4. Model Architecture (v3 Logic) ---\n",
    "\n",
    "# --- 4.1. Relational Graph Transformer Layer (MODIFIED) ---\n",
    "class RelationalGraphTransformerLayer(MessagePassing):\n",
    "    # --- CHANGED: Added residual connection & robust message passing ---\n",
    "    def __init__(self, in_channels, out_channels, edge_dim, heads=4, dropout=0.1, **kwargs):\n",
    "        super().__init__(aggr='add', node_dim=0, **kwargs)\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.lin_q = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_k = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_v = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_edge = nn.Linear(edge_dim, heads * out_channels, bias=False) \n",
    "        self.att_coeff = nn.Parameter(torch.Tensor(1, heads, 2 * out_channels)) \n",
    "        self.lin_out = nn.Linear(heads * out_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # --- CHANGED: Add a skip connection linear layer ---\n",
    "        # This is CRITICAL for stability and to handle nodes/graphs with 0 edges.\n",
    "        if in_channels == out_channels:\n",
    "            self.lin_skip = nn.Identity()\n",
    "        else:\n",
    "            self.lin_skip = nn.Linear(in_channels, out_channels)\n",
    "            \n",
    "        nn.init.xavier_uniform_(self.att_coeff)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        Q = self.lin_q(x).view(-1, self.heads, self.out_channels)\n",
    "        K = self.lin_k(x).view(-1, self.heads, self.out_channels)\n",
    "        V = self.lin_v(x).view(-1, self.heads, self.out_channels)\n",
    "        \n",
    "        # --- Handle empty edge_attr tensor ---\n",
    "        if edge_attr.size(0) > 0:\n",
    "             E = self.lin_edge(edge_attr).view(-1, self.heads, self.out_channels)\n",
    "        else:\n",
    "             # Create empty E on the correct device with correct shape\n",
    "             E = torch.empty((0, self.heads, self.out_channels), device=x.device)\n",
    "\n",
    "        out = self.propagate(edge_index, Q=Q, K=K, V=V, E_k=E) # <--- Renamed E=E to E_k=E\n",
    "        out = out.view(-1, self.heads * self.out_channels)\n",
    "        out = self.lin_out(out)\n",
    "        \n",
    "        # --- CHANGED: Add the residual (skip) connection ---\n",
    "        x_skip = self.lin_skip(x)\n",
    "        out = out + x_skip \n",
    "        \n",
    "        return out\n",
    "\n",
    "    def message(self, Q_i, K_j, V_j, E_k, index):\n",
    "        # --- CHANGED: Replaced brittle try/except with robust check ---\n",
    "        # E_k is the tensor of edge features [num_edges_in_batch, heads, out_channels]\n",
    "        if E_k is None or E_k.size(0) == 0:\n",
    "            # If there are no edges, E_bias is zero.\n",
    "            # Q_i has shape [num_edges, heads, out_channels], so Q_i.size(0) is num_edges\n",
    "            E_bias = torch.zeros(Q_i.size(0), self.heads, 1, device=Q_i.device)\n",
    "        else:\n",
    "            E_bias = E_k.mean(dim=-1, keepdim=True) \n",
    "            \n",
    "        QK_cat = torch.cat([Q_i, K_j], dim=-1)\n",
    "        e_ij = (QK_cat * self.att_coeff).sum(dim=-1, keepdim=True)\n",
    "        e_ij = e_ij + E_bias\n",
    "        e_ij = F.leaky_relu(e_ij)\n",
    "        alpha = softmax(e_ij, index)\n",
    "        alpha = self.dropout(alpha)\n",
    "        return V_j * alpha.view(-1, self.heads, 1)\n",
    "\n",
    "# --- 4.2. Discriminator (v3 Logic) ---\n",
    "class Discriminator(nn.Module):\n",
    "    # --- edge_dim is BOND_FEAT_DIM_DISCRIMINATOR (4) ---\n",
    "    def __init__(self, node_features, edge_dim, t_embed_dim, embed_dim, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_d = node_features if i == 0 else embed_dim\n",
    "            # --- This layer now expects edge_dim = 4 ---\n",
    "            self.layers.append(RelationalGraphTransformerLayer(in_d, embed_dim, edge_dim))\n",
    "        self.lin_final = nn.Linear(embed_dim + t_embed_dim, 1)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # --- Handle empty batch gracefully ---\n",
    "        if data.num_graphs == 0 or x.size(0) == 0:\n",
    "             # Return a single 0 for the batch, requires grad\n",
    "             return torch.tensor([0.0], device=x.device, requires_grad=True)\n",
    "\n",
    "        t_embed = data.target_embed\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        graph_embed = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Ensure t_embed is [B, D]\n",
    "        if t_embed.dim() == 3 and t_embed.size(1) == 1: \n",
    "            t_embed = t_embed.squeeze(1) \n",
    "        elif t_embed.dim() == 1: # Single item in batch\n",
    "            t_embed = t_embed.unsqueeze(0)\n",
    "        \n",
    "        # Handle mismatch if global_mean_pool returns empty\n",
    "        if graph_embed.size(0) == 0:\n",
    "            return torch.tensor([0.0], device=x.device, requires_grad=True)\n",
    "\n",
    "        # Ensure batch sizes match (t_embed might be [1, D] broadcasted)\n",
    "        if t_embed.size(0) == 1 and graph_embed.size(0) > 1:\n",
    "            t_embed = t_embed.repeat(graph_embed.size(0), 1)\n",
    "\n",
    "        final_input = torch.cat([graph_embed, t_embed], dim=1)\n",
    "        return self.lin_final(final_input).squeeze(1)\n",
    "\n",
    "# --- 4.3. Generator (v3 Logic) ---\n",
    "class Generator(nn.Module):\n",
    "    # --- bond_features is BOND_FEAT_DIM_GENERATOR (5) ---\n",
    "    def __init__(self, z_dim, t_embed_dim, node_features, bond_features, max_nodes=MAX_NODES):\n",
    "        super().__init__()\n",
    "        self.max_nodes = max_nodes\n",
    "        self.node_features = node_features\n",
    "        self.bond_features = bond_features\n",
    "        self.lin_x = nn.Sequential(nn.Linear(z_dim + t_embed_dim, 256), nn.ReLU(), nn.Linear(256, max_nodes * node_features))\n",
    "        # --- This layer now outputs 5 features per bond ---\n",
    "        self.lin_adj = nn.Sequential(nn.Linear(z_dim + t_embed_dim, 256), nn.ReLU(), nn.Linear(256, max_nodes * max_nodes * bond_features))\n",
    "\n",
    "    def forward(self, z, t_embed):\n",
    "        zt = torch.cat([z, t_embed], dim=1)\n",
    "        x_fake_logits = self.lin_x(zt).view(-1, self.max_nodes, self.node_features)\n",
    "        adj_fake_logits = self.lin_adj(zt).view(-1, self.max_nodes, self.max_nodes, self.bond_features)\n",
    "        return x_fake_logits, adj_fake_logits\n",
    "\n",
    "# --- Model Initialization ---\n",
    "print(\"Initializing models...\")\n",
    "# --- Pass the correct dimensions ---\n",
    "generator = Generator(Z_DIM, T_EMBED_DIM, ATOM_FEAT_DIM, BOND_FEAT_DIM_GENERATOR).to(DEVICE)\n",
    "discriminator = Discriminator(ATOM_FEAT_DIM, BOND_FEAT_DIM_DISCRIMINATOR, T_EMBED_DIM, EMBED_DIM).to(DEVICE)\n",
    "\n",
    "# --- Learning Rate (as in your script) ---\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=5e-5, betas=(0.5, 0.9)) # <-- 5x FASTER\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=1e-5, betas=(0.5, 0.9)) # <-- 2x SLOWER\n",
    "\n",
    "\n",
    "# --- 5. Training Utilities (v3 Logic) ---\n",
    "\n",
    "# --- 5.1. Sparse Graph Conversion (FIXED) ---\n",
    "def convert_fake_to_SPARSE_data_vectorized(x_fake_logits, adj_fake_logits, t_embed_batch, device, gumbel=False, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Converts Generator output (logits) to a BATCH of sparse PyG Data objects.\n",
    "    \"\"\"\n",
    "    batch_size = x_fake_logits.size(0)\n",
    "    data_list = []\n",
    "\n",
    "    # 1. Sample nodes (still vectorized)\n",
    "    if gumbel:\n",
    "        x_fake_tensor = F.gumbel_softmax(x_fake_logits, tau=temperature, hard=True)\n",
    "    else:\n",
    "        x_indices = torch.argmax(x_fake_logits, dim=-1)\n",
    "        x_fake_tensor = F.one_hot(x_indices, num_classes=ATOM_FEAT_DIM).float()\n",
    "\n",
    "    # 2. Sample bonds (still vectorized)\n",
    "    # --- !! FIXED: Apply Gumbel to bonds if in training (gumbel=True) !! ---\n",
    "    if gumbel:\n",
    "        adj_fake_tensor = F.gumbel_softmax(adj_fake_logits, tau=temperature, hard=True)\n",
    "        # adj_fake_tensor is [B, N, N, 5] (one-hot)\n",
    "    else:\n",
    "        adj_indices = torch.argmax(adj_fake_logits, dim=-1)\n",
    "        adj_fake_tensor = F.one_hot(adj_indices, num_classes=BOND_FEAT_DIM_GENERATOR).float()\n",
    "        # adj_fake_tensor is [B, N, N, 5] (one-hot)\n",
    "    \n",
    "    # 3. Loop over batch to build sparse graphs\n",
    "    for i in range(batch_size):\n",
    "        x = x_fake_tensor[i] # Shape [N, ATOM_FEAT_DIM]\n",
    "        adj_full_one_hot = adj_fake_tensor[i] # Shape [N, N, 5] (one-hot)\n",
    "        \n",
    "        edge_indices = []\n",
    "        edge_attrs = []\n",
    "\n",
    "        # Iterate over upper triangle\n",
    "        for j in range(MAX_NODES):\n",
    "            for k in range(j + 1, MAX_NODES):\n",
    "                \n",
    "                # --- CHANGED: Get bond type from one-hot tensor ---\n",
    "                bond_one_hot = adj_full_one_hot[j, k] # Shape [5]\n",
    "                bond_type_idx = torch.argmax(bond_one_hot).item()\n",
    "                \n",
    "                # --- Check if it's NOT a \"No Bond\" ---\n",
    "                if bond_type_idx != NO_BOND_IDX:\n",
    "                    # Add this edge\n",
    "                    edge_indices.extend([[j, k], [k, j]])\n",
    "                    \n",
    "                    # --- Create 4-dim one-hot vector ---\n",
    "                    bond_attr = torch.zeros(BOND_FEAT_DIM_DISCRIMINATOR, device=device)\n",
    "                    # We know bond_type_idx is 0, 1, 2, or 3\n",
    "                    bond_attr[bond_type_idx] = 1.0\n",
    "                    edge_attrs.extend([bond_attr, bond_attr])\n",
    "\n",
    "        if not edge_indices:\n",
    "            # No bonds were formed, create a dummy\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long, device=device)\n",
    "            edge_attr = torch.empty((0, BOND_FEAT_DIM_DISCRIMINATOR), dtype=torch.float, device=device)\n",
    "        else:\n",
    "            edge_index = torch.tensor(edge_indices, dtype=torch.long, device=device).t().contiguous()\n",
    "            edge_attr = torch.stack(edge_attrs)\n",
    "\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            target_embed=t_embed_batch[i].unsqueeze(0) # Embed for this single graph\n",
    "        )\n",
    "        data_list.append(data)\n",
    "\n",
    "    # 4. Re-batch the sparse graphs\n",
    "    \n",
    "    # --- CHANGED: Use Batch.from_data_list (more efficient) ---\n",
    "    if not data_list:\n",
    "        return Batch() # Return an empty batch object\n",
    "        \n",
    "    batch = Batch.from_data_list(data_list)\n",
    "    \n",
    "    # Squeeze the target_embed back to [B, T_EMBED_DIM]\n",
    "    if batch.target_embed.dim() == 3:\n",
    "        batch.target_embed = batch.target_embed.squeeze(1)\n",
    "    \n",
    "    return batch.to(device)\n",
    "\n",
    "\n",
    "# --- 5.2. WGAN-GP Gradient Penalty (v3 Logic) ---\n",
    "def calculate_gradient_penalty(discriminator, real_data, fake_data, lambda_gp, device):\n",
    "    \"\"\"\n",
    "    Interpolates on the GLOBAL graph embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Get graph embeddings first ---\n",
    "    # We don't need eval/train switching if we only use .lin_final\n",
    "    \n",
    "    # --- Get Real graph_embed ---\n",
    "    real_x, real_edge_index, real_edge_attr, real_batch = real_data.x, real_data.edge_index, real_data.edge_attr, real_data.batch\n",
    "    real_t_embed = real_data.target_embed\n",
    "    with torch.no_grad(): # Don't need grads for this part\n",
    "        for layer in discriminator.layers:\n",
    "            real_x = layer(real_x, real_edge_index, real_edge_attr)\n",
    "            real_x = F.relu(real_x)\n",
    "    real_graph_embed = global_mean_pool(real_x, real_batch)\n",
    "    \n",
    "    # --- Get Fake graph_embed ---\n",
    "    fake_x, fake_edge_index, fake_edge_attr, fake_batch = fake_data.x, fake_data.edge_index, fake_data.edge_attr, fake_data.batch\n",
    "    if fake_data.num_graphs == 0 or fake_x.size(0) == 0:\n",
    "        return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "        \n",
    "    fake_t_embed = fake_data.target_embed\n",
    "    with torch.no_grad(): # Don't need grads for this part\n",
    "        for layer in discriminator.layers:\n",
    "            fake_x = layer(fake_x, fake_edge_index, fake_edge_attr)\n",
    "            fake_x = F.relu(fake_x)\n",
    "    fake_graph_embed = global_mean_pool(fake_x, fake_batch)\n",
    "    \n",
    "    \n",
    "    # Match batch sizes if they differ\n",
    "    batch_size = min(real_graph_embed.size(0), fake_graph_embed.size(0))\n",
    "    if batch_size == 0:\n",
    "        return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "        \n",
    "    real_graph_embed = real_graph_embed[:batch_size]\n",
    "    fake_graph_embed = fake_graph_embed[:batch_size]\n",
    "    real_t_embed = real_t_embed[:batch_size]\n",
    "\n",
    "    # --- Interpolate on graph_embed ---\n",
    "    alpha = torch.rand(batch_size, 1).to(device)\n",
    "    interpolated_embed = (alpha * real_graph_embed.detach()) + ((1 - alpha) * fake_graph_embed.detach())\n",
    "    interpolated_embed.requires_grad_(True)\n",
    "    \n",
    "    # --- Combine with target and pass to *final layer only* ---\n",
    "    final_input = torch.cat([interpolated_embed, real_t_embed.detach()], dim=1)\n",
    "    disc_interpolates = discriminator.lin_final(final_input).squeeze(1)\n",
    "\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=disc_interpolates, inputs=interpolated_embed, # Grad w.r.t. interpolated_embed\n",
    "        grad_outputs=torch.ones_like(disc_interpolates),\n",
    "        create_graph=True, retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = lambda_gp * ((gradient_norm - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty\n",
    "\n",
    "# --- 6. Main Training Loop (MODIFIED FOR CHECKPOINTING) ---\n",
    "# --- 6. Main Training Loop (CORRECTED INDENTATION) ---\n",
    "def run_wgan_gp_training(generator, discriminator, data_loader, epochs, n_critic, target_id, resume_from_checkpoint=None):\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    # --- Create checkpoint directory ---\n",
    "    CHECKPOINT_DIR = \"checkpoints\"\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    \n",
    "    # --- History lists to store epoch averages ---\n",
    "    d_loss_history = []\n",
    "    g_loss_history = []\n",
    "    d_real_history = []\n",
    "    d_fake_history = []\n",
    "    \n",
    "    start_epoch = 1\n",
    "    \n",
    "    # --- Logic to resume from checkpoint ---\n",
    "    if resume_from_checkpoint:\n",
    "        if os.path.exists(resume_from_checkpoint):\n",
    "            print(f\"Loading checkpoint: {resume_from_checkpoint}\")\n",
    "            checkpoint = torch.load(resume_from_checkpoint, map_location=DEVICE)\n",
    "            \n",
    "            generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "            discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "            optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "            optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "            \n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            \n",
    "            # Load histories\n",
    "            d_loss_history = checkpoint.get('d_loss_history', [])\n",
    "            g_loss_history = checkpoint.get('g_loss_history', [])\n",
    "            d_real_history = checkpoint.get('d_real_history', [])\n",
    "            d_fake_history = checkpoint.get('d_fake_history', [])\n",
    "            \n",
    "            print(f\"Resuming training from epoch {start_epoch}\")\n",
    "        else:\n",
    "            print(f\"Warning: Checkpoint file not found, starting from scratch: {resume_from_checkpoint}\")\n",
    "\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        # --- Reset sums for each epoch ---\n",
    "        g_loss_sum, d_loss_sum = 0, 0\n",
    "        d_real_sum, d_fake_sum = 0, 0\n",
    "        \n",
    "        progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "        \n",
    "        # --- THIS IS THE BATCH LOOP ---\n",
    "        for batch_idx, real_data in enumerate(progress_bar):\n",
    "            real_data = real_data.to(DEVICE)\n",
    "            \n",
    "            if real_data.num_graphs < 2: # Need at least 2 for GP\n",
    "                continue\n",
    "                \n",
    "            batch_size = real_data.num_graphs\n",
    "            target_embed_batch = real_data.target_embed\n",
    "\n",
    "            # 1. Train Discriminator (n_critic steps)\n",
    "            for _ in range(n_critic):\n",
    "                optimizer_D.zero_grad()\n",
    "                \n",
    "                d_real = discriminator(real_data).mean()\n",
    "                \n",
    "                z = torch.randn(batch_size, Z_DIM).to(DEVICE)\n",
    "                x_fake_logits, adj_fake_logits = generator(z, target_embed_batch)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    fake_data = convert_fake_to_SPARSE_data_vectorized(\n",
    "                        x_fake_logits.detach(), adj_fake_logits.detach(), \n",
    "                        target_embed_batch, DEVICE, gumbel=False\n",
    "                    )\n",
    "                \n",
    "                if fake_data.num_graphs == 0:\n",
    "                    continue\n",
    "                \n",
    "                d_fake = discriminator(fake_data).mean()\n",
    "                gp = calculate_gradient_penalty(discriminator, real_data, fake_data, LAMBDA_GP, DEVICE)\n",
    "                \n",
    "                d_loss = (d_fake - d_real) + gp\n",
    "                \n",
    "                d_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(discriminator.parameters(), 1.0)\n",
    "                optimizer_D.step()\n",
    "                \n",
    "                d_loss_sum += d_loss.item()\n",
    "                d_real_sum += d_real.item() \n",
    "                d_fake_sum += d_fake.item()\n",
    "            \n",
    "            # 2. Train Generator (1 step)\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size, Z_DIM).to(DEVICE)\n",
    "            x_fake_logits, adj_fake_logits = generator(z, target_embed_batch)\n",
    "            \n",
    "            # --- FIXED: We need the SOFT tensors for valency loss ---\n",
    "            x_fake_soft = F.gumbel_softmax(x_fake_logits, tau=0.5, hard=False)\n",
    "            adj_fake_soft = F.gumbel_softmax(adj_fake_logits, tau=0.5, hard=False)\n",
    "            \n",
    "            # --- Create the \"hard\" graph for the discriminator ---\n",
    "            fake_data =fake_data = convert_fake_to_SPARSE_data_vectorized(\n",
    "                x_fake_logits.detach(), adj_fake_logits.detach(), \n",
    "                target_embed_batch, DEVICE, gumbel=True\n",
    "            )\n",
    "            \n",
    "            if fake_data.num_graphs == 0:\n",
    "                continue\n",
    "\n",
    "            # --- CALCULATE NEW LOSSES ---\n",
    "            critic_loss = - discriminator(fake_data).mean()\n",
    "            valency_loss = calculate_valency_loss(x_fake_soft, adj_fake_soft)\n",
    "            g_loss = critic_loss + (LAMBDA_VALENCY * valency_loss)\n",
    "            \n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            g_loss_sum += g_loss.item()\n",
    "                \n",
    "            # --- Update postfix (THIS IS IN THE RIGHT PLACE) ---\n",
    "            num_batches = batch_idx + 1\n",
    "            progress_bar.set_postfix(\n",
    "                D_Loss=f\"{(d_loss_sum / num_batches / n_critic):.4f}\", \n",
    "                G_Loss=f\"{(g_loss_sum / num_batches):.4f}\",\n",
    "                D_Real=f\"{(d_real_sum / num_batches / n_critic):.4f}\",\n",
    "                D_Fake=f\"{(d_fake_sum / num_batches / n_critic):.4f}\"\n",
    "            )\n",
    "        # --- END OF THE BATCH LOOP ---\n",
    "\n",
    "        # --- !! MOVED: This logic now runs ONCE PER EPOCH !! ---\n",
    "        num_batches_total = len(data_loader)\n",
    "        if num_batches_total > 0:\n",
    "            d_loss_history.append(d_loss_sum / num_batches_total / n_critic)\n",
    "            g_loss_history.append(g_loss_sum / num_batches_total)\n",
    "            d_real_history.append(d_real_sum / num_batches_total / n_critic)\n",
    "            d_fake_history.append(d_fake_sum / num_batches_total / n_critic)\n",
    "        else:\n",
    "             d_loss_history.append(0)\n",
    "             g_loss_history.append(0)\n",
    "             d_real_history.append(0)\n",
    "             d_fake_history.append(0)\n",
    "\n",
    "        # --- !! MOVED: This logic now runs ONCE PER EPOCH !! ---\n",
    "        if epoch % 3 == 0:\n",
    "            checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"{target_id}_epoch_{epoch}.pth\")\n",
    "            print(f\"\\nSaving checkpoint to {checkpoint_path}...\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                'd_loss_history': d_loss_history,\n",
    "                'g_loss_history': g_loss_history,\n",
    "                'd_real_history': d_real_history,\n",
    "                'd_fake_history': d_fake_history\n",
    "            }, checkpoint_path)\n",
    "        # --- End of checkpointing logic ---\n",
    "\n",
    "    # --- ADDED: Return the histories ---\n",
    "    return d_loss_history, g_loss_history, d_real_history, d_fake_history\n",
    "# --- 5.3. NEW: Differentiable Valency Loss ---\n",
    "def calculate_valency_loss(x_fake_soft, adj_fake_soft):\n",
    "    \"\"\"\n",
    "    Calculates a differentiable valency loss.\n",
    "    x_fake_soft: [B, N, 9] (from Gumbel-Softmax, hard=False)\n",
    "    adj_fake_soft: [B, N, N, 5] (from Gumbel-Softmax, hard=False)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Calculate the \"expected max valency\" for each atom\n",
    "    # (x_fake_soft * VALENCY_TENSOR) -> [B, N, 9]\n",
    "    # .sum(dim=-1) -> [B, N]\n",
    "    expected_max_val = (x_fake_soft * VALENCY_TENSOR).sum(dim=-1)\n",
    "    \n",
    "    # 2. Calculate the \"expected bond order\" for each edge\n",
    "    # (adj_fake_soft * BOND_ORDER_TENSOR) -> [B, N, N, 5]\n",
    "    # .sum(dim=-1) -> [B, N, N]\n",
    "    expected_bond_orders = (adj_fake_soft * BOND_ORDER_TENSOR).sum(dim=-1)\n",
    "    \n",
    "    # 3. Sum bond orders for each atom to get its \"current valency\"\n",
    "    # .sum(dim=-1) sums over all 'k' for each 'j'\n",
    "    current_valency = expected_bond_orders.sum(dim=-1)\n",
    "    \n",
    "    # 4. Calculate the error (how much we are *over* the max valency)\n",
    "    # We only penalize *over-bonding*, so we use relu\n",
    "    valency_error = F.relu(current_valency - expected_max_val)\n",
    "    \n",
    "    # 5. Return the mean error\n",
    "    return valency_error.mean()\n",
    "\n",
    "# --- 7. Generation & SMILES Conversion (v3 Logic) ---\n",
    "\n",
    "def tensors_to_smiles(x_fake_one_hot, adj_fake_logits):\n",
    "    \"\"\"\n",
    "    Converts raw generator tensor output (one-hot nodes) into SMILES strings.\n",
    "    \"\"\"\n",
    "    # --- Get atomic number indices from one-hot nodes ---\n",
    "    x_fake_indices = torch.argmax(x_fake_one_hot, dim=-1).cpu().detach()\n",
    "    adj_fake_logits = adj_fake_logits.cpu().detach()\n",
    "    \n",
    "    # --- Get bond indices from 5-class logits ---\n",
    "    adj_bond_type_idx = torch.argmax(adj_fake_logits, dim=-1)\n",
    "    \n",
    "    batch_size = x_fake_indices.size(0)\n",
    "    generated_smiles = []\n",
    "    generated_mols = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        mol = Chem.RWMol()\n",
    "        atom_map = {} # Map from tensor index (0..MAX_NODES-1) to RDKit atom index\n",
    "        \n",
    "        # 1. Add atoms\n",
    "        for j in range(MAX_NODES):\n",
    "            atom_idx = x_fake_indices[i, j].item()\n",
    "            atom_num = ATOM_CLASSES[atom_idx]\n",
    "            \n",
    "            atom = Chem.Atom(atom_num)\n",
    "            rdkit_idx = mol.AddAtom(atom)\n",
    "            atom_map[j] = rdkit_idx\n",
    "                \n",
    "        # 2. Add bonds\n",
    "        for j in range(MAX_NODES):\n",
    "            for k in range(j + 1, MAX_NODES):\n",
    "                # --- Get bond type from 5-class indices ---\n",
    "                bond_type_idx = adj_bond_type_idx[i, j, k].item()\n",
    "                \n",
    "                # --- Add bond IF NOT \"No Bond\" ---\n",
    "                if bond_type_idx != NO_BOND_IDX:\n",
    "                    # --- Check if bond_type_idx is valid for RDKit bonds ---\n",
    "                    if 0 <= bond_type_idx < len(BOND_CLASSES_RDKIT):\n",
    "                        bond_type = BOND_CLASSES_RDKIT[bond_type_idx]\n",
    "                        mol.AddBond(atom_map[j], atom_map[k], bond_type)\n",
    "        \n",
    "        # 3. Sanitize and Convert\n",
    "        try:\n",
    "            Chem.SanitizeMol(mol)\n",
    "            smi = Chem.MolToSmiles(mol)\n",
    "            \n",
    "            # --- Filter out disconnected fragments ---\n",
    "            if '.' in smi:\n",
    "                generated_smiles.append(None) # Invalid fragment\n",
    "                generated_mols.append(None)\n",
    "            else:\n",
    "                generated_smiles.append(smi)\n",
    "                generated_mols.append(mol)\n",
    "        except Exception as e:\n",
    "            # print(f\"RDKit Error: {e}\") # Uncomment for debugging\n",
    "            generated_smiles.append(None) # Invalid molecule\n",
    "            generated_mols.append(None)\n",
    "\n",
    "    valid_smiles = [s for s in generated_smiles if s is not None]\n",
    "    valid_mols = [m for m in generated_mols if m is not None]\n",
    "    \n",
    "    return valid_smiles, valid_mols, generated_smiles\n",
    "\n",
    "# --- 8. Performance Metrics & Plotting (v3 Logic) ---\n",
    "\n",
    "def calculate_and_plot_metrics(generator, target_embed, real_smiles_list, num_to_generate, device):\n",
    "    \"\"\"\n",
    "    Generates molecules and calculates Validity, Uniqueness, Novelty,\n",
    "    and plots property distributions.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Generation & Evaluation ---\")\n",
    "    warnings.filterwarnings('ignore', '.*Implicit valence.*') # Suppress RDKit warnings\n",
    "    \n",
    "    generator.eval() # Set generator to evaluation mode\n",
    "    \n",
    "    real_mols = [Chem.MolFromSmiles(s) for s in real_smiles_list]\n",
    "    real_mols = [m for m in real_mols if m is not None]\n",
    "    real_smiles_set = set(real_smiles_list)\n",
    "    \n",
    "    all_valid_smiles = []\n",
    "    all_valid_mols = []\n",
    "    total_attempts = 0 # Track total attempts\n",
    "\n",
    "    print(f\"Generating {num_to_generate} *valid* molecules for evaluation...\")\n",
    "    with torch.no_grad():\n",
    "        while len(all_valid_smiles) < num_to_generate:\n",
    "            batch_size = BATCH_SIZE\n",
    "            total_attempts += batch_size\n",
    "\n",
    "            z = torch.randn(batch_size, Z_DIM).to(device)\n",
    "            t_embed_batch = target_embed.unsqueeze(0).repeat(batch_size, 1)\n",
    "            \n",
    "            x_fake_logits, adj_fake_logits = generator(z, t_embed_batch)\n",
    "            \n",
    "            # --- Use argmax (not Gumbel) for final generation ---\n",
    "            x_indices = torch.argmax(x_fake_logits, dim=-1)\n",
    "            x_fake_one_hot = F.one_hot(x_indices, num_classes=ATOM_FEAT_DIM).float()\n",
    "            \n",
    "            # --- Pass 5-dim bond logits to smiles converter ---\n",
    "            valid_smiles, valid_mols, _ = tensors_to_smiles(x_fake_one_hot, adj_fake_logits)\n",
    "            \n",
    "            all_valid_smiles.extend(valid_smiles)\n",
    "            all_valid_mols.extend(valid_mols)\n",
    "            \n",
    "            print(f\"Generated: {len(all_valid_smiles)}/{num_to_generate} valid molecules...\", end='\\r')\n",
    "            \n",
    "            if total_attempts > num_to_generate * 50 and not all_valid_smiles:\n",
    "                    print(\"\\nError: Generated too many molecules with 0 validity. Stopping.\")\n",
    "                    break\n",
    "            if total_attempts > num_to_generate * 10 and (len(all_valid_smiles) < num_to_generate): \n",
    "                # If we've tried 10x and still not enough, stop early\n",
    "                if len(all_valid_smiles) < (num_to_generate / 10):\n",
    "                    print(f\"\\nWarning: Very low validity. Stopping generation at {len(all_valid_smiles)} molecules.\")\n",
    "                    break\n",
    "\n",
    "\n",
    "    print(\"\\nGeneration complete. Calculating metrics...\")\n",
    "    \n",
    "    # --- 1. Calculate Metrics ---\n",
    "    \n",
    "    if total_attempts == 0: total_attempts = 1\n",
    "    validity = len(all_valid_smiles) / total_attempts\n",
    "    \n",
    "    if len(all_valid_smiles) > 0:\n",
    "        uniqueness = len(set(all_valid_smiles)) / len(all_valid_smiles)\n",
    "    else:\n",
    "        uniqueness = 0.0\n",
    "        \n",
    "    if len(all_valid_smiles) > 0:\n",
    "        unique_valid_smiles = set(all_valid_smiles)\n",
    "        novel_smiles = unique_valid_smiles - real_smiles_set\n",
    "        novelty = len(novel_smiles) / len(unique_valid_smiles)\n",
    "    else:\n",
    "        novelty = 0.0\n",
    "\n",
    "    print(\"\\n--- Generative Performance Metrics ---\")\n",
    "    print(f\"Total Attempts: {total_attempts}\")\n",
    "    print(f\"Total Valid Generated: {len(all_valid_smiles)}\")\n",
    "    print(f\"✅ Validity:       {validity * 100:.2f}%\")\n",
    "    print(f\"🧬 Uniqueness:     {uniqueness * 100:.2f}%\")\n",
    "    print(f\"⭐ Novelty:        {novelty * 100:.2f}%\")\n",
    "    print(\"----------------------------------------\")\n",
    "    \n",
    "    if not all_valid_mols:\n",
    "        print(\"No valid molecules generated. Skipping plots.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Calculate Properties ---\n",
    "    props_real = {\n",
    "        'MolWt': [Descriptors.MolWt(m) for m in real_mols],\n",
    "        'LogP': [Descriptors.MolLogP(m) for m in real_mols],\n",
    "        'QED': [QED.qed(m) for m in real_mols]\n",
    "    }\n",
    "    \n",
    "    props_fake = {\n",
    "        'MolWt': [Descriptors.MolWt(m) for m in all_valid_mols],\n",
    "        'LogP': [Descriptors.MolLogP(m) for m in all_valid_mols],\n",
    "        'QED': [QED.qed(m) for m in all_valid_mols]\n",
    "    }\n",
    "\n",
    "    # --- 3. Plot Distributions ---\n",
    "    print(\"Generating property distribution plots...\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    plot_titles = ['Molecular Weight (MolWt)', 'LogP', 'Quantitative Esimation of Drug-likeness (QED)']\n",
    "    prop_keys = ['MolWt', 'LogP', 'QED']\n",
    "    \n",
    "    for ax, title, key in zip(axes, plot_titles, prop_keys):\n",
    "        ax.hist(props_real[key], bins=50, alpha=0.7, label='Real (Training)', color='blue', density=True)\n",
    "        ax.hist(props_fake[key], bins=50, alpha=0.7, label='Generated (Fake)', color='red', density=True)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(\"Value\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        ax.legend()\n",
    "        \n",
    "    plt.suptitle(f\"Property Distributions (Real vs. Generated) for {TARGET_UNIPROT_ID}\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(f\"property_plots_{TARGET_UNIPROT_ID}_v3.1_checkpointed.png\")\n",
    "    print(f\"Plots saved to property_plots_{TARGET_UNIPROT_ID}_v3.1_checkpointed.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    warnings.filterwarnings('default', '.*Implicit valence.*') # Restore warnings\n",
    "\n",
    "# --- NEW FUNCTION (insert before Section 9) ---\n",
    "def plot_training_losses(d_loss_hist, g_loss_hist, d_real_hist, d_fake_hist, target_id):\n",
    "    \"\"\"\n",
    "    Plots the training history of WGAN-GP losses and scores.\n",
    "    \"\"\"\n",
    "    print(\"Generating training loss plots...\")\n",
    "    if not g_loss_hist: # Check if histories are empty\n",
    "        print(\"No loss history found. Skipping loss plots.\")\n",
    "        return\n",
    "        \n",
    "    epochs_range = range(1, len(g_loss_hist) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "    \n",
    "    # --- Plot 1: G_Loss vs D_Loss ---\n",
    "    ax1.plot(epochs_range, g_loss_hist, label='Generator Loss (G_Loss)', color='blue')\n",
    "    ax1.plot(epochs_range, d_loss_hist, label='Discriminator Loss (D_Loss)', color='red')\n",
    "    ax1.set_title(f\"Generator & Discriminator Losses for {target_id}\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # --- Plot 2: D(Real) vs D(Fake) ---\n",
    "    ax2.plot(epochs_range, d_real_hist, label='Avg. D(Real) Score', color='green')\n",
    "    ax2.plot(epochs_range, d_fake_hist, label='Avg. D(Fake) Score', color='orange')\n",
    "    ax2.set_title(f\"Critic Scores for {target_id}\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Score\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"training_loss_plots_{target_id}_v3.1_checkpointed.png\")\n",
    "    print(f\"Loss plots saved to training_loss_plots_{target_id}_v3.1_checkpointed.png\")\n",
    "    plt.show()\n",
    "\n",
    "# --- 9. --- Main Execution (Train & Evaluate) ---\n",
    "\n",
    "# --- !! MODIFIED: Set resume path here !! ---\n",
    "# Set to None to start a new training run\n",
    "# Set to a valid path to resume, e.g., \"checkpoints/P00533_epoch_3.pth\"\n",
    "RESUME_CHECKPOINT_PATH = \"checkpoints/P00533_epoch_39.pth\" # <-- CHANGE THIS\n",
    "\n",
    "# --- Execute Training ---\n",
    "print(\"\\n--- Starting WGAN-GP Training (v3.1 - Sparse & Stabilized) ---\")\n",
    "# --- MODIFIED: Capture the returned histories & pass resume path ---\n",
    "d_loss_hist, g_loss_hist, d_real_hist, d_fake_hist = run_wgan_gp_training(\n",
    "    generator, \n",
    "    discriminator, \n",
    "    real_loader, \n",
    "    EPOCHS, \n",
    "    N_CRITIC,\n",
    "    TARGET_UNIPROT_ID, # <-- Pass the ID for checkpoint filenames\n",
    "    resume_from_checkpoint=RESUME_CHECKPOINT_PATH # <-- Pass the resume path\n",
    ") \n",
    "print(\"\\nTraining completed.\")\n",
    "\n",
    "# --- ADDED: Call the new plotting function ---\n",
    "plot_training_losses(d_loss_hist, g_loss_hist, d_real_hist, d_fake_hist, TARGET_UNIPROT_ID)\n",
    "\n",
    "# --- Execute Evaluation ---\n",
    "# This will evaluate the FINAL model state after training/resuming\n",
    "num_to_eval = len(real_data_list) # --- Use count of *filtered* real data ---\n",
    "calculate_and_plot_metrics(generator, TARGET_EMBED, inhibitor_smiles, num_to_eval, DEVICE)\n",
    "\n",
    "# --- To plot from a specific checkpoint (e.g., epoch 30) ---\n",
    "# 1. You would *comment out* the training run and evaluation above.\n",
    "# 2. You would uncomment and run code like this:\n",
    "\n",
    "# print(\"\\n--- Loading Checkpoint for Evaluation ONLY ---\")\n",
    "# CHECKPOINT_TO_EVAL = \"checkpoints/P00533_epoch_30.pth\" \n",
    "# if os.path.exists(CHECKPOINT_TO_EVAL):\n",
    "#     checkpoint = torch.load(CHECKPOINT_TO_EVAL, map_location=DEVICE)\n",
    "    \n",
    "#     # Load model state\n",
    "#     generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    \n",
    "#     # Plot metrics from this checkpoint's generator\n",
    "#     print(f\"Evaluating generator from epoch {checkpoint['epoch']}...\")\n",
    "#     calculate_and_plot_metrics(generator, TARGET_EMBED, inhibitor_smiles, num_to_eval, DEVICE)\n",
    "    \n",
    "#     # Plot loss history *up to* this checkpoint\n",
    "#     print(f\"Plotting loss history up to epoch {checkpoint['epoch']}...\")\n",
    "#     plot_training_losses(\n",
    "#         checkpoint['d_loss_history'],\n",
    "#         checkpoint['g_loss_history'],\n",
    "#         checkpoint['d_real_history'],\n",
    "#         checkpoint['d_fake_history'],\n",
    "#         TARGET_UNIPROT_ID\n",
    "#     )\n",
    "# else:\n",
    "#     print(f\"Checkpoint not found: {CHECKPOINT_TO_EVAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b48c55c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: checkpoints/P00533_epoch_30.pth\n",
      "Generating 100 molecules for diagnosis...\n",
      "\n",
      "--- DIAGNOSTIC REPORT ---\n",
      "Total generated: 100\n",
      "Valid molecules: 1\n",
      "\n",
      "Failure Modes:\n",
      " - Valence Error: 94\n",
      " - Disconnected Fragments: 5\n",
      " - None (Valid): 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from rdkit import Chem\n",
    "import io\n",
    "import sys\n",
    "\n",
    "# --- SET THIS to your latest checkpoint ---\n",
    "CHECKPOINT_TO_LOAD = \"checkpoints/P00533_epoch_30.pth\"\n",
    "NUM_TO_DIAGNOSE = 100\n",
    "\n",
    "print(f\"Loading checkpoint: {CHECKPOINT_TO_LOAD}\")\n",
    "try:\n",
    "    checkpoint = torch.load(CHECKPOINT_TO_LOAD, map_location=DEVICE)\n",
    "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    generator.eval()\n",
    "\n",
    "    print(f\"Generating {NUM_TO_DIAGNOSE} molecules for diagnosis...\")\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(NUM_TO_DIAGNOSE, Z_DIM).to(DEVICE)\n",
    "        t_embed_batch = TARGET_EMBED.unsqueeze(0).repeat(NUM_TO_DIAGNOSE, 1)\n",
    "        x_fake_logits, adj_fake_logits = generator(z, t_embed_batch)\n",
    "\n",
    "        x_indices = torch.argmax(x_fake_logits, dim=-1)\n",
    "        x_fake_one_hot = F.one_hot(x_indices, num_classes=ATOM_FEAT_DIM).float()\n",
    "        adj_bond_type_idx = torch.argmax(adj_fake_logits, dim=-1)\n",
    "\n",
    "    print(\"\\n--- DIAGNOSTIC REPORT ---\")\n",
    "    valid_count = 0\n",
    "    error_counts = {}\n",
    "    \n",
    "    # Capture stderr to see RDKit errors\n",
    "    stderr_capture = io.StringIO()\n",
    "    original_stderr = sys.stderr\n",
    "    sys.stderr = stderr_capture\n",
    "\n",
    "    for i in range(NUM_TO_DIAGNOSE):\n",
    "        mol = Chem.RWMol()\n",
    "        atom_map = {}\n",
    "        for j in range(MAX_NODES):\n",
    "            atom_idx = x_indices[i, j].item()\n",
    "            atom = Chem.Atom(ATOM_CLASSES[atom_idx])\n",
    "            atom_map[j] = mol.AddAtom(atom)\n",
    "        for j in range(MAX_NODES):\n",
    "            for k in range(j + 1, MAX_NODES):\n",
    "                bond_idx = adj_bond_type_idx[i, j, k].item()\n",
    "                if bond_idx != NO_BOND_IDX and 0 <= bond_idx < len(BOND_CLASSES_RDKIT):\n",
    "                    mol.AddBond(atom_map[j], atom_map[k], BOND_CLASSES_RDKIT[bond_idx])\n",
    "        \n",
    "        try:\n",
    "            Chem.SanitizeMol(mol)\n",
    "            smi = Chem.MolToSmiles(mol)\n",
    "            if '.' in smi:\n",
    "                error = \"Disconnected Fragments\"\n",
    "            else:\n",
    "                valid_count += 1\n",
    "                error = \"None (Valid)\"\n",
    "        except Exception:\n",
    "            # Catch the RDKit error from stderr\n",
    "            error_msg = stderr_capture.getvalue().split('\\n')[-2] if stderr_capture.getvalue() else \"Unknown RDKit Error\"\n",
    "            # Simplify common errors for counting\n",
    "            if \"valence\" in error_msg.lower(): error = \"Valence Error\"\n",
    "            elif \"kekulize\" in error_msg.lower(): error = \"Ring/Kekulization Error\"\n",
    "            else: error = \"Other RDKit Error\"\n",
    "\n",
    "        error_counts[error] = error_counts.get(error, 0) + 1\n",
    "    \n",
    "    # Restore stderr\n",
    "    sys.stderr = original_stderr\n",
    "    \n",
    "    print(f\"Total generated: {NUM_TO_DIAGNOSE}\")\n",
    "    print(f\"Valid molecules: {valid_count}\")\n",
    "    print(\"\\nFailure Modes:\")\n",
    "    for error, count in error_counts.items():\n",
    "        print(f\" - {error}: {count}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
