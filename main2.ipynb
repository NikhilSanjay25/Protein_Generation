{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da76c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA is available! GPU will be used for training.\n",
      "PyTorch CUDA Version: 12.1\n",
      "GPU Name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Using device: cuda\n",
      "Opening gzipped FASTA file: DL_ENDSEM__DATASET/chembl_35_blast.fa.gz\n",
      "Loading ProtT5 model... (This may take a moment)\n",
      "Generated protein embedding of shape: torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.utils import softmax\n",
    "from rdkit import Chem\n",
    "from Bio import SeqIO\n",
    "import random\n",
    "import re\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import gzip  # <-- FIX 1: ADDED THIS IMPORT\n",
    "\n",
    "# --- 1. System & Configuration ---\n",
    "\n",
    "# --- Configuration (UPDATE THESE PATHS & ID) ---\n",
    "CHEMPL_DB_PATH = 'DL_ENDSEM__DATASET/chembl_35/chembl_35_sqlite/chembl_35.db'\n",
    "BLAST_FASTA_PATH = 'DL_ENDSEM__DATASET/chembl_35_blast.fa.gz'\n",
    "TARGET_UNIPROT_ID = \"P00533\" # Example: EGFR Kinase\n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "Z_DIM = 100         # Latent noise dimension\n",
    "ATOM_FEAT_DIM = 9   # Atom feature size\n",
    "BOND_FEAT_DIM = 4   # Bond feature size (Single, Double, Triple, Aromatic)\n",
    "EMBED_DIM = 128     # Hidden dimension for the Graph Transformer\n",
    "T_EMBED_DIM = 1024  # Target embedding dimension (from ProtT5)\n",
    "LAMBDA_GP = 10.0    # Gradient Penalty weight\n",
    "MAX_NODES = 30      # Max atoms in generated molecules (for Generator tensor shape)\n",
    "N_CRITIC = 5        # Discriminator training steps per Generator step\n",
    "EPOCHS = 150       # Total epochs\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# --- CUDA Check ---\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ CUDA is available! GPU will be used for training.\")\n",
    "    print(f\"PyTorch CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    print(\"❌ CUDA not found. Running on CPU.\")\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "# --- 2. Real Protein Embedding Generation ---\n",
    "\n",
    "# --- FIX 2: REPLACED THIS FUNCTION ---\n",
    "def load_target_sequence(fasta_path, uniprot_id):\n",
    "    \"\"\"Loads a protein sequence from a gzipped FASTA file.\"\"\"\n",
    "    print(f\"Opening gzipped FASTA file: {fasta_path}\")\n",
    "    try:\n",
    "        # 'rt' = read in text mode\n",
    "        with gzip.open(fasta_path, \"rt\") as handle:\n",
    "            for record in SeqIO.parse(handle, \"fasta\"):\n",
    "                if uniprot_id in record.id or uniprot_id in record.description:\n",
    "                    return str(record.seq)\n",
    "        print(f\"Warning: Could not find sequence for {uniprot_id} in {fasta_path}\")\n",
    "        return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FATAL ERROR: FASTA file not found at {fasta_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL ERROR: Could not read FASTA file. Error: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_protein_embedding(sequence, device):\n",
    "    \"\"\"\n",
    "    Generates a protein embedding using the pre-trained ProtT5 model.\n",
    "    This replaces the \"mock\" random embedding.\n",
    "    \"\"\"\n",
    "    print(\"Loading ProtT5 model... (This may take a moment)\")\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "    model = T5EncoderModel.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc').to(device)\n",
    "    model.eval() # Set to evaluation mode\n",
    "\n",
    "    # Pre-process sequence: add spaces between AAs and handle rare AAs\n",
    "    sequence_preprocessed = \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))\n",
    "    \n",
    "    inputs = tokenizer(sequence_preprocessed, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = model(**inputs).last_hidden_state\n",
    "    \n",
    "    # Average pool the embedding to get a single vector [1, 1024]\n",
    "    protein_vec = embedding.mean(dim=1).squeeze(0) # Squeeze to [1024]\n",
    "    print(f\"Generated protein embedding of shape: {protein_vec.shape}\")\n",
    "    return protein_vec\n",
    "\n",
    "# --- Generate the REAL Target Embedding ---\n",
    "target_seq = load_target_sequence(BLAST_FASTA_PATH, TARGET_UNIPROT_ID)\n",
    "if target_seq is None:\n",
    "    raise ValueError(f\"Target sequence for {TARGET_UNIPROT_ID} not found. Exiting.\")\n",
    "\n",
    "TARGET_EMBED = get_protein_embedding(target_seq, DEVICE)\n",
    "\n",
    "\n",
    "# --- 3. Data Pipeline (Molecules -> Graphs) ---\n",
    "\n",
    "def extract_potent_inhibitors(db_path, uniprot_id, potency_cutoff_nM=100):\n",
    "    \"\"\"\n",
    "    Connects to ChEMBL DB and extracts SMILES for potent inhibitors of a given Uniprot ID.\n",
    "    (Using your robust SQL query)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        sql_query = f\"\"\"\n",
    "        SELECT DISTINCT\n",
    "            cs.canonical_smiles\n",
    "        FROM\n",
    "            activities acts\n",
    "        JOIN assays a ON acts.assay_id = a.assay_id\n",
    "        JOIN target_dictionary td ON a.tid = td.tid\n",
    "        JOIN target_components tc ON td.tid = tc.tid\n",
    "        JOIN component_sequences cseq ON tc.component_id = cseq.component_id\n",
    "        JOIN compound_structures cs ON acts.molregno = cs.molregno\n",
    "        WHERE\n",
    "            cseq.accession = '{uniprot_id}' AND\n",
    "            acts.standard_type = 'IC50' AND\n",
    "            acts.standard_units = 'nM' AND\n",
    "            acts.standard_relation = '=' AND\n",
    "            acts.standard_value <= {potency_cutoff_nM}\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(sql_query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"Found {len(df)} potent inhibitors for UniProt ID {uniprot_id}.\")\n",
    "        return df['canonical_smiles'].unique().tolist()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during database query. This is likely due to missing tables or a critical file path issue.\")\n",
    "        raise RuntimeError(f\"Database Error: {e}. Please ensure the file is the full ChEMBL SQLite dump.\") from e\n",
    "\n",
    "def get_atom_features(atom):\n",
    "    \"\"\"Creates the 9-dimensional atom feature vector.\"\"\"\n",
    "    return [\n",
    "        atom.GetAtomicNum(), atom.GetDegree(), atom.GetFormalCharge(),\n",
    "        int(atom.GetIsAromatic()),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP2),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP3),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP3D),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP3D2)\n",
    "    ]\n",
    "\n",
    "def smiles_to_graph(smiles, target_embed):\n",
    "    \"\"\"Converts SMILES to a PyG Data object.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol: return None\n",
    "    if mol.GetNumAtoms() > MAX_NODES: return None\n",
    "\n",
    "    x = torch.tensor([get_atom_features(atom) for atom in mol.GetAtoms()], dtype=torch.float).to(DEVICE)\n",
    "    \n",
    "    edge_indices, edge_attrs = [], []\n",
    "    bond_types = [Chem.BondType.SINGLE, Chem.BondType.DOUBLE, Chem.BondType.TRIPLE, Chem.BondType.AROMATIC]\n",
    "    \n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        bond_type_oh = [int(bond.GetBondType() == t) for t in bond_types]\n",
    "        edge_indices.extend([[i, j], [j, i]])\n",
    "        edge_attrs.extend([bond_type_oh, bond_type_oh])\n",
    "\n",
    "    if not edge_indices: return None\n",
    "    \n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous().to(DEVICE)\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, \n",
    "                target_embed=target_embed.unsqueeze(0))\n",
    "    return data\n",
    "\n",
    "# --- Data Pipeline Execution ---\n",
    "inhibitor_smiles = extract_potent_inhibitors(CHEMPL_DB_PATH, TARGET_UNIPROT_ID)\n",
    "real_data_list = [smiles_to_graph(s, TARGET_EMBED) for s in inhibitor_smiles]\n",
    "real_data_list = [d for d in real_data_list if d is not None]\n",
    "\n",
    "if not real_data_list:\n",
    "    print(\"FATAL: No valid inhibitor data found. The model cannot be trained.\")\n",
    "    exit()\n",
    "\n",
    "real_loader = DataLoader(real_data_list, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(f\"Prepared {len(real_data_list)} real graph samples for training.\")\n",
    "\n",
    "\n",
    "# --- 4. Model Architecture ---\n",
    "\n",
    "# --- 4.1. Relational Graph Transformer Layer (No Changes) ---\n",
    "class RelationalGraphTransformerLayer(MessagePassing):\n",
    "    \"\"\"Graph Transformer Layer with explicit edge/bond feature integration.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, edge_dim, heads=4, dropout=0.1, **kwargs):\n",
    "        super().__init__(aggr='add', node_dim=0, **kwargs)\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        \n",
    "        self.lin_q = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_k = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_v = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_edge = nn.Linear(edge_dim, heads * out_channels, bias=False) \n",
    "        self.att_coeff = nn.Parameter(torch.Tensor(1, heads, 2 * out_channels)) \n",
    "        self.lin_out = nn.Linear(heads * out_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize attention coefficients (Xavier initialization)\n",
    "        nn.init.xavier_uniform_(self.att_coeff)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        Q = self.lin_q(x).view(-1, self.heads, self.out_channels)\n",
    "        K = self.lin_k(x).view(-1, self.heads, self.out_channels)\n",
    "        V = self.lin_v(x).view(-1, self.heads, self.out_channels)\n",
    "        E = self.lin_edge(edge_attr).view(-1, self.heads, self.out_channels)\n",
    "\n",
    "        out = self.propagate(edge_index, Q=Q, K=K, V=V, E=E)\n",
    "        out = out.view(-1, self.heads * self.out_channels)\n",
    "        out = self.lin_out(out)\n",
    "        return out\n",
    "\n",
    "    def message(self, Q_i, K_j, V_j, E_k, index):\n",
    "        try:\n",
    "            is_empty = (E_k.size(0) == 0)\n",
    "        except AttributeError:\n",
    "            is_empty = True\n",
    "        \n",
    "        if is_empty:\n",
    "            E_bias = torch.zeros(Q_i.size(0), self.heads, 1, device=self.lin_q.weight.device)\n",
    "        else:\n",
    "            E_bias = E_k.mean(dim=-1, keepdim=True) \n",
    "\n",
    "        QK_cat = torch.cat([Q_i, K_j], dim=-1)\n",
    "        e_ij = (QK_cat * self.att_coeff).sum(dim=-1, keepdim=True)\n",
    "        e_ij = e_ij + E_bias\n",
    "        \n",
    "        e_ij = F.leaky_relu(e_ij)\n",
    "        alpha = softmax(e_ij, index)\n",
    "        alpha = self.dropout(alpha)\n",
    "        \n",
    "        return V_j * alpha.view(-1, self.heads, 1)\n",
    "\n",
    "# --- 4.2. Discriminator (No Changes) ---\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, node_features, edge_dim, t_embed_dim, embed_dim, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_d = node_features if i == 0 else embed_dim\n",
    "            self.layers.append(RelationalGraphTransformerLayer(in_d, embed_dim, edge_dim))\n",
    "        self.lin_final = nn.Linear(embed_dim + t_embed_dim, 1)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        t_embed = data.target_embed\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        graph_embed = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Ensure t_embed is [batch_size, t_embed_dim]\n",
    "        if t_embed.dim() > 2:\n",
    "            t_embed = t_embed.squeeze(1) \n",
    "            \n",
    "        final_input = torch.cat([graph_embed, t_embed], dim=1)\n",
    "        \n",
    "        return self.lin_final(final_input).squeeze(1)\n",
    "\n",
    "# --- 4.3. Generator (FIXED: Outputs Logits) ---\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, t_embed_dim, node_features, bond_features, max_nodes=MAX_NODES):\n",
    "        super().__init__()\n",
    "        self.max_nodes = max_nodes\n",
    "        self.node_features = node_features\n",
    "        self.bond_features = bond_features\n",
    "        \n",
    "        self.lin_x = nn.Sequential(\n",
    "            nn.Linear(z_dim + t_embed_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, max_nodes * node_features)\n",
    "        )\n",
    "        self.lin_adj = nn.Sequential(\n",
    "            nn.Linear(z_dim + t_embed_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, max_nodes * max_nodes * bond_features) \n",
    "        )\n",
    "\n",
    "    def forward(self, z, t_embed):\n",
    "        zt = torch.cat([z, t_embed], dim=1)\n",
    "        \n",
    "        x_fake = self.lin_x(zt).view(-1, self.max_nodes, self.node_features)\n",
    "        \n",
    "        # Output raw logits for the adjacency matrix (NO SOFTMAX)\n",
    "        adj_fake_logits = self.lin_adj(zt).view(-1, self.max_nodes, self.max_nodes, self.bond_features)\n",
    "        \n",
    "        # Note: We don't softmax x_fake as it contains continuous features (e.g., atomic num)\n",
    "        \n",
    "        return x_fake, adj_fake_logits\n",
    "\n",
    "# --- Model Initialization ---\n",
    "print(\"Initializing models...\")\n",
    "generator = Generator(Z_DIM, T_EMBED_DIM, ATOM_FEAT_DIM, BOND_FEAT_DIM).to(DEVICE)\n",
    "discriminator = Discriminator(ATOM_FEAT_DIM, BOND_FEAT_DIM, T_EMBED_DIM, EMBED_DIM).to(DEVICE)\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "\n",
    "# --- 5. Training Utilities (FIXED) ---\n",
    "\n",
    "# --- 5.1. Differentiable Graph Conversion (NEW) ---\n",
    "def convert_fake_to_data_differentiable(x_fake_tensor, adj_fake_logits, t_embed_batch, device, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Converts Generator output to PyG Data using Gumbel-Softmax for the G-step.\n",
    "    This IS differentiable, allowing the generator to learn bond formation.\n",
    "    \"\"\"\n",
    "    batch_size = x_fake_tensor.size(0)\n",
    "    data_list = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        num_nodes = MAX_NODES\n",
    "        x_i = x_fake_tensor[i, :, :]\n",
    "        adj_i_logits = adj_fake_logits[i, :, :, :]\n",
    "        \n",
    "        edge_indices, edge_attrs_gumbel = [], []\n",
    "        \n",
    "        for r in range(num_nodes):\n",
    "            for c in range(num_nodes):\n",
    "                if r == c: continue\n",
    "                \n",
    "                # Gumbel-Softmax Sampling (Differentiable \"argmax\")\n",
    "                bond_probs = F.gumbel_softmax(\n",
    "                    adj_i_logits[r, c], \n",
    "                    tau=temperature, \n",
    "                    hard=True\n",
    "                )\n",
    "                \n",
    "                edge_indices.append([r, c])\n",
    "                edge_attrs_gumbel.append(bond_probs)\n",
    "\n",
    "        if not edge_indices: continue\n",
    "\n",
    "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous().to(device)\n",
    "        edge_attr = torch.stack(edge_attrs_gumbel).to(device)\n",
    "\n",
    "        data_list.append(Data(x=x_i, edge_index=edge_index, edge_attr=edge_attr, \n",
    "                              target_embed=t_embed_batch[i].unsqueeze(0)))\n",
    "    \n",
    "    if not data_list: return None\n",
    "    # We return the .dataset attribute for direct use in GP calculation\n",
    "    return DataLoader(data_list, batch_size=batch_size).dataset\n",
    "\n",
    "# --- 5.2. Discrete Graph Conversion (NEW) ---\n",
    "def convert_fake_to_data_discrete(x_fake_tensor, adj_fake_logits, t_embed_batch, device):\n",
    "    \"\"\"\n",
    "    Converts Generator output to PyG Data using .argmax() for the D-step.\n",
    "    This is NOT differentiable and is used when we don't need grads (faster).\n",
    "    \"\"\"\n",
    "    batch_size = x_fake_tensor.size(0)\n",
    "    data_list = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        num_nodes = MAX_NODES\n",
    "        x_i = x_fake_tensor[i, :, :].detach() # Detach all inputs\n",
    "        adj_i_logits = adj_fake_logits[i, :, :, :].detach()\n",
    "        \n",
    "        edge_indices, edge_attrs_list = [], []\n",
    "        \n",
    "        for r in range(num_nodes):\n",
    "            for c in range(num_nodes):\n",
    "                if r == c: continue\n",
    "                \n",
    "                # Hard .argmax() sampling\n",
    "                bond_type_index = adj_i_logits[r, c].argmax().item()\n",
    "                bond_one_hot = F.one_hot(\n",
    "                    torch.tensor(bond_type_index, device=device), \n",
    "                    num_classes=BOND_FEAT_DIM\n",
    "                ).float()\n",
    "                \n",
    "                edge_indices.append([r, c])\n",
    "                edge_attrs_list.append(bond_one_hot)\n",
    "\n",
    "        if not edge_indices: continue\n",
    "        \n",
    "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous().to(device)\n",
    "        edge_attr = torch.stack(edge_attrs_list).to(device)\n",
    "\n",
    "        data_list.append(Data(x=x_i, edge_index=edge_index, edge_attr=edge_attr, \n",
    "                              target_embed=t_embed_batch[i].unsqueeze(0)))\n",
    "    \n",
    "    if not data_list: return None\n",
    "    return DataLoader(data_list, batch_size=batch_size).dataset\n",
    "\n",
    "# --- 5.3. WGAN-GP Gradient Penalty (Preserved User's Fix) ---\n",
    "def calculate_gradient_penalty(discriminator, real_data, fake_data, lambda_gp, device):\n",
    "    \"\"\"Calculates the Gradient Penalty on interpolated node features (X).\"\"\"\n",
    "    \n",
    "    real_x = real_data.x.detach()\n",
    "    fake_x = fake_data.x.detach()\n",
    "    real_x_size = real_x.size(0)\n",
    "    \n",
    "    # --- FIX: Match fake_data.x size to real_data.x size for interpolation ---\n",
    "    if fake_x.size(0) > real_x_size:\n",
    "        fake_x = fake_x[:real_x_size]\n",
    "    elif fake_x.size(0) < real_x_size:\n",
    "        padding = torch.zeros(real_x_size - fake_x.size(0), fake_x.size(1), device=device)\n",
    "        fake_x = torch.cat([fake_x, padding], dim=0)\n",
    "\n",
    "    # 1. Linear Interpolation\n",
    "    alpha = torch.rand(real_x_size, 1).to(device) \n",
    "    interpolated_x = (alpha * real_x) + ((1 - alpha) * fake_x)\n",
    "    interpolated_x.requires_grad_(True)\n",
    "\n",
    "    # 2. Create interpolated Data object\n",
    "    # We use the real data's structure (edges, batch) as a template\n",
    "    interpolated_data = Data(x=interpolated_x, \n",
    "                             edge_index=real_data.edge_index, \n",
    "                             edge_attr=real_data.edge_attr, \n",
    "                             batch=real_data.batch, \n",
    "                             target_embed=real_data.target_embed)\n",
    "\n",
    "    disc_interpolates = discriminator(interpolated_data)\n",
    "    \n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=disc_interpolates, inputs=interpolated_x,\n",
    "        grad_outputs=torch.ones_like(disc_interpolates),\n",
    "        create_graph=True, retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = lambda_gp * ((gradient_norm - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty\n",
    "\n",
    "# --- 6. Main Training Loop (FIXED) ---\n",
    "def run_wgan_gp_training(generator, discriminator, data_loader, epochs, n_critic):\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        g_loss_sum, d_loss_sum = 0, 0\n",
    "        \n",
    "        for batch_idx, real_data in enumerate(data_loader):\n",
    "            # --- THIS IS THE LINE YOU REQUESTED ---\n",
    "            print(f\"--- Epoch {epoch}, Processing Batch {batch_idx+1}/{len(data_loader)} ---\")\n",
    "\n",
    "            real_data = real_data.to(DEVICE)\n",
    "            batch_size = real_data.num_graphs\n",
    "            \n",
    "            # Ensure target_embed_batch is [batch_size, T_EMBED_DIM]\n",
    "            target_embed_batch = real_data.target_embed\n",
    "            if target_embed_batch.dim() > 2:\n",
    "                target_embed_batch = target_embed_batch.view(batch_size, T_EMBED_DIM)\n",
    "\n",
    "            # 1. Train Discriminator (n_critic steps)\n",
    "            for _ in range(n_critic):\n",
    "                optimizer_D.zero_grad()\n",
    "                \n",
    "                # Real Loss\n",
    "                d_real = discriminator(real_data).mean()\n",
    "                \n",
    "                # Fake Loss\n",
    "                z = torch.randn(batch_size, Z_DIM).to(DEVICE)\n",
    "                x_fake, adj_fake_logits = generator(z, target_embed_batch)\n",
    "                \n",
    "                # --- D-step uses DISCRETE (non-differentiable) sampling ---\n",
    "                fake_data_list = convert_fake_to_data_discrete(\n",
    "                    x_fake.detach(), adj_fake_logits.detach(), target_embed_batch, DEVICE\n",
    "                )\n",
    "                if fake_data_list is None: continue\n",
    "                # Convert list of Data objects back into a batched Data object\n",
    "                fake_data_loader = DataLoader(fake_data_list, batch_size=batch_size)\n",
    "                fake_data = next(iter(fake_data_loader)).to(DEVICE)\n",
    "\n",
    "                d_fake = discriminator(fake_data).mean()\n",
    "                \n",
    "                # Gradient Penalty\n",
    "                gp = calculate_gradient_penalty(discriminator, real_data, fake_data, LAMBDA_GP, DEVICE)\n",
    "                \n",
    "                d_loss = - (d_real - d_fake) + gp\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "                d_loss_sum += d_loss.item()\n",
    "            \n",
    "            # 2. Train Generator (1 step)\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size, Z_DIM).to(DEVICE)\n",
    "            x_fake, adj_fake_logits = generator(z, target_embed_batch)\n",
    "            \n",
    "            # --- G-step uses DIFFERENTIABLE Gumbel-Softmax sampling ---\n",
    "            fake_data_list = convert_fake_to_data_differentiable(\n",
    "                x_fake, adj_fake_logits, target_embed_batch, DEVICE\n",
    "            )\n",
    "            if fake_data_list is None: continue\n",
    "            fake_data_loader = DataLoader(fake_data_list, batch_size=batch_size)\n",
    "            fake_data = next(iter(fake_data_loader)).to(DEVICE)\n",
    "            \n",
    "            # Generator Loss\n",
    "            g_loss = - discriminator(fake_data).mean()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            g_loss_sum += g_loss.item()\n",
    "            \n",
    "        avg_d_loss = d_loss_sum / len(data_loader) / n_critic\n",
    "        avg_g_loss = g_loss_sum / len(data_loader)\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} | D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f}\")\n",
    "\n",
    "# --- Execute Training ---\n",
    "print(\"\\n--- Starting WGAN-GP Training ---\")\n",
    "run_wgan_gp_training(generator, discriminator, real_loader, EPOCHS, N_CRITIC) \n",
    "print(\"\\nTraining completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce7cf618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA is available! GPU will be used for training.\n",
      "PyTorch CUDA Version: 12.1\n",
      "GPU Name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Using device: cuda\n",
      "Opening gzipped FASTA file: DL_ENDSEM__DATASET/chembl_35_blast.fa.gz\n",
      "Loading ProtT5 model... (This may take a moment)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated protein embedding of shape: torch.Size([1024])\n",
      "Found 3989 potent inhibitors for UniProt ID P00533.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikhi\\OneDrive\\Attachments\\Desktop\\Git_Repos\\NNDL_endsem\\venv\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 1334 real graph samples for training.\n",
      "Initializing models...\n",
      "\n",
      "--- Starting WGAN-GP Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/21 [00:00<?, ?it/s]c:\\Users\\nikhi\\OneDrive\\Attachments\\Desktop\\Git_Repos\\NNDL_endsem\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ..\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Epoch 1/10: 100%|██████████| 21/21 [00:21<00:00,  1.04s/it, D_Loss=-202.8050, G_Loss=-3.8393]\n",
      "Epoch 2/10: 100%|██████████| 21/21 [00:19<00:00,  1.06it/s, D_Loss=-761.2867, G_Loss=-47.0787]\n",
      "Epoch 3/10: 100%|██████████| 21/21 [00:27<00:00,  1.32s/it, D_Loss=-836.8307, G_Loss=-103.6194]\n",
      "Epoch 4/10: 100%|██████████| 21/21 [00:28<00:00,  1.37s/it, D_Loss=-772.8533, G_Loss=-158.7001]\n",
      "Epoch 5/10: 100%|██████████| 21/21 [00:28<00:00,  1.37s/it, D_Loss=-700.1403, G_Loss=-210.0796]\n",
      "Epoch 6/10: 100%|██████████| 21/21 [00:20<00:00,  1.01it/s, D_Loss=-626.3723, G_Loss=-258.0512]\n",
      "Epoch 7/10: 100%|██████████| 21/21 [00:26<00:00,  1.26s/it, D_Loss=-543.0943, G_Loss=-306.2921]\n",
      "Epoch 8/10: 100%|██████████| 21/21 [00:28<00:00,  1.37s/it, D_Loss=-462.6541, G_Loss=-334.1303]\n",
      "Epoch 9/10: 100%|██████████| 21/21 [00:27<00:00,  1.33s/it, D_Loss=-396.6316, G_Loss=-353.0944]\n",
      "Epoch 10/10: 100%|██████████| 21/21 [00:20<00:00,  1.04it/s, D_Loss=-327.0524, G_Loss=-381.8533]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.utils import softmax\n",
    "from rdkit import Chem\n",
    "from Bio import SeqIO\n",
    "import random\n",
    "import re\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import gzip\n",
    "\n",
    "# --- 1. System & Configuration ---\n",
    "\n",
    "# --- Configuration (UPDATE THESE PATHS & ID) ---\n",
    "CHEMPL_DB_PATH = 'DL_ENDSEM__DATASET/chembl_35/chembl_35_sqlite/chembl_35.db'\n",
    "BLAST_FASTA_PATH = 'DL_ENDSEM__DATASET/chembl_35_blast.fa.gz'\n",
    "TARGET_UNIPROT_ID = \"P00533\" # Example: EGFR Kinase\n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "Z_DIM = 100         # Latent noise dimension\n",
    "ATOM_FEAT_DIM = 9   # Atom feature size\n",
    "BOND_FEAT_DIM = 4   # Bond feature size (Single, Double, Triple, Aromatic)\n",
    "EMBED_DIM = 128     # Hidden dimension for the Graph Transformer\n",
    "T_EMBED_DIM = 1024  # Target embedding dimension (from ProtT5)\n",
    "LAMBDA_GP = 10.0    # Gradient Penalty weight\n",
    "MAX_NODES = 30      # Max atoms in generated molecules (for Generator tensor shape)\n",
    "N_CRITIC = 5        # Discriminator training steps per Generator step\n",
    "EPOCHS = 10         # Total epochs\n",
    "BATCH_SIZE = 64    # Your increased batch size\n",
    "\n",
    "# --- OPTIMIZATION 1: Set num_workers based on your 16-core CPU ---\n",
    "# Use ~half your cores to pre-fetch real data\n",
    "CPU_WORKERS = 4  \n",
    "\n",
    "# --- CUDA Check ---\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ CUDA is available! GPU will be used for training.\")\n",
    "    print(f\"PyTorch CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    print(\"❌ CUDA not found. Running on CPU.\")\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "# --- 2. Real Protein Embedding Generation (No changes) ---\n",
    "\n",
    "def load_target_sequence(fasta_path, uniprot_id):\n",
    "    \"\"\"Loads a protein sequence from a gzipped FASTA file.\"\"\"\n",
    "    print(f\"Opening gzipped FASTA file: {fasta_path}\")\n",
    "    try:\n",
    "        with gzip.open(fasta_path, \"rt\") as handle:\n",
    "            for record in SeqIO.parse(handle, \"fasta\"):\n",
    "                if uniprot_id in record.id or uniprot_id in record.description:\n",
    "                    return str(record.seq)\n",
    "        print(f\"Warning: Could not find sequence for {uniprot_id} in {fasta_path}\")\n",
    "        return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FATAL ERROR: FASTA file not found at {fasta_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL ERROR: Could not read FASTA file. Error: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_protein_embedding(sequence, device):\n",
    "    \"\"\"Generates a protein embedding using the pre-trained ProtT5 model.\"\"\"\n",
    "    print(\"Loading ProtT5 model... (This may take a moment)\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "    model = T5EncoderModel.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc').to(device)\n",
    "    model.eval() \n",
    "\n",
    "    sequence_preprocessed = \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))\n",
    "    inputs = tokenizer(sequence_preprocessed, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = model(**inputs).last_hidden_state\n",
    "    \n",
    "    protein_vec = embedding.mean(dim=1).squeeze(0)\n",
    "    print(f\"Generated protein embedding of shape: {protein_vec.shape}\")\n",
    "    return protein_vec\n",
    "\n",
    "# --- Generate the REAL Target Embedding ---\n",
    "target_seq = load_target_sequence(BLAST_FASTA_PATH, TARGET_UNIPROT_ID)\n",
    "if target_seq is None:\n",
    "    raise ValueError(f\"Target sequence for {TARGET_UNIPROT_ID} not found. Exiting.\")\n",
    "TARGET_EMBED = get_protein_embedding(target_seq, DEVICE)\n",
    "\n",
    "\n",
    "# --- 3. Data Pipeline (Molecules -> Graphs) (No changes) ---\n",
    "\n",
    "def extract_potent_inhibitors(db_path, uniprot_id, potency_cutoff_nM=100):\n",
    "    \"\"\"Connects to ChEMBL DB and extracts SMILES for potent inhibitors.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        sql_query = f\"\"\"\n",
    "        SELECT DISTINCT cs.canonical_smiles\n",
    "        FROM activities acts\n",
    "        JOIN assays a ON acts.assay_id = a.assay_id\n",
    "        JOIN target_dictionary td ON a.tid = td.tid\n",
    "        JOIN target_components tc ON td.tid = tc.tid\n",
    "        JOIN component_sequences cseq ON tc.component_id = cseq.component_id\n",
    "        JOIN compound_structures cs ON acts.molregno = cs.molregno\n",
    "        WHERE\n",
    "            cseq.accession = '{uniprot_id}' AND\n",
    "            acts.standard_type = 'IC50' AND\n",
    "            acts.standard_units = 'nM' AND\n",
    "            acts.standard_relation = '=' AND\n",
    "            acts.standard_value <= {potency_cutoff_nM}\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(sql_query, conn)\n",
    "        conn.close()\n",
    "        print(f\"Found {len(df)} potent inhibitors for UniProt ID {uniprot_id}.\")\n",
    "        return df['canonical_smiles'].unique().tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during database query: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_atom_features(atom):\n",
    "    \"\"\"Creates the 9-dimensional atom feature vector.\"\"\"\n",
    "    return [\n",
    "        atom.GetAtomicNum(), atom.GetDegree(), atom.GetFormalCharge(),\n",
    "        int(atom.GetIsAromatic()),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP2),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP3),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP3D),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP3D2)\n",
    "    ]\n",
    "\n",
    "def smiles_to_graph(smiles, target_embed):\n",
    "    \"\"\"Converts SMILES to a PyG Data object.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol: return None\n",
    "    if mol.GetNumAtoms() > MAX_NODES: return None\n",
    "\n",
    "    x = torch.tensor([get_atom_features(atom) for atom in mol.GetAtoms()], dtype=torch.float)\n",
    "    \n",
    "    edge_indices, edge_attrs = [], []\n",
    "    bond_types = [Chem.BondType.SINGLE, Chem.BondType.DOUBLE, Chem.BondType.TRIPLE, Chem.BondType.AROMATIC]\n",
    "    \n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        bond_type_oh = [int(bond.GetBondType() == t) for t in bond_types]\n",
    "        edge_indices.extend([[i, j], [j, i]])\n",
    "        edge_attrs.extend([bond_type_oh, bond_type_oh])\n",
    "\n",
    "    if not edge_indices: return None\n",
    "    \n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "\n",
    "    # Note: We send to DEVICE in the data loader, not here.\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, \n",
    "                target_embed=target_embed.unsqueeze(0))\n",
    "    return data\n",
    "\n",
    "# --- Data Pipeline Execution ---\n",
    "inhibitor_smiles = extract_potent_inhibitors(CHEMPL_DB_PATH, TARGET_UNIPROT_ID)\n",
    "# Note: We keep data on CPU first, for num_workers to be efficient.\n",
    "real_data_list = [smiles_to_graph(s, TARGET_EMBED.cpu()) for s in inhibitor_smiles]\n",
    "real_data_list = [d for d in real_data_list if d is not None]\n",
    "\n",
    "if not real_data_list:\n",
    "    print(\"FATAL: No valid inhibitor data found. The model cannot be trained.\")\n",
    "    exit()\n",
    "\n",
    "# --- OPTIMIZATION 2: Added num_workers and pin_memory ---\n",
    "real_loader = DataLoader(\n",
    "    real_data_list, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=CPU_WORKERS, \n",
    "    pin_memory=True  # Speeds up CPU-to-GPU data transfer\n",
    ")\n",
    "print(f\"Prepared {len(real_data_list)} real graph samples for training.\")\n",
    "\n",
    "\n",
    "# --- 4. Model Architecture (No changes) ---\n",
    "\n",
    "# --- 4.1. Relational Graph Transformer Layer ---\n",
    "class RelationalGraphTransformerLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, edge_dim, heads=4, dropout=0.1, **kwargs):\n",
    "        super().__init__(aggr='add', node_dim=0, **kwargs)\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.lin_q = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_k = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_v = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_edge = nn.Linear(edge_dim, heads * out_channels, bias=False) \n",
    "        self.att_coeff = nn.Parameter(torch.Tensor(1, heads, 2 * out_channels)) \n",
    "        self.lin_out = nn.Linear(heads * out_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.xavier_uniform_(self.att_coeff)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        Q = self.lin_q(x).view(-1, self.heads, self.out_channels)\n",
    "        K = self.lin_k(x).view(-1, self.heads, self.out_channels)\n",
    "        V = self.lin_v(x).view(-1, self.heads, self.out_channels)\n",
    "        E = self.lin_edge(edge_attr).view(-1, self.heads, self.out_channels)\n",
    "        out = self.propagate(edge_index, Q=Q, K=K, V=V, E=E)\n",
    "        out = out.view(-1, self.heads * self.out_channels)\n",
    "        out = self.lin_out(out)\n",
    "        return out\n",
    "\n",
    "    def message(self, Q_i, K_j, V_j, E_k, index):\n",
    "        try: is_empty = (E_k.size(0) == 0)\n",
    "        except AttributeError: is_empty = True\n",
    "        \n",
    "        if is_empty: E_bias = torch.zeros(Q_i.size(0), self.heads, 1, device=self.lin_q.weight.device)\n",
    "        else: E_bias = E_k.mean(dim=-1, keepdim=True) \n",
    "\n",
    "        QK_cat = torch.cat([Q_i, K_j], dim=-1)\n",
    "        e_ij = (QK_cat * self.att_coeff).sum(dim=-1, keepdim=True)\n",
    "        e_ij = e_ij + E_bias\n",
    "        e_ij = F.leaky_relu(e_ij)\n",
    "        alpha = softmax(e_ij, index)\n",
    "        alpha = self.dropout(alpha)\n",
    "        return V_j * alpha.view(-1, self.heads, 1)\n",
    "\n",
    "# --- 4.2. Discriminator ---\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, node_features, edge_dim, t_embed_dim, embed_dim, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_d = node_features if i == 0 else embed_dim\n",
    "            self.layers.append(RelationalGraphTransformerLayer(in_d, embed_dim, edge_dim))\n",
    "        self.lin_final = nn.Linear(embed_dim + t_embed_dim, 1)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        t_embed = data.target_embed\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        graph_embed = global_mean_pool(x, batch)\n",
    "        if t_embed.dim() > 2: t_embed = t_embed.squeeze(1) \n",
    "        final_input = torch.cat([graph_embed, t_embed], dim=1)\n",
    "        return self.lin_final(final_input).squeeze(1)\n",
    "\n",
    "# --- 4.3. Generator ---\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, t_embed_dim, node_features, bond_features, max_nodes=MAX_NODES):\n",
    "        super().__init__()\n",
    "        self.max_nodes = max_nodes\n",
    "        self.node_features = node_features\n",
    "        self.bond_features = bond_features\n",
    "        self.lin_x = nn.Sequential(nn.Linear(z_dim + t_embed_dim, 256), nn.ReLU(), nn.Linear(256, max_nodes * node_features))\n",
    "        self.lin_adj = nn.Sequential(nn.Linear(z_dim + t_embed_dim, 256), nn.ReLU(), nn.Linear(256, max_nodes * max_nodes * bond_features))\n",
    "\n",
    "    def forward(self, z, t_embed):\n",
    "        zt = torch.cat([z, t_embed], dim=1)\n",
    "        x_fake = self.lin_x(zt).view(-1, self.max_nodes, self.node_features)\n",
    "        adj_fake_logits = self.lin_adj(zt).view(-1, self.max_nodes, self.max_nodes, self.bond_features)\n",
    "        return x_fake, adj_fake_logits\n",
    "\n",
    "# --- Model Initialization ---\n",
    "print(\"Initializing models...\")\n",
    "generator = Generator(Z_DIM, T_EMBED_DIM, ATOM_FEAT_DIM, BOND_FEAT_DIM).to(DEVICE)\n",
    "discriminator = Discriminator(ATOM_FEAT_DIM, BOND_FEAT_DIM, T_EMBED_DIM, EMBED_DIM).to(DEVICE)\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "\n",
    "# --- 5. Training Utilities (FIXED & VECTORIZED) ---\n",
    "\n",
    "# --- OPTIMIZATION 3: Create a helper for the dense edge_index template ---\n",
    "# We compute this once on CPU and move it to GPU,\n",
    "# so it's not recomputed every batch.\n",
    "N = MAX_NODES\n",
    "# Create a [N, N] adjacency matrix with 1s everywhere except the diagonal\n",
    "adj_template = (torch.ones(N, N) - torch.eye(N)).bool()\n",
    "# Convert to sparse edge_index format [2, N*(N-1)]\n",
    "EDGE_INDEX_TEMPLATE = adj_template.nonzero(as_tuple=False).t().contiguous().to(DEVICE)\n",
    "# This template has N*(N-1) edges\n",
    "NUM_DENSE_EDGES = EDGE_INDEX_TEMPLATE.size(1)\n",
    "\n",
    "# --- OPTIMIZATION 4: Vectorized Fake Graph Generation ---\n",
    "# This one function replaces both old convert_fake_to_data functions\n",
    "# It's fully vectorized and runs on the GPU. No Python loops!\n",
    "\n",
    "def convert_fake_to_data_vectorized(x_fake_tensor, adj_fake_logits, t_embed_batch, device, gumbel=False, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Converts Generator output to a single batched PyG Data object *on the GPU*.\n",
    "    This is the core optimization, replacing the slow Python loops.\n",
    "    \"\"\"\n",
    "    batch_size, num_nodes, _ = x_fake_tensor.shape\n",
    "    \n",
    "    # 1. Create Batched Node Features (x)\n",
    "    # Reshape [B, N, F] -> [B*N, F]\n",
    "    x_batched = x_fake_tensor.reshape(batch_size * num_nodes, -1)\n",
    "    \n",
    "    # 2. Create Batched Batch Index (batch)\n",
    "    # Create [0, 0, ..., 1, 1, ..., B-1, B-1]\n",
    "    batch_vec = torch.arange(batch_size, device=device).repeat_interleave(num_nodes)\n",
    "    \n",
    "    # 3. Create Batched Edge Index (edge_index)\n",
    "    # Repeat the [2, N*(N-1)] template B times\n",
    "    edge_index_batched = EDGE_INDEX_TEMPLATE.repeat(1, batch_size)\n",
    "    # Create offsets: [0, 0, ..., N, N, ..., 2N, 2N, ...]\n",
    "    offset = torch.arange(0, batch_size * num_nodes, num_nodes, device=device).repeat_interleave(NUM_DENSE_EDGES)\n",
    "    # Add offsets to create the full [2, B*N*(N-1)] edge_index\n",
    "    edge_index_batched = edge_index_batched + offset\n",
    "    \n",
    "    # 4. Create Batched Edge Attributes (edge_attr)\n",
    "    # We need to gather the logits corresponding to our new edge_index\n",
    "    # Get batch, row, and column indices from the batched edge_index\n",
    "    batch_indices = edge_index_batched[0] // num_nodes\n",
    "    row_indices = edge_index_batched[0] % num_nodes\n",
    "    col_indices = edge_index_batched[1] % num_nodes\n",
    "    \n",
    "    # Gather the [B*N*(N-1), Bonds] logits\n",
    "    adj_logits_flat = adj_fake_logits[batch_indices, row_indices, col_indices]\n",
    "    \n",
    "    # 5. Sample edge attributes\n",
    "    if gumbel:\n",
    "        # Differentiable Gumbel-Softmax for Generator step\n",
    "        edge_attr_batched = F.gumbel_softmax(adj_logits_flat, tau=temperature, hard=True)\n",
    "    else:\n",
    "        # Non-differentiable argmax for Discriminator step\n",
    "        bond_indices = torch.argmax(adj_logits_flat, dim=-1)\n",
    "        edge_attr_batched = F.one_hot(bond_indices, num_classes=BOND_FEAT_DIM).float()\n",
    "        \n",
    "    # 6. Create the single, batched Data object\n",
    "    fake_data = Data(\n",
    "        x=x_batched,\n",
    "        edge_index=edge_index_batched,\n",
    "        edge_attr=edge_attr_batched,\n",
    "        batch=batch_vec,\n",
    "        target_embed=t_embed_batch  # Already [B, T_EMBED_DIM]\n",
    "    )\n",
    "    return fake_data\n",
    "\n",
    "\n",
    "# --- 5.3. WGAN-GP Gradient Penalty (No changes) ---\n",
    "def calculate_gradient_penalty(discriminator, real_data, fake_data, lambda_gp, device):\n",
    "    \"\"\"Calculates the Gradient Penalty on interpolated node features (X).\"\"\"\n",
    "    real_x = real_data.x.detach()\n",
    "    fake_x = fake_data.x.detach()\n",
    "    real_x_size = real_x.size(0)\n",
    "    \n",
    "    if fake_x.size(0) > real_x_size:\n",
    "        fake_x = fake_x[:real_x_size]\n",
    "    elif fake_x.size(0) < real_x_size:\n",
    "        padding = torch.zeros(real_x_size - fake_x.size(0), fake_x.size(1), device=device)\n",
    "        fake_x = torch.cat([fake_x, padding], dim=0)\n",
    "\n",
    "    alpha = torch.rand(real_x_size, 1).to(device) \n",
    "    interpolated_x = (alpha * real_x) + ((1 - alpha) * fake_x)\n",
    "    interpolated_x.requires_grad_(True)\n",
    "\n",
    "    interpolated_data = Data(x=interpolated_x, \n",
    "                             edge_index=real_data.edge_index, \n",
    "                             edge_attr=real_data.edge_attr, \n",
    "                             batch=real_data.batch, \n",
    "                             target_embed=real_data.target_embed)\n",
    "\n",
    "    disc_interpolates = discriminator(interpolated_data)\n",
    "    \n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=disc_interpolates, inputs=interpolated_x,\n",
    "        grad_outputs=torch.ones_like(disc_interpolates),\n",
    "        create_graph=True, retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = lambda_gp * ((gradient_norm - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty\n",
    "\n",
    "# --- 6. Main Training Loop (FIXED & OPTIMIZED) ---\n",
    "def run_wgan_gp_training(generator, discriminator, data_loader, epochs, n_critic):\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        g_loss_sum, d_loss_sum = 0, 0\n",
    "        \n",
    "        # Use a for loop that automatically prints progress\n",
    "        from tqdm import tqdm\n",
    "        progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "        \n",
    "        for batch_idx, real_data in enumerate(progress_bar):\n",
    "            # OPTIMIZATION: Move real_data to GPU here\n",
    "            real_data = real_data.to(DEVICE)\n",
    "            batch_size = real_data.num_graphs\n",
    "            \n",
    "            # target_embed is already [B, T_EMBED_DIM] from the loader\n",
    "            target_embed_batch = real_data.target_embed\n",
    "\n",
    "            # 1. Train Discriminator (n_critic steps)\n",
    "            for _ in range(n_critic):\n",
    "                optimizer_D.zero_grad()\n",
    "                \n",
    "                # Real Loss\n",
    "                d_real = discriminator(real_data).mean()\n",
    "                \n",
    "                # Fake Loss\n",
    "                z = torch.randn(batch_size, Z_DIM).to(DEVICE)\n",
    "                x_fake, adj_fake_logits = generator(z, target_embed_batch)\n",
    "                \n",
    "                # --- D-step: Use VECTORIZED function (non-differentiable) ---\n",
    "                with torch.no_grad(): # Ensure no grads are computed here\n",
    "                    fake_data = convert_fake_to_data_vectorized(\n",
    "                        x_fake.detach(), adj_fake_logits.detach(), target_embed_batch, DEVICE, gumbel=False\n",
    "                    )\n",
    "                \n",
    "                d_fake = discriminator(fake_data).mean()\n",
    "                \n",
    "                # Gradient Penalty\n",
    "                gp = calculate_gradient_penalty(discriminator, real_data, fake_data, LAMBDA_GP, DEVICE)\n",
    "                \n",
    "                d_loss = - (d_real - d_fake) + gp\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "                d_loss_sum += d_loss.item()\n",
    "            \n",
    "            # 2. Train Generator (1 step)\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size, Z_DIM).to(DEVICE)\n",
    "            x_fake, adj_fake_logits = generator(z, target_embed_batch)\n",
    "            \n",
    "            # --- G-step: Use VECTORIZED function (DIFFERENTIABLE) ---\n",
    "            fake_data = convert_fake_to_data_vectorized(\n",
    "                x_fake, adj_fake_logits, target_embed_batch, DEVICE, gumbel=True\n",
    "            )\n",
    "            \n",
    "            # Generator Loss\n",
    "            g_loss = - discriminator(fake_data).mean()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            g_loss_sum += g_loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(\n",
    "                D_Loss=f\"{(d_loss_sum / (batch_idx+1) / n_critic):.4f}\", \n",
    "                G_Loss=f\"{(g_loss_sum / (batch_idx+1)):.4f}\"\n",
    "            )\n",
    "            \n",
    "        avg_d_loss = d_loss_sum / len(data_loader) / n_critic\n",
    "        avg_g_loss = g_loss_sum / len(data_loader)\n",
    "        # TQDM handles the epoch printout\n",
    "        # print(f\"Epoch {epoch}/{EPOCHS} | D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f}\")\n",
    "\n",
    "# --- Execute Training ---\n",
    "print(\"\\n--- Starting WGAN-GP Training ---\")\n",
    "# Add 'tqdm' to your environment: pip install tqdm\n",
    "run_wgan_gp_training(generator, discriminator, real_loader, EPOCHS, N_CRITIC) \n",
    "print(\"\\nTraining completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "445467cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA is available! GPU will be used for training.\n",
      "PyTorch CUDA Version: 12.1\n",
      "GPU Name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Using device: cuda\n",
      "Opening gzipped FASTA file: DL_ENDSEM__DATASET/chembl_35_blast.fa.gz\n",
      "Loading ProtT5 model... (This may take a moment)\n",
      "Generated protein embedding of shape: torch.Size([1024])\n",
      "Found 3989 potent inhibitors for UniProt ID P00533.\n",
      "Prepared 1332 real graph samples for training.\n",
      "Initializing models...\n",
      "\n",
      "--- Starting WGAN-GP Training (FIXED) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  52%|█████▏    | 11/21 [00:30<00:28,  2.81s/it, D_Loss=7.2260, G_Loss=0.0266]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 669\u001b[39m\n\u001b[32m    665\u001b[39m \u001b[38;5;66;03m# --- 9. --- Main Execution (Train & Evaluate) ---\u001b[39;00m\n\u001b[32m    666\u001b[39m \n\u001b[32m    667\u001b[39m \u001b[38;5;66;03m# --- Execute Training ---\u001b[39;00m\n\u001b[32m    668\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Starting WGAN-GP Training (FIXED) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[43mrun_wgan_gp_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_CRITIC\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m    670\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining completed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    672\u001b[39m \u001b[38;5;66;03m# --- Execute Evaluation ---\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 447\u001b[39m, in \u001b[36mrun_wgan_gp_training\u001b[39m\u001b[34m(generator, discriminator, data_loader, epochs, n_critic)\u001b[39m\n\u001b[32m    445\u001b[39m     d_loss.backward()\n\u001b[32m    446\u001b[39m     optimizer_D.step()\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m     d_loss_sum += \u001b[43md_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[38;5;66;03m# 2. Train Generator (1 step)\u001b[39;00m\n\u001b[32m    450\u001b[39m optimizer_G.zero_grad()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.utils import softmax\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import Descriptors, QED, AllChem\n",
    "from Bio import SeqIO\n",
    "import random\n",
    "import re\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# --- 1. System & Configuration ---\n",
    "\n",
    "# --- Configuration (UPDATE THESE PATHS & ID) ---\n",
    "CHEMPL_DB_PATH = 'DL_ENDSEM__DATASET/chembl_35/chembl_35_sqlite/chembl_35.db'\n",
    "BLAST_FASTA_PATH = 'DL_ENDSEM__DATASET/chembl_35_blast.fa.gz'\n",
    "TARGET_UNIPROT_ID = \"P00533\" # Example: EGFR Kinase\n",
    "\n",
    "# --- CHANGED: Define Atom Classes ---\n",
    "# We will now classify atoms instead of regressing 9 features\n",
    "# These are the atoms our model is allowed to generate.\n",
    "ATOM_CLASSES = [6, 7, 8, 9, 15, 16, 17, 35, 53] # C, N, O, F, P, S, Cl, Br, I\n",
    "ATOM_CLASSES_MAP = {num: i for i, num in enumerate(ATOM_CLASSES)} # Helper map\n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "Z_DIM = 100         # Latent noise dimension\n",
    "ATOM_FEAT_DIM = len(ATOM_CLASSES) # --- CHANGED: Now 9 (for 9 classes) ---\n",
    "BOND_FEAT_DIM = 4   # Bond feature size (Single, Double, Triple, Aromatic)\n",
    "EMBED_DIM = 128     # Hidden dimension for the Graph Transformer\n",
    "T_EMBED_DIM = 1024  # Target embedding dimension (from ProtT5)\n",
    "LAMBDA_GP = 10.0    # Gradient Penalty weight\n",
    "MAX_NODES = 30      # Max atoms in generated molecules (for Generator tensor shape)\n",
    "N_CRITIC = 5        # Discriminator training steps per Generator step\n",
    "EPOCHS = 100        # Total epochs\n",
    "BATCH_SIZE = 64     # Your increased batch size\n",
    "\n",
    "# --- OPTIMIZATION 1: Set num_workers based on your 16-core CPU ---\n",
    "CPU_WORKERS = 4     \n",
    "\n",
    "# --- NEW: Constants for Generation ---\n",
    "BOND_TYPES_RDKIT = [Chem.BondType.SINGLE, Chem.BondType.DOUBLE, Chem.BondType.TRIPLE, Chem.BondType.AROMATIC]\n",
    "\n",
    "# --- CUDA Check ---\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ CUDA is available! GPU will be used for training.\")\n",
    "    print(f\"PyTorch CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    print(\"❌ CUDA not found. Running on CPU.\")\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "# --- 2. Real Protein Embedding Generation (No changes) ---\n",
    "\n",
    "def load_target_sequence(fasta_path, uniprot_id):\n",
    "    \"\"\"Loads a protein sequence from a gzipped FASTA file.\"\"\"\n",
    "    print(f\"Opening gzipped FASTA file: {fasta_path}\")\n",
    "    try:\n",
    "        with gzip.open(fasta_path, \"rt\") as handle:\n",
    "            for record in SeqIO.parse(handle, \"fasta\"):\n",
    "                if uniprot_id in record.id or uniprot_id in record.description:\n",
    "                    return str(record.seq)\n",
    "            print(f\"Warning: Could not find sequence for {uniprot_id} in {fasta_path}\")\n",
    "            return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FATAL ERROR: FASTA file not found at {fasta_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL ERROR: Could not read FASTA file. Error: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_protein_embedding(sequence, device):\n",
    "    \"\"\"Generates a protein embedding using the pre-trained ProtT5 model.\"\"\"\n",
    "    print(\"Loading ProtT5 model... (This may take a moment)\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "    model = T5EncoderModel.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc').to(device)\n",
    "    model.eval() \n",
    "\n",
    "    sequence_preprocessed = \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))\n",
    "    inputs = tokenizer(sequence_preprocessed, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = model(**inputs).last_hidden_state\n",
    "    \n",
    "    protein_vec = embedding.mean(dim=1).squeeze(0)\n",
    "    print(f\"Generated protein embedding of shape: {protein_vec.shape}\")\n",
    "    return protein_vec\n",
    "\n",
    "# --- Generate the REAL Target Embedding ---\n",
    "target_seq = load_target_sequence(BLAST_FASTA_PATH, TARGET_UNIPROT_ID)\n",
    "if target_seq is None:\n",
    "    raise ValueError(f\"Target sequence for {TARGET_UNIPROT_ID} not found. Exiting.\")\n",
    "TARGET_EMBED = get_protein_embedding(target_seq, DEVICE)\n",
    "\n",
    "\n",
    "# --- 3. Data Pipeline (Molecules -> Graphs) (CHANGES) ---\n",
    "\n",
    "def extract_potent_inhibitors(db_path, uniprot_id, potency_cutoff_nM=100):\n",
    "    \"\"\"Connects to ChEMBL DB and extracts SMILES for potent inhibitors.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        sql_query = f\"\"\"\n",
    "        SELECT DISTINCT cs.canonical_smiles\n",
    "        FROM activities acts\n",
    "        JOIN assays a ON acts.assay_id = a.assay_id\n",
    "        JOIN target_dictionary td ON a.tid = td.tid\n",
    "        JOIN target_components tc ON td.tid = tc.tid\n",
    "        JOIN component_sequences cseq ON tc.component_id = cseq.component_id\n",
    "        JOIN compound_structures cs ON acts.molregno = cs.molregno\n",
    "        WHERE\n",
    "            cseq.accession = '{uniprot_id}' AND\n",
    "            acts.standard_type = 'IC50' AND\n",
    "            acts.standard_units = 'nM' AND\n",
    "            acts.standard_relation = '=' AND\n",
    "            acts.standard_value <= {potency_cutoff_nM}\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(sql_query, conn)\n",
    "        conn.close()\n",
    "        print(f\"Found {len(df)} potent inhibitors for UniProt ID {uniprot_id}.\")\n",
    "        return df['canonical_smiles'].unique().tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during database query: {e}\")\n",
    "        raise\n",
    "\n",
    "# --- CHANGED: get_atom_features ---\n",
    "def get_atom_features(atom):\n",
    "    \"\"\"Creates a one-hot vector for the atom type.\"\"\"\n",
    "    atom_num = atom.GetAtomicNum()\n",
    "    if atom_num not in ATOM_CLASSES_MAP:\n",
    "        return None # Atom is not in our allowed list\n",
    "        \n",
    "    atom_index = ATOM_CLASSES_MAP[atom_num]\n",
    "    # Create a one-hot vector\n",
    "    atom_one_hot = torch.zeros(ATOM_FEAT_DIM, dtype=torch.float)\n",
    "    atom_one_hot[atom_index] = 1.0\n",
    "    return atom_one_hot\n",
    "\n",
    "# --- CHANGED: smiles_to_graph ---\n",
    "def smiles_to_graph(smiles, target_embed):\n",
    "    \"\"\"Converts SMILES to a PyG Data object.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol: return None\n",
    "    if mol.GetNumAtoms() > MAX_NODES: return None\n",
    "\n",
    "    atom_features_list = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        features = get_atom_features(atom)\n",
    "        if features is None: # Skip molecule if it contains an invalid atom\n",
    "            return None\n",
    "        atom_features_list.append(features)\n",
    "\n",
    "    # --- CHANGED: Stack the one-hot vectors ---\n",
    "    x = torch.stack(atom_features_list)\n",
    "    \n",
    "    edge_indices, edge_attrs = [], []\n",
    "    bond_types = [Chem.BondType.SINGLE, Chem.BondType.DOUBLE, Chem.BondType.TRIPLE, Chem.BondType.AROMATIC]\n",
    "    \n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        bond_type_oh = [int(bond.GetBondType() == t) for t in bond_types]\n",
    "        edge_indices.extend([[i, j], [j, i]])\n",
    "        edge_attrs.extend([bond_type_oh, bond_type_oh])\n",
    "\n",
    "    if not edge_indices: return None\n",
    "    \n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "\n",
    "    # Note: We send to DEVICE in the data loader, not here.\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, \n",
    "                target_embed=target_embed.unsqueeze(0))\n",
    "    return data\n",
    "\n",
    "# --- Data Pipeline Execution ---\n",
    "inhibitor_smiles = extract_potent_inhibitors(CHEMPL_DB_PATH, TARGET_UNIPROT_ID)\n",
    "# Note: We keep data on CPU first, for num_workers to be efficient.\n",
    "real_data_list = [smiles_to_graph(s, TARGET_EMBED.cpu()) for s in inhibitor_smiles]\n",
    "real_data_list = [d for d in real_data_list if d is not None]\n",
    "\n",
    "if not real_data_list:\n",
    "    print(f\"FATAL: No valid inhibitor data found (or all were filtered out). Check ATOM_CLASSES.\")\n",
    "    exit()\n",
    "\n",
    "# --- OPTIMIZATION 2: Added num_workers and pin_memory ---\n",
    "real_loader = DataLoader(\n",
    "    real_data_list, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=CPU_WORKERS, \n",
    "    pin_memory=True  # Speeds up CPU-to-GPU data transfer\n",
    ")\n",
    "print(f\"Prepared {len(real_data_list)} real graph samples for training.\")\n",
    "\n",
    "\n",
    "# --- 4. Model Architecture (No changes) ---\n",
    "# The models are already correctly parameterized to accept ATOM_FEAT_DIM.\n",
    "# The *meaning* of ATOM_FEAT_DIM has changed, but the *shape* is the same.\n",
    "\n",
    "# --- 4.1. Relational Graph Transformer Layer ---\n",
    "class RelationalGraphTransformerLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, edge_dim, heads=4, dropout=0.1, **kwargs):\n",
    "        super().__init__(aggr='add', node_dim=0, **kwargs)\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.lin_q = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_k = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_v = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_edge = nn.Linear(edge_dim, heads * out_channels, bias=False) \n",
    "        self.att_coeff = nn.Parameter(torch.Tensor(1, heads, 2 * out_channels)) \n",
    "        self.lin_out = nn.Linear(heads * out_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.xavier_uniform_(self.att_coeff)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        Q = self.lin_q(x).view(-1, self.heads, self.out_channels)\n",
    "        K = self.lin_k(x).view(-1, self.heads, self.out_channels)\n",
    "        V = self.lin_v(x).view(-1, self.heads, self.out_channels)\n",
    "        E = self.lin_edge(edge_attr).view(-1, self.heads, self.out_channels)\n",
    "        out = self.propagate(edge_index, Q=Q, K=K, V=V, E=E)\n",
    "        out = out.view(-1, self.heads * self.out_channels)\n",
    "        out = self.lin_out(out)\n",
    "        return out\n",
    "\n",
    "    def message(self, Q_i, K_j, V_j, E_k, index):\n",
    "        try: is_empty = (E_k.size(0) == 0)\n",
    "        except AttributeError: is_empty = True\n",
    "        \n",
    "        if is_empty: E_bias = torch.zeros(Q_i.size(0), self.heads, 1, device=self.lin_q.weight.device)\n",
    "        else: E_bias = E_k.mean(dim=-1, keepdim=True) \n",
    "\n",
    "        QK_cat = torch.cat([Q_i, K_j], dim=-1)\n",
    "        e_ij = (QK_cat * self.att_coeff).sum(dim=-1, keepdim=True)\n",
    "        e_ij = e_ij + E_bias\n",
    "        e_ij = F.leaky_relu(e_ij)\n",
    "        alpha = softmax(e_ij, index)\n",
    "        alpha = self.dropout(alpha)\n",
    "        return V_j * alpha.view(-1, self.heads, 1)\n",
    "\n",
    "# --- 4.2. Discriminator ---\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, node_features, edge_dim, t_embed_dim, embed_dim, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_d = node_features if i == 0 else embed_dim\n",
    "            self.layers.append(RelationalGraphTransformerLayer(in_d, embed_dim, edge_dim))\n",
    "        self.lin_final = nn.Linear(embed_dim + t_embed_dim, 1)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        t_embed = data.target_embed\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        graph_embed = global_mean_pool(x, batch)\n",
    "        if t_embed.dim() > 2: t_embed = t_embed.squeeze(1) \n",
    "        final_input = torch.cat([graph_embed, t_embed], dim=1)\n",
    "        return self.lin_final(final_input).squeeze(1)\n",
    "\n",
    "# --- 4.3. Generator ---\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, t_embed_dim, node_features, bond_features, max_nodes=MAX_NODES):\n",
    "        super().__init__()\n",
    "        self.max_nodes = max_nodes\n",
    "        self.node_features = node_features\n",
    "        self.bond_features = bond_features\n",
    "        # --- CHANGED: lin_x now outputs logits for atom classes ---\n",
    "        self.lin_x = nn.Sequential(nn.Linear(z_dim + t_embed_dim, 256), nn.ReLU(), nn.Linear(256, max_nodes * node_features))\n",
    "        self.lin_adj = nn.Sequential(nn.Linear(z_dim + t_embed_dim, 256), nn.ReLU(), nn.Linear(256, max_nodes * max_nodes * bond_features))\n",
    "\n",
    "    def forward(self, z, t_embed):\n",
    "        zt = torch.cat([z, t_embed], dim=1)\n",
    "        # --- CHANGED: x_fake is now x_fake_logits ---\n",
    "        x_fake_logits = self.lin_x(zt).view(-1, self.max_nodes, self.node_features)\n",
    "        adj_fake_logits = self.lin_adj(zt).view(-1, self.max_nodes, self.max_nodes, self.bond_features)\n",
    "        return x_fake_logits, adj_fake_logits\n",
    "\n",
    "# --- Model Initialization ---\n",
    "print(\"Initializing models...\")\n",
    "generator = Generator(Z_DIM, T_EMBED_DIM, ATOM_FEAT_DIM, BOND_FEAT_DIM).to(DEVICE)\n",
    "discriminator = Discriminator(ATOM_FEAT_DIM, BOND_FEAT_DIM, T_EMBED_DIM, EMBED_DIM).to(DEVICE)\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "\n",
    "# --- 5. Training Utilities (CHANGES) ---\n",
    "\n",
    "# --- OPTIMIZATION 3: Create a helper for the dense edge_index template ---\n",
    "N = MAX_NODES\n",
    "adj_template = (torch.ones(N, N) - torch.eye(N)).bool()\n",
    "EDGE_INDEX_TEMPLATE = adj_template.nonzero(as_tuple=False).t().contiguous().to(DEVICE)\n",
    "NUM_DENSE_EDGES = EDGE_INDEX_TEMPLATE.size(1)\n",
    "\n",
    "# --- OPTIMIZATION 4: Vectorized Fake Graph Generation ---\n",
    "# --- CHANGED: Now applies Gumbel-Softmax to node features (x) as well ---\n",
    "def convert_fake_to_data_vectorized(x_fake_logits, adj_fake_logits, t_embed_batch, device, gumbel=False, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Converts Generator output (logits) to a single batched PyG Data object.\n",
    "    Applies Gumbel-Softmax to both nodes and edges.\n",
    "    \"\"\"\n",
    "    batch_size, num_nodes, _ = x_fake_logits.shape\n",
    "    \n",
    "    # 1. Create Batched Node Features (x)\n",
    "    # --- CHANGED: Apply Gumbel-Softmax to node logits ---\n",
    "    if gumbel:\n",
    "        x_fake_tensor = F.gumbel_softmax(x_fake_logits, tau=temperature, hard=True)\n",
    "    else:\n",
    "        # For discriminator, use discrete argmax\n",
    "        x_indices = torch.argmax(x_fake_logits, dim=-1)\n",
    "        x_fake_tensor = F.one_hot(x_indices, num_classes=ATOM_FEAT_DIM).float()\n",
    "    \n",
    "    # Reshape [B, N, F] -> [B*N, F]\n",
    "    x_batched = x_fake_tensor.reshape(batch_size * num_nodes, -1)\n",
    "    \n",
    "    # 2. Create Batched Batch Index (batch)\n",
    "    batch_vec = torch.arange(batch_size, device=device).repeat_interleave(num_nodes)\n",
    "    \n",
    "    # 3. Create Batched Edge Index (edge_index)\n",
    "    edge_index_batched = EDGE_INDEX_TEMPLATE.repeat(1, batch_size)\n",
    "    offset = torch.arange(0, batch_size * num_nodes, num_nodes, device=device).repeat_interleave(NUM_DENSE_EDGES)\n",
    "    edge_index_batched = edge_index_batched + offset\n",
    "    \n",
    "    # 4. Create Batched Edge Attributes (edge_attr)\n",
    "    batch_indices = edge_index_batched[0] // num_nodes\n",
    "    row_indices = edge_index_batched[0] % num_nodes\n",
    "    col_indices = edge_index_batched[1] % num_nodes\n",
    "    \n",
    "    adj_logits_flat = adj_fake_logits[batch_indices, row_indices, col_indices]\n",
    "    \n",
    "    # 5. Sample edge attributes\n",
    "    if gumbel:\n",
    "        # Differentiable Gumbel-Softmax for Generator step\n",
    "        edge_attr_batched = F.gumbel_softmax(adj_logits_flat, tau=temperature, hard=True)\n",
    "    else:\n",
    "        # Non-differentiable argmax for Discriminator step\n",
    "        bond_indices = torch.argmax(adj_logits_flat, dim=-1)\n",
    "        edge_attr_batched = F.one_hot(bond_indices, num_classes=BOND_FEAT_DIM).float()\n",
    "        \n",
    "    # 6. Create the single, batched Data object\n",
    "    fake_data = Data(\n",
    "        x=x_batched,\n",
    "        edge_index=edge_index_batched,\n",
    "        edge_attr=edge_attr_batched,\n",
    "        batch=batch_vec,\n",
    "        target_embed=t_embed_batch \n",
    "    )\n",
    "    return fake_data\n",
    "\n",
    "\n",
    "# --- 5.3. WGAN-GP Gradient Penalty (No changes) ---\n",
    "def calculate_gradient_penalty(discriminator, real_data, fake_data, lambda_gp, device):\n",
    "    \"\"\"Calculates the Gradient Penalty on interpolated node features (X).\"\"\"\n",
    "    real_x = real_data.x.detach()\n",
    "    fake_x = fake_data.x.detach()\n",
    "    real_x_size = real_x.size(0)\n",
    "    \n",
    "    if fake_x.size(0) > real_x_size:\n",
    "        fake_x = fake_x[:real_x_size]\n",
    "    elif fake_x.size(0) < real_x_size:\n",
    "        padding = torch.zeros(real_x_size - fake_x.size(0), fake_x.size(1), device=device)\n",
    "        fake_x = torch.cat([fake_x, padding], dim=0)\n",
    "\n",
    "    # --- CHANGED: Alpha must now match the dimensions of x ---\n",
    "    alpha = torch.rand(real_x_size, 1).to(device) \n",
    "    # Ensure alpha broadcasts correctly: [real_x_size, 1]\n",
    "    \n",
    "    interpolated_x = (alpha * real_x) + ((1 - alpha) * fake_x)\n",
    "    interpolated_x.requires_grad_(True)\n",
    "\n",
    "    interpolated_data = Data(x=interpolated_x, \n",
    "                             edge_index=real_data.edge_index, \n",
    "                             edge_attr=real_data.edge_attr, \n",
    "                             batch=real_data.batch, \n",
    "                             target_embed=real_data.target_embed)\n",
    "\n",
    "    disc_interpolates = discriminator(interpolated_data)\n",
    "    \n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=disc_interpolates, inputs=interpolated_x,\n",
    "        grad_outputs=torch.ones_like(disc_interpolates),\n",
    "        create_graph=True, retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = lambda_gp * ((gradient_norm - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty\n",
    "\n",
    "# --- 6. Main Training Loop (FIXED & OPTIMIZED) ---\n",
    "def run_wgan_gp_training(generator, discriminator, data_loader, epochs, n_critic):\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        g_loss_sum, d_loss_sum = 0, 0\n",
    "        \n",
    "        progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "        \n",
    "        for batch_idx, real_data in enumerate(progress_bar):\n",
    "            real_data = real_data.to(DEVICE)\n",
    "            batch_size = real_data.num_graphs\n",
    "            \n",
    "            target_embed_batch = real_data.target_embed\n",
    "\n",
    "            # 1. Train Discriminator (n_critic steps)\n",
    "            for _ in range(n_critic):\n",
    "                optimizer_D.zero_grad()\n",
    "                \n",
    "                # Real Loss\n",
    "                d_real = discriminator(real_data).mean()\n",
    "                \n",
    "                # Fake Loss\n",
    "                z = torch.randn(batch_size, Z_DIM).to(DEVICE)\n",
    "                # --- CHANGED: Generator now outputs logits ---\n",
    "                x_fake_logits, adj_fake_logits = generator(z, target_embed_batch)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    fake_data = convert_fake_to_data_vectorized(\n",
    "                        x_fake_logits.detach(), adj_fake_logits.detach(), \n",
    "                        target_embed_batch, DEVICE, gumbel=False\n",
    "                    )\n",
    "                \n",
    "                d_fake = discriminator(fake_data).mean()\n",
    "                \n",
    "                gp = calculate_gradient_penalty(discriminator, real_data, fake_data, LAMBDA_GP, DEVICE)\n",
    "                \n",
    "                d_loss = - (d_real - d_fake) + gp\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "                d_loss_sum += d_loss.item()\n",
    "            \n",
    "            # 2. Train Generator (1 step)\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size, Z_DIM).to(DEVICE)\n",
    "            # --- CHANGED: Generator now outputs logits ---\n",
    "            x_fake_logits, adj_fake_logits = generator(z, target_embed_batch)\n",
    "            \n",
    "            # --- G-step: Use VECTORIZED function (DIFFERENTIABLE) ---\n",
    "            fake_data = convert_fake_to_data_vectorized(\n",
    "                x_fake_logits, adj_fake_logits, \n",
    "                target_embed_batch, DEVICE, gumbel=True\n",
    "            )\n",
    "            \n",
    "            g_loss = - discriminator(fake_data).mean()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            g_loss_sum += g_loss.item()\n",
    "            \n",
    "            progress_bar.set_postfix(\n",
    "                D_Loss=f\"{(d_loss_sum / (batch_idx+1) / n_critic):.4f}\", \n",
    "                G_Loss=f\"{(g_loss_sum / (batch_idx+1)):.4f}\"\n",
    "            )\n",
    "\n",
    "# --- 7. --- CHANGED: Generation & SMILES Conversion ---\n",
    "\n",
    "def tensors_to_smiles(x_fake_one_hot, adj_fake_logits, bond_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Converts raw generator tensor output (one-hot nodes) into SMILES strings.\n",
    "    \"\"\"\n",
    "    # --- CHANGED: x_fake is now one-hot, find the class index ---\n",
    "    x_fake_indices = torch.argmax(x_fake_one_hot, dim=-1).cpu().detach()\n",
    "    adj_fake_logits = adj_fake_logits.cpu().detach()\n",
    "    \n",
    "    adj_probs = F.softmax(adj_fake_logits, dim=-1)\n",
    "    adj_bond_probs_max, adj_bond_type_idx = torch.max(adj_probs, dim=-1)\n",
    "    \n",
    "    batch_size = x_fake_indices.size(0)\n",
    "    generated_smiles = []\n",
    "    generated_mols = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        mol = Chem.RWMol()\n",
    "        atom_map = {} # Map from tensor index (0..MAX_NODES-1) to RDKit atom index\n",
    "        \n",
    "        # 1. Add atoms\n",
    "        for j in range(MAX_NODES):\n",
    "            # --- CHANGED: Get atom type from ATOM_CLASSES list ---\n",
    "            atom_idx = x_fake_indices[i, j].item()\n",
    "            atom_num = ATOM_CLASSES[atom_idx]\n",
    "            \n",
    "            # --- CHANGED: No longer need to check validity, but\n",
    "            # we can use atom_num=6 (Carbon) as a \"padding\" atom\n",
    "            # and only add non-Carbon atoms to avoid tiny fragments.\n",
    "            # This is a heuristic. A better way is to learn a \"stop\" token.\n",
    "            # For now, let's just add all atoms.\n",
    "            \n",
    "            atom = Chem.Atom(atom_num)\n",
    "            rdkit_idx = mol.AddAtom(atom)\n",
    "            atom_map[j] = rdkit_idx\n",
    "                \n",
    "        # 2. Add bonds\n",
    "        for j in range(MAX_NODES):\n",
    "            for k in range(j + 1, MAX_NODES):\n",
    "                prob = adj_bond_probs_max[i, j, k].item()\n",
    "                \n",
    "                # Apply threshold to create sparsity\n",
    "                if prob > bond_threshold:\n",
    "                    bond_type_idx = adj_bond_type_idx[i, j, k].item()\n",
    "                    bond_type = BOND_TYPES_RDKIT[bond_type_idx]\n",
    "                    \n",
    "                    mol.AddBond(atom_map[j], atom_map[k], bond_type)\n",
    "        \n",
    "        # 3. Sanitize and Convert\n",
    "        try:\n",
    "            Chem.SanitizeMol(mol)\n",
    "            smi = Chem.MolToSmiles(mol)\n",
    "            # --- CHANGED: Filter out disconnected fragments ---\n",
    "            if '.' in smi:\n",
    "                generated_smiles.append(None) # Invalid fragment\n",
    "                generated_mols.append(None)\n",
    "            else:\n",
    "                generated_smiles.append(smi)\n",
    "                generated_mols.append(mol)\n",
    "        except Exception as e:\n",
    "            generated_smiles.append(None) # Invalid molecule\n",
    "            generated_mols.append(None)\n",
    "\n",
    "    valid_smiles = [s for s in generated_smiles if s is not None]\n",
    "    valid_mols = [m for m in generated_mols if m is not None]\n",
    "    \n",
    "    return valid_smiles, valid_mols, generated_smiles\n",
    "\n",
    "# --- 8. --- NEW: Performance Metrics & Plotting ---\n",
    "\n",
    "def calculate_and_plot_metrics(generator, target_embed, real_smiles_list, num_to_generate, device):\n",
    "    \"\"\"\n",
    "    Generates molecules and calculates Validity, Uniqueness, Novelty,\n",
    "    and plots property distributions.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Generation & Evaluation ---\")\n",
    "    warnings.filterwarnings('ignore', '.*Implicit valence.*') # Suppress RDKit warnings\n",
    "    \n",
    "    generator.eval() # Set generator to evaluation mode\n",
    "    \n",
    "    real_mols = [Chem.MolFromSmiles(s) for s in real_smiles_list]\n",
    "    real_mols = [m for m in real_mols if m is not None]\n",
    "    real_smiles_set = set(real_smiles_list)\n",
    "    \n",
    "    all_valid_smiles = []\n",
    "    all_valid_mols = []\n",
    "    num_generated = 0\n",
    "    total_attempts = 0 # Track total attempts\n",
    "\n",
    "    print(f\"Generating {num_to_generate} *valid* molecules for evaluation...\")\n",
    "    with torch.no_grad():\n",
    "        # --- CHANGED: Loop until we have enough *valid* molecules ---\n",
    "        while len(all_valid_smiles) < num_to_generate:\n",
    "            batch_size = BATCH_SIZE\n",
    "            total_attempts += batch_size\n",
    "\n",
    "            z = torch.randn(batch_size, Z_DIM).to(device)\n",
    "            t_embed_batch = target_embed.unsqueeze(0).repeat(batch_size, 1)\n",
    "            \n",
    "            # --- CHANGED: Generator outputs logits ---\n",
    "            x_fake_logits, adj_fake_logits = generator(z, t_embed_batch)\n",
    "            \n",
    "            # --- CHANGED: Use argmax (not Gumbel) for final generation ---\n",
    "            x_indices = torch.argmax(x_fake_logits, dim=-1)\n",
    "            x_fake_one_hot = F.one_hot(x_indices, num_classes=ATOM_FEAT_DIM).float()\n",
    "            \n",
    "            valid_smiles, valid_mols, _ = tensors_to_smiles(x_fake_one_hot, adj_fake_logits)\n",
    "            \n",
    "            all_valid_smiles.extend(valid_smiles)\n",
    "            all_valid_mols.extend(valid_mols)\n",
    "            \n",
    "            print(f\"Generated: {len(all_valid_smiles)}/{num_to_generate} valid molecules...\", end='\\r')\n",
    "            \n",
    "            if total_attempts > num_to_generate * 50 and not all_valid_smiles:\n",
    "                 print(\"\\nError: Generated too many molecules with 0 validity. Stopping.\")\n",
    "                 break\n",
    "            if total_attempts > num_to_generate * 10: # Safety break\n",
    "                 print(f\"\\nWarning: Low validity. Stopping generation at {len(all_valid_smiles)} molecules.\")\n",
    "                 break\n",
    "\n",
    "\n",
    "    print(\"\\nGeneration complete. Calculating metrics...\")\n",
    "    \n",
    "    # --- 1. Calculate Metrics ---\n",
    "    \n",
    "    # Validity\n",
    "    if total_attempts == 0: total_attempts = 1 # avoid divide by zero\n",
    "    validity = len(all_valid_smiles) / total_attempts\n",
    "    \n",
    "    # Uniqueness\n",
    "    if len(all_valid_smiles) > 0:\n",
    "        uniqueness = len(set(all_valid_smiles)) / len(all_valid_smiles)\n",
    "    else:\n",
    "        uniqueness = 0.0\n",
    "        \n",
    "    # Novelty\n",
    "    if len(all_valid_smiles) > 0:\n",
    "        unique_valid_smiles = set(all_valid_smiles)\n",
    "        novel_smiles = unique_valid_smiles - real_smiles_set\n",
    "        novelty = len(novel_smiles) / len(unique_valid_smiles)\n",
    "    else:\n",
    "        novelty = 0.0\n",
    "\n",
    "    print(\"\\n--- Generative Performance Metrics ---\")\n",
    "    print(f\"Total Attempts: {total_attempts}\")\n",
    "    print(f\"Total Valid Generated: {len(all_valid_smiles)}\")\n",
    "    print(f\"✅ Validity:     {validity * 100:.2f}%\")\n",
    "    print(f\"🧬 Uniqueness:   {uniqueness * 100:.2f}%\")\n",
    "    print(f\"⭐ Novelty:      {novelty * 100:.2f}%\")\n",
    "    print(\"----------------------------------------\")\n",
    "    \n",
    "    if not all_valid_mols:\n",
    "        print(\"No valid molecules generated. Skipping plots.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Calculate Properties ---\n",
    "    props_real = {\n",
    "        'MolWt': [Descriptors.MolWt(m) for m in real_mols],\n",
    "        'LogP': [Descriptors.MolLogP(m) for m in real_mols],\n",
    "        'QED': [QED.qed(m) for m in real_mols]\n",
    "    }\n",
    "    \n",
    "    props_fake = {\n",
    "        'MolWt': [Descriptors.MolWt(m) for m in all_valid_mols],\n",
    "        'LogP': [Descriptors.MolLogP(m) for m in all_valid_mols],\n",
    "        'QED': [QED.qed(m) for m in all_valid_mols]\n",
    "    }\n",
    "\n",
    "    # --- 3. Plot Distributions ---\n",
    "    print(\"Generating property distribution plots...\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    plot_titles = ['Molecular Weight (MolWt)', 'LogP', 'Quantitative Esimation of Drug-likeness (QED)']\n",
    "    prop_keys = ['MolWt', 'LogP', 'QED']\n",
    "    \n",
    "    for ax, title, key in zip(axes, plot_titles, prop_keys):\n",
    "        ax.hist(props_real[key], bins=50, alpha=0.7, label='Real (Training)', color='blue', density=True)\n",
    "        ax.hist(props_fake[key], bins=50, alpha=0.7, label='Generated (Fake)', color='red', density=True)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(\"Value\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        ax.legend()\n",
    "        \n",
    "    plt.suptitle(f\"Property Distributions (Real vs. Generated) for {TARGET_UNIPROT_ID}\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(f\"property_plots_{TARGET_UNIPROT_ID}_FIXED.png\")\n",
    "    print(f\"Plots saved to property_plots_{TARGET_UNIPROT_ID}_FIXED.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    warnings.filterwarnings('default', '.*Implicit valence.*') # Restore warnings\n",
    "\n",
    "\n",
    "# --- 9. --- Main Execution (Train & Evaluate) ---\n",
    "\n",
    "# --- Execute Training ---\n",
    "print(\"\\n--- Starting WGAN-GP Training (FIXED) ---\")\n",
    "run_wgan_gp_training(generator, discriminator, real_loader, EPOCHS, N_CRITIC) \n",
    "print(\"\\nTraining completed.\")\n",
    "\n",
    "# --- Execute Evaluation ---\n",
    "num_to_eval = len(inhibitor_smiles) \n",
    "calculate_and_plot_metrics(generator, TARGET_EMBED, inhibitor_smiles, num_to_eval, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57bde899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA is available! GPU will be used for training.\n",
      "PyTorch CUDA Version: 12.1\n",
      "GPU Name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Using device: cuda\n",
      "Opening gzipped FASTA file: DL_ENDSEM__DATASET/chembl_35_blast.fa.gz\n",
      "Loading ProtT5 model... (This may take a moment)\n",
      "Generated protein embedding of shape: torch.Size([1024])\n",
      "Found 3989 potent inhibitors for UniProt ID P00533.\n",
      "Prepared 1332 real graph samples for training.\n",
      "Initializing models...\n",
      "\n",
      "--- Starting WGAN-GP Training (v3.1 - Sparse & Stabilized) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 21/21 [06:41<00:00, 19.10s/it, D_Loss=6.1672, G_Loss=-0.0204]\n",
      "Epoch 2/100: 100%|██████████| 21/21 [06:49<00:00, 19.52s/it, D_Loss=5.9477, G_Loss=-0.0136]\n",
      "Epoch 3/100: 100%|██████████| 21/21 [06:29<00:00, 18.55s/it, D_Loss=5.6340, G_Loss=0.0049] \n",
      "Epoch 4/100: 100%|██████████| 21/21 [07:01<00:00, 20.09s/it, D_Loss=5.1151, G_Loss=0.0406]\n",
      "Epoch 5/100: 100%|██████████| 21/21 [07:11<00:00, 20.53s/it, D_Loss=4.1920, G_Loss=0.0994]\n",
      "Epoch 6/100: 100%|██████████| 21/21 [07:33<00:00, 21.61s/it, D_Loss=2.5161, G_Loss=0.1862]\n",
      "Epoch 7/100: 100%|██████████| 21/21 [07:18<00:00, 20.88s/it, D_Loss=-0.4970, G_Loss=0.3029]\n",
      "Epoch 8/100: 100%|██████████| 21/21 [07:12<00:00, 20.60s/it, D_Loss=-5.6417, G_Loss=0.4704]\n",
      "Epoch 9/100: 100%|██████████| 21/21 [07:23<00:00, 21.12s/it, D_Loss=-13.8790, G_Loss=0.6855]\n",
      "Epoch 10/100: 100%|██████████| 21/21 [07:17<00:00, 20.85s/it, D_Loss=-26.5500, G_Loss=0.9217]\n",
      "Epoch 11/100: 100%|██████████| 21/21 [07:15<00:00, 20.71s/it, D_Loss=-44.8536, G_Loss=1.2416]\n",
      "Epoch 12/100: 100%|██████████| 21/21 [07:25<00:00, 21.21s/it, D_Loss=-70.4168, G_Loss=1.5409]\n",
      "Epoch 13/100: 100%|██████████| 21/21 [07:00<00:00, 20.04s/it, D_Loss=-104.6119, G_Loss=1.9430]\n",
      "Epoch 14/100: 100%|██████████| 21/21 [07:26<00:00, 21.26s/it, D_Loss=-150.0136, G_Loss=2.2218]\n",
      "Epoch 15/100: 100%|██████████| 21/21 [18:36<00:00, 53.16s/it, D_Loss=-208.1699, G_Loss=2.3930] \n",
      "Epoch 16/100: 100%|██████████| 21/21 [06:52<00:00, 19.63s/it, D_Loss=-283.3000, G_Loss=2.9456]\n",
      "Epoch 17/100:  38%|███▊      | 8/21 [02:13<03:37, 16.72s/it, D_Loss=-346.7058, G_Loss=2.8932]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 716\u001b[39m\n\u001b[32m    712\u001b[39m \u001b[38;5;66;03m# --- 9. --- Main Execution (Train & Evaluate) ---\u001b[39;00m\n\u001b[32m    713\u001b[39m \n\u001b[32m    714\u001b[39m \u001b[38;5;66;03m# --- Execute Training ---\u001b[39;00m\n\u001b[32m    715\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Starting WGAN-GP Training (v3.1 - Sparse & Stabilized) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m \u001b[43mrun_wgan_gp_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_CRITIC\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m    717\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining completed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    719\u001b[39m \u001b[38;5;66;03m# --- Execute Evaluation ---\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 480\u001b[39m, in \u001b[36mrun_wgan_gp_training\u001b[39m\u001b[34m(generator, discriminator, data_loader, epochs, n_critic)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;66;03m# --- Use new SPARSIFYING function ---\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     fake_data = \u001b[43mconvert_fake_to_SPARSE_data_vectorized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx_fake_logits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_fake_logits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_embed_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgumbel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fake_data.num_graphs == \u001b[32m0\u001b[39m:\n\u001b[32m    486\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWarning: Fake data batch was empty. Skipping D-step.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 350\u001b[39m, in \u001b[36mconvert_fake_to_SPARSE_data_vectorized\u001b[39m\u001b[34m(x_fake_logits, adj_fake_logits, t_embed_batch, device, gumbel, temperature)\u001b[39m\n\u001b[32m    347\u001b[39m edge_indices.extend([[j, k], [k, j]])\n\u001b[32m    349\u001b[39m \u001b[38;5;66;03m# --- Create 4-dim one-hot vector ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m bond_attr = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBOND_FEAT_DIM_DISCRIMINATOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# Handle case where index might be out of bounds if something is wrong\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m0\u001b[39m <= bond_type_idx < BOND_FEAT_DIM_DISCRIMINATOR:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "# --- IMPORT THE PyG DATALOADER FOR BATCHING ---\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader \n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.utils import softmax\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import Descriptors, QED, AllChem\n",
    "from Bio import SeqIO\n",
    "import random\n",
    "import re\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# --- 1. System & Configuration ---\n",
    "\n",
    "# --- Configuration (UPDATE THESE PATHS & ID) ---\n",
    "CHEMPL_DB_PATH = 'DL_ENDSEM__DATASET/chembl_35/chembl_35_sqlite/chembl_35.db'\n",
    "BLAST_FASTA_PATH = 'DL_ENDSEM__DATASET/chembl_35_blast.fa.gz'\n",
    "TARGET_UNIPROT_ID = \"P00533\" # Example: EGFR Kinase\n",
    "\n",
    "# --- ATOM_CLASSES from your (v2) script ---\n",
    "# This is the list that gave you 9 batches\n",
    "ATOM_CLASSES = [6, 7, 8, 9, 15, 16, 17, 35, 53] # C, N, O, F, P, S, Cl, Br, I\n",
    "ATOM_CLASSES_MAP = {num: i for i, num in enumerate(ATOM_CLASSES)}\n",
    "ATOM_FEAT_DIM = len(ATOM_CLASSES) # Now 9\n",
    "\n",
    "# Define bond types RDKit knows\n",
    "BOND_CLASSES_RDKIT = [Chem.BondType.SINGLE, Chem.BondType.DOUBLE, Chem.BondType.TRIPLE, Chem.BondType.AROMATIC]\n",
    "# --- Add a 5th \"No Bond\" class for the Generator ---\n",
    "BOND_FEAT_DIM_GENERATOR = len(BOND_CLASSES_RDKIT) + 1 # Now 5\n",
    "NO_BOND_IDX = len(BOND_CLASSES_RDKIT) # Index 4\n",
    "\n",
    "# --- Discriminator only sees 4 bond types ---\n",
    "BOND_FEAT_DIM_DISCRIMINATOR = len(BOND_CLASSES_RDKIT) # Back to 4\n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "Z_DIM = 100         # Latent noise dimension\n",
    "EMBED_DIM = 128     # Hidden dimension for the Graph Transformer\n",
    "T_EMBED_DIM = 1024  # Target embedding dimension (from ProtT5)\n",
    "LAMBDA_GP = 10.0    # Gradient Penalty weight\n",
    "MAX_NODES = 30      # Max atoms in generated molecules (for Generator tensor shape)\n",
    "N_CRITIC = 5        # Discriminator training steps per Generator step\n",
    "EPOCHS = 100        # Run for 100 epochs\n",
    "BATCH_SIZE = 64     # Your increased batch size\n",
    "CPU_WORKERS = 4     \n",
    "\n",
    "# --- CUDA Check ---\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ CUDA is available! GPU will be used for training.\")\n",
    "    print(f\"PyTorch CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    print(\"❌ CUDA not found. Running on CPU.\")\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "# --- 2. Real Protein Embedding Generation (No changes) ---\n",
    "\n",
    "def load_target_sequence(fasta_path, uniprot_id):\n",
    "    # (No changes to this function)\n",
    "    print(f\"Opening gzipped FASTA file: {fasta_path}\")\n",
    "    try:\n",
    "        with gzip.open(fasta_path, \"rt\") as handle:\n",
    "            for record in SeqIO.parse(handle, \"fasta\"):\n",
    "                if uniprot_id in record.id or uniprot_id in record.description:\n",
    "                    return str(record.seq)\n",
    "            print(f\"Warning: Could not find sequence for {uniprot_id} in {fasta_path}\")\n",
    "            return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FATAL ERROR: FASTA file not found at {fasta_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL ERROR: Could not read FASTA file. Error: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_protein_embedding(sequence, device):\n",
    "    # (No changes to this function)\n",
    "    print(\"Loading ProtT5 model... (This may take a moment)\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "    model = T5EncoderModel.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc').to(device)\n",
    "    model.eval() \n",
    "    sequence_preprocessed = \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))\n",
    "    inputs = tokenizer(sequence_preprocessed, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = model(**inputs).last_hidden_state\n",
    "    protein_vec = embedding.mean(dim=1).squeeze(0)\n",
    "    print(f\"Generated protein embedding of shape: {protein_vec.shape}\")\n",
    "    return protein_vec\n",
    "\n",
    "# --- Generate the REAL Target Embedding ---\n",
    "target_seq = load_target_sequence(BLAST_FASTA_PATH, TARGET_UNIPROT_ID)\n",
    "if target_seq is None:\n",
    "    raise ValueError(f\"Target sequence for {TARGET_UNIPROT_ID} not found. Exiting.\")\n",
    "TARGET_EMBED = get_protein_embedding(target_seq, DEVICE)\n",
    "\n",
    "\n",
    "# --- 3. Data Pipeline (Molecules -> Graphs) (v3 Logic) ---\n",
    "\n",
    "def extract_potent_inhibitors(db_path, uniprot_id, potency_cutoff_nM=100):\n",
    "    # (No changes to this function)\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        sql_query = f\"\"\"\n",
    "        SELECT DISTINCT cs.canonical_smiles\n",
    "        FROM activities acts\n",
    "        JOIN assays a ON acts.assay_id = a.assay_id\n",
    "        JOIN target_dictionary td ON a.tid = td.tid\n",
    "        JOIN target_components tc ON td.tid = tc.tid\n",
    "        JOIN component_sequences cseq ON tc.component_id = cseq.component_id\n",
    "        JOIN compound_structures cs ON acts.molregno = cs.molregno\n",
    "        WHERE\n",
    "            cseq.accession = '{uniprot_id}' AND\n",
    "            acts.standard_type = 'IC50' AND\n",
    "            acts.standard_units = 'nM' AND\n",
    "            acts.standard_relation = '=' AND\n",
    "            acts.standard_value <= {potency_cutoff_nM}\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(sql_query, conn)\n",
    "        conn.close()\n",
    "        print(f\"Found {len(df)} potent inhibitors for UniProt ID {uniprot_id}.\")\n",
    "        return df['canonical_smiles'].unique().tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during database query: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_atom_features(atom):\n",
    "    \"\"\"Creates a one-hot vector for the atom type.\"\"\"\n",
    "    atom_num = atom.GetAtomicNum()\n",
    "    if atom_num not in ATOM_CLASSES_MAP:\n",
    "        return None # Atom is not in our allowed list\n",
    "        \n",
    "    atom_index = ATOM_CLASSES_MAP[atom_num]\n",
    "    atom_one_hot = torch.zeros(ATOM_FEAT_DIM, dtype=torch.float)\n",
    "    atom_one_hot[atom_index] = 1.0\n",
    "    return atom_one_hot\n",
    "\n",
    "def smiles_to_graph(smiles, target_embed):\n",
    "    \"\"\"Converts SMILES to a PyG Data object.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol: return None\n",
    "    if mol.GetNumAtoms() > MAX_NODES: return None\n",
    "\n",
    "    atom_features_list = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        features = get_atom_features(atom)\n",
    "        if features is None: # Skip molecule if it contains an invalid atom\n",
    "            return None\n",
    "        atom_features_list.append(features)\n",
    "\n",
    "    if not atom_features_list:\n",
    "        return None\n",
    "    x = torch.stack(atom_features_list)\n",
    "    \n",
    "    edge_indices, edge_attrs = [], []\n",
    "    # --- Use the 4-class bond list ---\n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        # Create a 4-dim one-hot vector\n",
    "        bond_type_oh = [int(bond.GetBondType() == t) for t in BOND_CLASSES_RDKIT]\n",
    "        \n",
    "        # --- Ensure bond type is one we recognize ---\n",
    "        if sum(bond_type_oh) == 1: #i.e., it's S, D, T, or Aromatic\n",
    "            edge_indices.extend([[i, j], [j, i]])\n",
    "            edge_attrs.extend([bond_type_oh, bond_type_oh])\n",
    "\n",
    "    # --- Return None if molecule has no *recognized* bonds ---\n",
    "    if not edge_indices: \n",
    "        return None\n",
    "    \n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.float) # Shape: [num_bonds, 4]\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, \n",
    "                target_embed=target_embed.unsqueeze(0))\n",
    "    return data\n",
    "\n",
    "# --- Data Pipeline Execution ---\n",
    "inhibitor_smiles = extract_potent_inhibitors(CHEMPL_DB_PATH, TARGET_UNIPROT_ID)\n",
    "real_data_list = [smiles_to_graph(s, TARGET_EMBED.cpu()) for s in inhibitor_smiles]\n",
    "real_data_list = [d for d in real_data_list if d is not None]\n",
    "\n",
    "if not real_data_list:\n",
    "    print(f\"FATAL: No valid inhibitor data found (or all were filtered out). Check ATOM_CLASSES and BOND_CLASSES.\")\n",
    "    exit()\n",
    "\n",
    "# --- USE THE STANDARD PyG DATALOADER ---\n",
    "real_loader = PyGDataLoader(\n",
    "    real_data_list, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=CPU_WORKERS, \n",
    "    pin_memory=True \n",
    ")\n",
    "print(f\"Prepared {len(real_data_list)} real graph samples for training.\")\n",
    "\n",
    "\n",
    "# --- 4. Model Architecture (v3 Logic) ---\n",
    "\n",
    "# --- 4.1. Relational Graph Transformer Layer (No changes) ---\n",
    "class RelationalGraphTransformerLayer(MessagePassing):\n",
    "    # (No changes to this class)\n",
    "    def __init__(self, in_channels, out_channels, edge_dim, heads=4, dropout=0.1, **kwargs):\n",
    "        super().__init__(aggr='add', node_dim=0, **kwargs)\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.lin_q = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_k = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_v = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_edge = nn.Linear(edge_dim, heads * out_channels, bias=False) \n",
    "        self.att_coeff = nn.Parameter(torch.Tensor(1, heads, 2 * out_channels)) \n",
    "        self.lin_out = nn.Linear(heads * out_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.xavier_uniform_(self.att_coeff)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        Q = self.lin_q(x).view(-1, self.heads, self.out_channels)\n",
    "        K = self.lin_k(x).view(-1, self.heads, self.out_channels)\n",
    "        V = self.lin_v(x).view(-1, self.heads, self.out_channels)\n",
    "        E = self.lin_edge(edge_attr).view(-1, self.heads, self.out_channels)\n",
    "        out = self.propagate(edge_index, Q=Q, K=K, V=V, E=E)\n",
    "        out = out.view(-1, self.heads * self.out_channels)\n",
    "        out = self.lin_out(out)\n",
    "        return out\n",
    "\n",
    "    def message(self, Q_i, K_j, V_j, E_k, index):\n",
    "        try: is_empty = (E_k.size(0) == 0)\n",
    "        except AttributeError: is_empty = True\n",
    "        if is_empty: E_bias = torch.zeros(Q_i.size(0), self.heads, 1, device=self.lin_q.weight.device)\n",
    "        else: E_bias = E_k.mean(dim=-1, keepdim=True) \n",
    "        QK_cat = torch.cat([Q_i, K_j], dim=-1)\n",
    "        e_ij = (QK_cat * self.att_coeff).sum(dim=-1, keepdim=True)\n",
    "        e_ij = e_ij + E_bias\n",
    "        e_ij = F.leaky_relu(e_ij)\n",
    "        alpha = softmax(e_ij, index)\n",
    "        alpha = self.dropout(alpha)\n",
    "        return V_j * alpha.view(-1, self.heads, 1)\n",
    "\n",
    "# --- 4.2. Discriminator (v3 Logic) ---\n",
    "class Discriminator(nn.Module):\n",
    "    # --- edge_dim is BOND_FEAT_DIM_DISCRIMINATOR (4) ---\n",
    "    def __init__(self, node_features, edge_dim, t_embed_dim, embed_dim, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_d = node_features if i == 0 else embed_dim\n",
    "            # --- This layer now expects edge_dim = 4 ---\n",
    "            self.layers.append(RelationalGraphTransformerLayer(in_d, embed_dim, edge_dim))\n",
    "        self.lin_final = nn.Linear(embed_dim + t_embed_dim, 1)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        t_embed = data.target_embed\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        graph_embed = global_mean_pool(x, batch)\n",
    "        if t_embed.dim() > 2: t_embed = t_embed.squeeze(1) \n",
    "        final_input = torch.cat([graph_embed, t_embed], dim=1)\n",
    "        return self.lin_final(final_input).squeeze(1)\n",
    "\n",
    "# --- 4.3. Generator (v3 Logic) ---\n",
    "class Generator(nn.Module):\n",
    "    # --- bond_features is BOND_FEAT_DIM_GENERATOR (5) ---\n",
    "    def __init__(self, z_dim, t_embed_dim, node_features, bond_features, max_nodes=MAX_NODES):\n",
    "        super().__init__()\n",
    "        self.max_nodes = max_nodes\n",
    "        self.node_features = node_features\n",
    "        self.bond_features = bond_features\n",
    "        self.lin_x = nn.Sequential(nn.Linear(z_dim + t_embed_dim, 256), nn.ReLU(), nn.Linear(256, max_nodes * node_features))\n",
    "        # --- This layer now outputs 5 features per bond ---\n",
    "        self.lin_adj = nn.Sequential(nn.Linear(z_dim + t_embed_dim, 256), nn.ReLU(), nn.Linear(256, max_nodes * max_nodes * bond_features))\n",
    "\n",
    "    def forward(self, z, t_embed):\n",
    "        zt = torch.cat([z, t_embed], dim=1)\n",
    "        x_fake_logits = self.lin_x(zt).view(-1, self.max_nodes, self.node_features)\n",
    "        adj_fake_logits = self.lin_adj(zt).view(-1, self.max_nodes, self.max_nodes, self.bond_features)\n",
    "        return x_fake_logits, adj_fake_logits\n",
    "\n",
    "# --- Model Initialization ---\n",
    "print(\"Initializing models...\")\n",
    "# --- Pass the correct dimensions ---\n",
    "generator = Generator(Z_DIM, T_EMBED_DIM, ATOM_FEAT_DIM, BOND_FEAT_DIM_GENERATOR).to(DEVICE)\n",
    "discriminator = Discriminator(ATOM_FEAT_DIM, BOND_FEAT_DIM_DISCRIMINATOR, T_EMBED_DIM, EMBED_DIM).to(DEVICE)\n",
    "\n",
    "# --- !!!!!!!!!!!!!!!!!!!!!!!!!!! ---\n",
    "# --- 1st CHANGE: Lowered Learning Rate ---\n",
    "# --- !!!!!!!!!!!!!!!!!!!!!!!!!!! ---\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=1e-5, betas=(0.5, 0.9)) # Was 1e-4\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=1e-5, betas=(0.5, 0.9)) # Was 1e-4\n",
    "\n",
    "\n",
    "# --- 5. Training Utilities (v3 Logic) ---\n",
    "\n",
    "# --- 5.1. Sparse Graph Conversion ---\n",
    "def convert_fake_to_SPARSE_data_vectorized(x_fake_logits, adj_fake_logits, t_embed_batch, device, gumbel=False, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Converts Generator output (logits) to a BATCH of sparse PyG Data objects.\n",
    "    \"\"\"\n",
    "    batch_size = x_fake_logits.size(0)\n",
    "    data_list = []\n",
    "\n",
    "    # 1. Sample nodes (still vectorized)\n",
    "    if gumbel:\n",
    "        x_fake_tensor = F.gumbel_softmax(x_fake_logits, tau=temperature, hard=True)\n",
    "    else:\n",
    "        x_indices = torch.argmax(x_fake_logits, dim=-1)\n",
    "        x_fake_tensor = F.one_hot(x_indices, num_classes=ATOM_FEAT_DIM).float()\n",
    "\n",
    "    # 2. Sample bonds (still vectorized)\n",
    "    if gumbel:\n",
    "        adj_fake_tensor = F.gumbel_softmax(adj_fake_logits, tau=temperature, hard=True)\n",
    "    else:\n",
    "        adj_indices = torch.argmax(adj_fake_logits, dim=-1)\n",
    "        adj_fake_tensor = F.one_hot(adj_indices, num_classes=BOND_FEAT_DIM_GENERATOR).float()\n",
    "    \n",
    "    # 3. Loop over batch to build sparse graphs\n",
    "    for i in range(batch_size):\n",
    "        x = x_fake_tensor[i] # Shape [N, ATOM_FEAT_DIM]\n",
    "        adj_full = adj_fake_tensor[i] # Shape [N, N, BOND_FEAT_DIM_GENERATOR]\n",
    "        \n",
    "        edge_indices = []\n",
    "        edge_attrs = []\n",
    "\n",
    "        # Iterate over upper triangle\n",
    "        for j in range(MAX_NODES):\n",
    "            for k in range(j + 1, MAX_NODES):\n",
    "                bond_logits = adj_full[j, k] # Shape [5]\n",
    "                bond_type_idx = torch.argmax(bond_logits).item()\n",
    "                \n",
    "                # --- Check if it's NOT a \"No Bond\" ---\n",
    "                if bond_type_idx != NO_BOND_IDX:\n",
    "                    # Add this edge\n",
    "                    edge_indices.extend([[j, k], [k, j]])\n",
    "                    \n",
    "                    # --- Create 4-dim one-hot vector ---\n",
    "                    bond_attr = torch.zeros(BOND_FEAT_DIM_DISCRIMINATOR, device=device)\n",
    "                    # Handle case where index might be out of bounds if something is wrong\n",
    "                    if 0 <= bond_type_idx < BOND_FEAT_DIM_DISCRIMINATOR:\n",
    "                         bond_attr[bond_type_idx] = 1.0\n",
    "                    edge_attrs.extend([bond_attr, bond_attr])\n",
    "\n",
    "        if not edge_indices:\n",
    "            # No bonds were formed, create a dummy to avoid errors\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long, device=device)\n",
    "            edge_attr = torch.empty((0, BOND_FEAT_DIM_DISCRIMINATOR), dtype=torch.float, device=device)\n",
    "        else:\n",
    "            edge_index = torch.tensor(edge_indices, dtype=torch.long, device=device).t().contiguous()\n",
    "            edge_attr = torch.stack(edge_attrs)\n",
    "\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            target_embed=t_embed_batch[i].unsqueeze(0) # Embed for this single graph\n",
    "        )\n",
    "        data_list.append(data)\n",
    "\n",
    "    # 4. Re-batch the sparse graphs\n",
    "    \n",
    "    # Use a loader to properly collate the list of Data objects into a Batch object\n",
    "    temp_loader = PyGDataLoader(data_list, batch_size=batch_size, shuffle=False)\n",
    "    batch = next(iter(temp_loader))\n",
    "    \n",
    "    # Squeeze the target_embed back to [B, T_EMBED_DIM]\n",
    "    batch.target_embed = batch.target_embed.squeeze(1)\n",
    "    \n",
    "    return batch.to(device)\n",
    "\n",
    "\n",
    "# --- 5.2. WGAN-GP Gradient Penalty (v3 Logic) ---\n",
    "def calculate_gradient_penalty(discriminator, real_data, fake_data, lambda_gp, device):\n",
    "    \"\"\"\n",
    "    Interpolates on the GLOBAL graph embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Get graph embeddings first ---\n",
    "    discriminator.eval() # Freeze discriminator for this part\n",
    "    \n",
    "    real_x, real_edge_index, real_edge_attr, real_batch = real_data.x, real_data.edge_index, real_data.edge_attr, real_data.batch\n",
    "    real_t_embed = real_data.target_embed\n",
    "    for layer in discriminator.layers:\n",
    "        real_x = layer(real_x, real_edge_index, real_edge_attr)\n",
    "        real_x = F.relu(real_x)\n",
    "    real_graph_embed = global_mean_pool(real_x, real_batch)\n",
    "    \n",
    "    fake_x, fake_edge_index, fake_edge_attr, fake_batch = fake_data.x, fake_data.edge_index, fake_data.edge_attr, fake_data.batch\n",
    "    # --- Handle case where fake batch might be empty ---\n",
    "    if fake_data.num_graphs == 0:\n",
    "        discriminator.train()\n",
    "        # Return a 0 penalty if there's nothing to compare\n",
    "        return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "        \n",
    "    fake_t_embed = fake_data.target_embed\n",
    "    for layer in discriminator.layers:\n",
    "        fake_x = layer(fake_x, fake_edge_index, fake_edge_attr)\n",
    "        fake_x = F.relu(fake_x)\n",
    "    fake_graph_embed = global_mean_pool(fake_x, fake_batch)\n",
    "    \n",
    "    discriminator.train() # Unfreeze\n",
    "    \n",
    "    # Match batch sizes if they differ\n",
    "    batch_size = min(real_graph_embed.size(0), fake_graph_embed.size(0))\n",
    "    if batch_size == 0:\n",
    "        return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "        \n",
    "    real_graph_embed = real_graph_embed[:batch_size]\n",
    "    fake_graph_embed = fake_graph_embed[:batch_size]\n",
    "    real_t_embed = real_t_embed[:batch_size]\n",
    "\n",
    "    # --- Interpolate on graph_embed ---\n",
    "    alpha = torch.rand(batch_size, 1).to(device)\n",
    "    interpolated_embed = (alpha * real_graph_embed) + ((1 - alpha) * fake_graph_embed)\n",
    "    interpolated_embed.requires_grad_(True)\n",
    "    \n",
    "    # --- Combine with target and pass to *final layer only* ---\n",
    "    final_input = torch.cat([interpolated_embed, real_t_embed], dim=1)\n",
    "    disc_interpolates = discriminator.lin_final(final_input).squeeze(1)\n",
    "\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=disc_interpolates, inputs=interpolated_embed, # Grad w.r.t. interpolated_embed\n",
    "        grad_outputs=torch.ones_like(disc_interpolates),\n",
    "        create_graph=True, retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = lambda_gp * ((gradient_norm - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty\n",
    "\n",
    "# --- 6. Main Training Loop (v3 Logic + STABILIZATION) ---\n",
    "def run_wgan_gp_training(generator, discriminator, data_loader, epochs, n_critic):\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        g_loss_sum, d_loss_sum = 0, 0\n",
    "        \n",
    "        progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "        \n",
    "        for batch_idx, real_data in enumerate(progress_bar):\n",
    "            real_data = real_data.to(DEVICE)\n",
    "            \n",
    "            # --- Handle small final batch ---\n",
    "            if real_data.num_graphs < 2: # Need at least 2 for GP\n",
    "                print(\"Warning: Skipping batch with < 2 graphs.\")\n",
    "                continue\n",
    "                \n",
    "            batch_size = real_data.num_graphs\n",
    "            target_embed_batch = real_data.target_embed\n",
    "\n",
    "            # 1. Train Discriminator (n_critic steps)\n",
    "            for _ in range(n_critic):\n",
    "                optimizer_D.zero_grad()\n",
    "                \n",
    "                # Real Loss\n",
    "                d_real = discriminator(real_data).mean()\n",
    "                \n",
    "                # Fake Loss\n",
    "                z = torch.randn(batch_size, Z_DIM).to(DEVICE)\n",
    "                x_fake_logits, adj_fake_logits = generator(z, target_embed_batch)\n",
    "                \n",
    "                # --- Use new SPARSIFYING function ---\n",
    "                with torch.no_grad():\n",
    "                    fake_data = convert_fake_to_SPARSE_data_vectorized(\n",
    "                        x_fake_logits.detach(), adj_fake_logits.detach(), \n",
    "                        target_embed_batch, DEVICE, gumbel=False\n",
    "                    )\n",
    "                \n",
    "                if fake_data.num_graphs == 0:\n",
    "                    print(\"Warning: Fake data batch was empty. Skipping D-step.\")\n",
    "                    continue\n",
    "                \n",
    "                d_fake = discriminator(fake_data).mean()\n",
    "                \n",
    "                gp = calculate_gradient_penalty(discriminator, real_data, fake_data, LAMBDA_GP, DEVICE)\n",
    "                \n",
    "                d_loss = - (d_real - d_fake) + gp\n",
    "                d_loss.backward()\n",
    "                \n",
    "                # --- !!!!!!!!!!!!!!!!!!!!!!!!!!! ---\n",
    "                # --- 2nd CHANGE: Add Gradient Clipping ---\n",
    "                # --- !!!!!!!!!!!!!!!!!!!!!!!!!!! ---\n",
    "                torch.nn.utils.clip_grad_norm_(discriminator.parameters(), 1.0)\n",
    "                \n",
    "                optimizer_D.step()\n",
    "                d_loss_sum += d_loss.item()\n",
    "            \n",
    "            # 2. Train Generator (1 step)\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size, Z_DIM).to(DEVICE)\n",
    "            x_fake_logits, adj_fake_logits = generator(z, target_embed_batch)\n",
    "            \n",
    "            fake_data = convert_fake_to_SPARSE_data_vectorized(\n",
    "                x_fake_logits, adj_fake_logits, \n",
    "                target_embed_batch, DEVICE, gumbel=True\n",
    "            )\n",
    "            \n",
    "            if fake_data.num_graphs == 0:\n",
    "                print(\"Warning: Fake data batch was empty. Skipping G-step.\")\n",
    "                continue\n",
    "\n",
    "            g_loss = - discriminator(fake_data).mean()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            g_loss_sum += g_loss.item()\n",
    "            \n",
    "            progress_bar.set_postfix(\n",
    "                D_Loss=f\"{(d_loss_sum / (batch_idx+1) / n_critic):.4f}\", \n",
    "                G_Loss=f\"{(g_loss_sum / (batch_idx+1)):.4f}\"\n",
    "            )\n",
    "\n",
    "# --- 7. Generation & SMILES Conversion (v3 Logic) ---\n",
    "\n",
    "def tensors_to_smiles(x_fake_one_hot, adj_fake_logits):\n",
    "    \"\"\"\n",
    "    Converts raw generator tensor output (one-hot nodes) into SMILES strings.\n",
    "    \"\"\"\n",
    "    # --- Get atomic number indices from one-hot nodes ---\n",
    "    x_fake_indices = torch.argmax(x_fake_one_hot, dim=-1).cpu().detach()\n",
    "    adj_fake_logits = adj_fake_logits.cpu().detach()\n",
    "    \n",
    "    # --- Get bond indices from 5-class logits ---\n",
    "    adj_bond_type_idx = torch.argmax(adj_fake_logits, dim=-1)\n",
    "    \n",
    "    batch_size = x_fake_indices.size(0)\n",
    "    generated_smiles = []\n",
    "    generated_mols = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        mol = Chem.RWMol()\n",
    "        atom_map = {} # Map from tensor index (0..MAX_NODES-1) to RDKit atom index\n",
    "        \n",
    "        # 1. Add atoms\n",
    "        for j in range(MAX_NODES):\n",
    "            atom_idx = x_fake_indices[i, j].item()\n",
    "            atom_num = ATOM_CLASSES[atom_idx]\n",
    "            \n",
    "            atom = Chem.Atom(atom_num)\n",
    "            rdkit_idx = mol.AddAtom(atom)\n",
    "            atom_map[j] = rdkit_idx\n",
    "                \n",
    "        # 2. Add bonds\n",
    "        for j in range(MAX_NODES):\n",
    "            for k in range(j + 1, MAX_NODES):\n",
    "                # --- Get bond type from 5-class indices ---\n",
    "                bond_type_idx = adj_bond_type_idx[i, j, k].item()\n",
    "                \n",
    "                # --- Add bond IF NOT \"No Bond\" ---\n",
    "                if bond_type_idx != NO_BOND_IDX:\n",
    "                    # --- Check if bond_type_idx is valid for RDKit bonds ---\n",
    "                    if 0 <= bond_type_idx < len(BOND_CLASSES_RDKIT):\n",
    "                        bond_type = BOND_CLASSES_RDKIT[bond_type_idx]\n",
    "                        mol.AddBond(atom_map[j], atom_map[k], bond_type)\n",
    "        \n",
    "        # 3. Sanitize and Convert\n",
    "        try:\n",
    "            Chem.SanitizeMol(mol)\n",
    "            smi = Chem.MolToSmiles(mol)\n",
    "            \n",
    "            # --- Filter out disconnected fragments ---\n",
    "            if '.' in smi:\n",
    "                generated_smiles.append(None) # Invalid fragment\n",
    "                generated_mols.append(None)\n",
    "            else:\n",
    "                generated_smiles.append(smi)\n",
    "                generated_mols.append(mol)\n",
    "        except Exception as e:\n",
    "            # print(f\"RDKit Error: {e}\") # Uncomment for debugging\n",
    "            generated_smiles.append(None) # Invalid molecule\n",
    "            generated_mols.append(None)\n",
    "\n",
    "    valid_smiles = [s for s in generated_smiles if s is not None]\n",
    "    valid_mols = [m for m in generated_mols if m is not None]\n",
    "    \n",
    "    return valid_smiles, valid_mols, generated_smiles\n",
    "\n",
    "# --- 8. Performance Metrics & Plotting (v3 Logic) ---\n",
    "\n",
    "def calculate_and_plot_metrics(generator, target_embed, real_smiles_list, num_to_generate, device):\n",
    "    \"\"\"\n",
    "    Generates molecules and calculates Validity, Uniqueness, Novelty,\n",
    "    and plots property distributions.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Generation & Evaluation ---\")\n",
    "    warnings.filterwarnings('ignore', '.*Implicit valence.*') # Suppress RDKit warnings\n",
    "    \n",
    "    generator.eval() # Set generator to evaluation mode\n",
    "    \n",
    "    real_mols = [Chem.MolFromSmiles(s) for s in real_smiles_list]\n",
    "    real_mols = [m for m in real_mols if m is not None]\n",
    "    real_smiles_set = set(real_smiles_list)\n",
    "    \n",
    "    all_valid_smiles = []\n",
    "    all_valid_mols = []\n",
    "    total_attempts = 0 # Track total attempts\n",
    "\n",
    "    print(f\"Generating {num_to_generate} *valid* molecules for evaluation...\")\n",
    "    with torch.no_grad():\n",
    "        while len(all_valid_smiles) < num_to_generate:\n",
    "            batch_size = BATCH_SIZE\n",
    "            total_attempts += batch_size\n",
    "\n",
    "            z = torch.randn(batch_size, Z_DIM).to(device)\n",
    "            t_embed_batch = target_embed.unsqueeze(0).repeat(batch_size, 1)\n",
    "            \n",
    "            x_fake_logits, adj_fake_logits = generator(z, t_embed_batch)\n",
    "            \n",
    "            # --- Use argmax (not Gumbel) for final generation ---\n",
    "            x_indices = torch.argmax(x_fake_logits, dim=-1)\n",
    "            x_fake_one_hot = F.one_hot(x_indices, num_classes=ATOM_FEAT_DIM).float()\n",
    "            \n",
    "            # --- Pass 5-dim bond logits to smiles converter ---\n",
    "            valid_smiles, valid_mols, _ = tensors_to_smiles(x_fake_one_hot, adj_fake_logits)\n",
    "            \n",
    "            all_valid_smiles.extend(valid_smiles)\n",
    "            all_valid_mols.extend(valid_mols)\n",
    "            \n",
    "            print(f\"Generated: {len(all_valid_smiles)}/{num_to_generate} valid molecules...\", end='\\r')\n",
    "            \n",
    "            if total_attempts > num_to_generate * 50 and not all_valid_smiles:\n",
    "                 print(\"\\nError: Generated too many molecules with 0 validity. Stopping.\")\n",
    "                 break\n",
    "            if total_attempts > num_to_generate * 10 and len(all_valid_smiles) < num_to_generate: \n",
    "                 print(f\"\\nWarning: Low validity. Stopping generation at {len(all_valid_smiles)} molecules.\")\n",
    "                 break\n",
    "\n",
    "    print(\"\\nGeneration complete. Calculating metrics...\")\n",
    "    \n",
    "    # --- 1. Calculate Metrics ---\n",
    "    \n",
    "    if total_attempts == 0: total_attempts = 1\n",
    "    validity = len(all_valid_smiles) / total_attempts\n",
    "    \n",
    "    if len(all_valid_smiles) > 0:\n",
    "        uniqueness = len(set(all_valid_smiles)) / len(all_valid_smiles)\n",
    "    else:\n",
    "        uniqueness = 0.0\n",
    "        \n",
    "    if len(all_valid_smiles) > 0:\n",
    "        unique_valid_smiles = set(all_valid_smiles)\n",
    "        novel_smiles = unique_valid_smiles - real_smiles_set\n",
    "        novelty = len(novel_smiles) / len(unique_valid_smiles)\n",
    "    else:\n",
    "        novelty = 0.0\n",
    "\n",
    "    print(\"\\n--- Generative Performance Metrics ---\")\n",
    "    print(f\"Total Attempts: {total_attempts}\")\n",
    "    print(f\"Total Valid Generated: {len(all_valid_smiles)}\")\n",
    "    print(f\"✅ Validity:     {validity * 100:.2f}%\")\n",
    "    print(f\"🧬 Uniqueness:   {uniqueness * 100:.2f}%\")\n",
    "    print(f\"⭐ Novelty:      {novelty * 100:.2f}%\")\n",
    "    print(\"----------------------------------------\")\n",
    "    \n",
    "    if not all_valid_mols:\n",
    "        print(\"No valid molecules generated. Skipping plots.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Calculate Properties ---\n",
    "    props_real = {\n",
    "        'MolWt': [Descriptors.MolWt(m) for m in real_mols],\n",
    "        'LogP': [Descriptors.MolLogP(m) for m in real_mols],\n",
    "        'QED': [QED.qed(m) for m in real_mols]\n",
    "    }\n",
    "    \n",
    "    props_fake = {\n",
    "        'MolWt': [Descriptors.MolWt(m) for m in all_valid_mols],\n",
    "        'LogP': [Descriptors.MolLogP(m) for m in all_valid_mols],\n",
    "        'QED': [QED.qed(m) for m in all_valid_mols]\n",
    "    }\n",
    "\n",
    "    # --- 3. Plot Distributions ---\n",
    "    print(\"Generating property distribution plots...\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    plot_titles = ['Molecular Weight (MolWt)', 'LogP', 'Quantitative Esimation of Drug-likeness (QED)']\n",
    "    prop_keys = ['MolWt', 'LogP', 'QED']\n",
    "    \n",
    "    for ax, title, key in zip(axes, plot_titles, prop_keys):\n",
    "        ax.hist(props_real[key], bins=50, alpha=0.7, label='Real (Training)', color='blue', density=True)\n",
    "        ax.hist(props_fake[key], bins=50, alpha=0.7, label='Generated (Fake)', color='red', density=True)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(\"Value\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        ax.legend()\n",
    "        \n",
    "    plt.suptitle(f\"Property Distributions (Real vs. Generated) for {TARGET_UNIPROT_ID}\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(f\"property_plots_{TARGET_UNIPROT_ID}_v3.1.png\")\n",
    "    print(f\"Plots saved to property_plots_{TARGET_UNIPROT_ID}_v3.1.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    warnings.filterwarnings('default', '.*Implicit valence.*') # Restore warnings\n",
    "\n",
    "\n",
    "# --- 9. --- Main Execution (Train & Evaluate) ---\n",
    "\n",
    "# --- Execute Training ---\n",
    "print(\"\\n--- Starting WGAN-GP Training (v3.1 - Sparse & Stabilized) ---\")\n",
    "run_wgan_gp_training(generator, discriminator, real_loader, EPOCHS, N_CRITIC) \n",
    "print(\"\\nTraining completed.\")\n",
    "\n",
    "# --- Execute Evaluation ---\n",
    "num_to_eval = len(real_data_list) # --- Use count of *filtered* real data ---\n",
    "calculate_and_plot_metrics(generator, TARGET_EMBED, inhibitor_smiles, num_to_eval, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd79f6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA is available! GPU will be used for training.\n",
      "PyTorch CUDA Version: 12.1\n",
      "GPU Name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Using device: cuda\n",
      "Opening gzipped FASTA file: DL_ENDSEM__DATASET/chembl_35_blast.fa.gz\n",
      "Loading ProtT5 model... (This may take a moment)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated protein embedding of shape: torch.Size([1024])\n",
      "Found 3989 potent inhibitors for UniProt ID P00533.\n",
      "Prepared 1332 real graph samples for training.\n",
      "Initializing models...\n",
      "\n",
      "--- Starting WGAN-GP Training (v3.1 - Sparse & Stabilized) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/21 [00:00<?, ?it/s]c:\\Users\\nikhi\\OneDrive\\Attachments\\Desktop\\Git_Repos\\NNDL_endsem\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ..\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Epoch 1/100: 100%|██████████| 21/21 [05:12<00:00, 14.87s/it, D_Fake=0.0184, D_Loss=6.2638, D_Real=0.0207, G_Loss=-0.0184]\n",
      "Epoch 2/100: 100%|██████████| 21/21 [04:51<00:00, 13.89s/it, D_Fake=0.0190, D_Loss=6.2063, D_Real=0.0279, G_Loss=-0.0185]\n",
      "Epoch 3/100: 100%|██████████| 21/21 [04:58<00:00, 14.23s/it, D_Fake=0.0199, D_Loss=6.1459, D_Real=0.0385, G_Loss=-0.0182]\n",
      "Epoch 4/100: 100%|██████████| 21/21 [05:07<00:00, 14.64s/it, D_Fake=0.0211, D_Loss=6.0822, D_Real=0.0527, G_Loss=-0.0170]\n",
      "Epoch 5/100: 100%|██████████| 21/21 [05:13<00:00, 14.92s/it, D_Fake=0.0229, D_Loss=6.0150, D_Real=0.0708, G_Loss=-0.0147]\n",
      "Epoch 6/100: 100%|██████████| 21/21 [05:18<00:00, 15.15s/it, D_Fake=0.0252, D_Loss=5.9433, D_Real=0.0943, G_Loss=-0.0109]\n",
      "Epoch 7/100: 100%|██████████| 21/21 [05:25<00:00, 15.48s/it, D_Fake=0.0296, D_Loss=5.8664, D_Real=0.1250, G_Loss=-0.0058]\n",
      "Epoch 8/100: 100%|██████████| 21/21 [05:16<00:00, 15.06s/it, D_Fake=0.0373, D_Loss=5.7824, D_Real=0.1663, G_Loss=0.0001] \n",
      "Epoch 9/100: 100%|██████████| 21/21 [05:29<00:00, 15.71s/it, D_Fake=0.0510, D_Loss=5.6907, D_Real=0.2212, G_Loss=0.0065]\n",
      "Epoch 10/100: 100%|██████████| 21/21 [05:15<00:00, 15.03s/it, D_Fake=0.0740, D_Loss=5.5915, D_Real=0.2932, G_Loss=0.0131]\n",
      "Epoch 11/100: 100%|██████████| 21/21 [05:11<00:00, 14.85s/it, D_Fake=0.1089, D_Loss=5.4830, D_Real=0.3865, G_Loss=0.0217]\n",
      "Epoch 12/100: 100%|██████████| 21/21 [04:13<00:00, 12.07s/it, D_Fake=0.1649, D_Loss=5.3686, D_Real=0.5067, G_Loss=0.0299]\n",
      "Epoch 13/100: 100%|██████████| 21/21 [03:04<00:00,  8.79s/it, D_Fake=0.2405, D_Loss=5.2411, D_Real=0.6598, G_Loss=0.0379]\n",
      "Epoch 14/100: 100%|██████████| 21/21 [03:27<00:00,  9.90s/it, D_Fake=0.3554, D_Loss=5.1129, D_Real=0.8531, G_Loss=0.0455]\n",
      "Epoch 15/100: 100%|██████████| 21/21 [05:04<00:00, 14.48s/it, D_Fake=0.5112, D_Loss=4.9737, D_Real=1.0984, G_Loss=0.0563]\n",
      "Epoch 16/100: 100%|██████████| 21/21 [04:53<00:00, 14.00s/it, D_Fake=0.7256, D_Loss=4.8326, D_Real=1.4044, G_Loss=0.0651]\n",
      "Epoch 17/100: 100%|██████████| 21/21 [05:00<00:00, 14.31s/it, D_Fake=1.0096, D_Loss=4.6852, D_Real=1.7863, G_Loss=0.0759]\n",
      "Epoch 18/100: 100%|██████████| 21/21 [04:55<00:00, 14.06s/it, D_Fake=1.3899, D_Loss=4.5377, D_Real=2.2647, G_Loss=0.0862]\n",
      "Epoch 19/100: 100%|██████████| 21/21 [05:14<00:00, 14.96s/it, D_Fake=1.8843, D_Loss=4.3903, D_Real=2.8574, G_Loss=0.0920]\n",
      "Epoch 20/100: 100%|██████████| 21/21 [05:08<00:00, 14.70s/it, D_Fake=2.5035, D_Loss=4.2367, D_Real=3.5812, G_Loss=0.0996]\n",
      "Epoch 21/100: 100%|██████████| 21/21 [05:05<00:00, 14.53s/it, D_Fake=3.3073, D_Loss=4.1056, D_Real=4.4672, G_Loss=0.1127]\n",
      "Epoch 22/100: 100%|██████████| 21/21 [04:09<00:00, 11.89s/it, D_Fake=4.3203, D_Loss=3.9988, D_Real=5.5383, G_Loss=0.1143]\n",
      "Epoch 23/100: 100%|██████████| 21/21 [9:21:46<00:00, 1605.09s/it, D_Fake=5.5469, D_Loss=3.8758, D_Real=6.8393, G_Loss=0.1148]   \n",
      "Epoch 24/100: 100%|██████████| 21/21 [04:17<00:00, 12.28s/it, D_Fake=7.0689, D_Loss=3.8051, D_Real=8.3836, G_Loss=0.1098]\n",
      "Epoch 25/100: 100%|██████████| 21/21 [04:46<00:00, 13.66s/it, D_Fake=8.8680, D_Loss=3.7318, D_Real=10.2079, G_Loss=0.1044]\n",
      "Epoch 26/100: 100%|██████████| 21/21 [04:00<00:00, 11.45s/it, D_Fake=11.0537, D_Loss=3.6890, D_Real=12.3883, G_Loss=0.0774]\n",
      "Epoch 27/100: 100%|██████████| 21/21 [02:44<00:00,  7.84s/it, D_Fake=13.6191, D_Loss=3.7271, D_Real=14.8675, G_Loss=0.0710]\n",
      "Epoch 28/100: 100%|██████████| 21/21 [03:09<00:00,  9.03s/it, D_Fake=16.6442, D_Loss=3.7712, D_Real=17.8007, G_Loss=-0.0104]\n",
      "Epoch 29/100: 100%|██████████| 21/21 [03:00<00:00,  8.57s/it, D_Fake=20.1198, D_Loss=3.9271, D_Real=21.0728, G_Loss=-0.0547]\n",
      "Epoch 30/100: 100%|██████████| 21/21 [03:04<00:00,  8.80s/it, D_Fake=24.0017, D_Loss=4.1030, D_Real=24.7314, G_Loss=-0.2481]\n",
      "Epoch 31/100: 100%|██████████| 21/21 [02:59<00:00,  8.55s/it, D_Fake=27.9559, D_Loss=4.2274, D_Real=28.5142, G_Loss=-0.4180]\n",
      "Epoch 32/100: 100%|██████████| 21/21 [02:59<00:00,  8.57s/it, D_Fake=31.7902, D_Loss=4.3935, D_Real=32.1354, G_Loss=-0.6698]\n",
      "Epoch 33/100: 100%|██████████| 21/21 [04:04<00:00, 11.65s/it, D_Fake=34.8081, D_Loss=4.6057, D_Real=34.8944, G_Loss=-0.9358]\n",
      "Epoch 34/100: 100%|██████████| 21/21 [04:53<00:00, 13.98s/it, D_Fake=36.0368, D_Loss=4.7788, D_Real=35.9037, G_Loss=-1.1512]\n",
      "Epoch 35/100: 100%|██████████| 21/21 [05:05<00:00, 14.53s/it, D_Fake=35.4020, D_Loss=4.8211, D_Real=35.1806, G_Loss=-1.4980]\n",
      "Epoch 36/100: 100%|██████████| 21/21 [04:58<00:00, 14.20s/it, D_Fake=34.3360, D_Loss=4.7975, D_Real=34.0923, G_Loss=-1.7508]\n",
      "Epoch 37/100: 100%|██████████| 21/21 [05:03<00:00, 14.45s/it, D_Fake=32.7707, D_Loss=4.6851, D_Real=32.5936, G_Loss=-1.7649]\n",
      "Epoch 38/100: 100%|██████████| 21/21 [04:50<00:00, 13.85s/it, D_Fake=31.9973, D_Loss=4.5480, D_Real=31.9114, G_Loss=-2.1033]\n",
      "Epoch 39/100: 100%|██████████| 21/21 [05:01<00:00, 14.35s/it, D_Fake=31.2147, D_Loss=4.4241, D_Real=31.2073, G_Loss=-2.3513]\n",
      "Epoch 40/100: 100%|██████████| 21/21 [04:58<00:00, 14.22s/it, D_Fake=30.7686, D_Loss=4.3383, D_Real=30.8016, G_Loss=-2.7460]\n",
      "Epoch 41/100: 100%|██████████| 21/21 [05:04<00:00, 14.49s/it, D_Fake=31.0905, D_Loss=4.1374, D_Real=31.2791, G_Loss=-3.2359]\n",
      "Epoch 42/100: 100%|██████████| 21/21 [05:00<00:00, 14.31s/it, D_Fake=32.5646, D_Loss=3.9159, D_Real=32.9297, G_Loss=-4.0064]\n",
      "Epoch 43/100: 100%|██████████| 21/21 [04:59<00:00, 14.27s/it, D_Fake=34.9469, D_Loss=3.7455, D_Real=35.4377, G_Loss=-4.6061]\n",
      "Epoch 44/100: 100%|██████████| 21/21 [03:06<00:00,  8.90s/it, D_Fake=38.2444, D_Loss=3.4453, D_Real=38.9908, G_Loss=-6.0573]\n",
      "Epoch 45/100: 100%|██████████| 21/21 [03:13<00:00,  9.21s/it, D_Fake=42.3895, D_Loss=3.1758, D_Real=43.3611, G_Loss=-7.4140]\n",
      "Epoch 46/100: 100%|██████████| 21/21 [05:12<00:00, 14.87s/it, D_Fake=47.0424, D_Loss=2.8835, D_Real=48.2622, G_Loss=-9.0015]\n",
      "Epoch 47/100: 100%|██████████| 21/21 [05:03<00:00, 14.44s/it, D_Fake=51.3236, D_Loss=2.6130, D_Real=52.7699, G_Loss=-11.0493]\n",
      "Epoch 48/100: 100%|██████████| 21/21 [05:11<00:00, 14.83s/it, D_Fake=56.0217, D_Loss=2.1922, D_Real=57.8451, G_Loss=-13.1187]\n",
      "Epoch 49/100: 100%|██████████| 21/21 [05:03<00:00, 14.44s/it, D_Fake=61.1770, D_Loss=1.7940, D_Real=63.3553, G_Loss=-15.4904]\n",
      "Epoch 50/100: 100%|██████████| 21/21 [05:07<00:00, 14.66s/it, D_Fake=66.8874, D_Loss=1.3279, D_Real=69.4886, G_Loss=-18.3959]\n",
      "Epoch 51/100:  52%|█████▏    | 11/21 [02:47<02:32, 15.24s/it, D_Fake=71.6809, D_Loss=0.8930, D_Real=74.6843, G_Loss=-20.6492]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 760\u001b[39m\n\u001b[32m    758\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Starting WGAN-GP Training (v3.1 - Sparse & Stabilized) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    759\u001b[39m \u001b[38;5;66;03m# --- MODIFIED: Capture the returned histories ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m760\u001b[39m d_loss_hist, g_loss_hist, d_real_hist, d_fake_hist = \u001b[43mrun_wgan_gp_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreal_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m    \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m    \u001b[49m\u001b[43mN_CRITIC\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[43m)\u001b[49m \n\u001b[32m    767\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining completed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    769\u001b[39m \u001b[38;5;66;03m# --- ADDED: Call the new plotting function ---\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 484\u001b[39m, in \u001b[36mrun_wgan_gp_training\u001b[39m\u001b[34m(generator, discriminator, data_loader, epochs, n_critic)\u001b[39m\n\u001b[32m    481\u001b[39m x_fake_logits, adj_fake_logits = generator(z, target_embed_batch)\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     fake_data = \u001b[43mconvert_fake_to_SPARSE_data_vectorized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx_fake_logits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_fake_logits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_embed_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgumbel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fake_data.num_graphs == \u001b[32m0\u001b[39m:\n\u001b[32m    490\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 350\u001b[39m, in \u001b[36mconvert_fake_to_SPARSE_data_vectorized\u001b[39m\u001b[34m(x_fake_logits, adj_fake_logits, t_embed_batch, device, gumbel, temperature)\u001b[39m\n\u001b[32m    347\u001b[39m edge_indices.extend([[j, k], [k, j]])\n\u001b[32m    349\u001b[39m \u001b[38;5;66;03m# --- Create 4-dim one-hot vector ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m bond_attr = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBOND_FEAT_DIM_DISCRIMINATOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# Handle case where index might be out of bounds if something is wrong\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m0\u001b[39m <= bond_type_idx < BOND_FEAT_DIM_DISCRIMINATOR:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "# --- IMPORT THE PyG DATALOADER FOR BATCHING ---\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader \n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.utils import softmax\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import Descriptors, QED, AllChem\n",
    "from Bio import SeqIO\n",
    "import random\n",
    "import re\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# --- 1. System & Configuration ---\n",
    "\n",
    "# --- Configuration (UPDATE THESE PATHS & ID) ---\n",
    "CHEMPL_DB_PATH = 'DL_ENDSEM__DATASET/chembl_35/chembl_35_sqlite/chembl_35.db'\n",
    "BLAST_FASTA_PATH = 'DL_ENDSEM__DATASET/chembl_35_blast.fa.gz'\n",
    "TARGET_UNIPROT_ID = \"P00533\" # Example: EGFR Kinase\n",
    "\n",
    "# --- ATOM_CLASSES from your (v2) script ---\n",
    "# This is the list that gave you 9 batches\n",
    "ATOM_CLASSES = [6, 7, 8, 9, 15, 16, 17, 35, 53] # C, N, O, F, P, S, Cl, Br, I\n",
    "ATOM_CLASSES_MAP = {num: i for i, num in enumerate(ATOM_CLASSES)}\n",
    "ATOM_FEAT_DIM = len(ATOM_CLASSES) # Now 9\n",
    "\n",
    "# Define bond types RDKit knows\n",
    "BOND_CLASSES_RDKIT = [Chem.BondType.SINGLE, Chem.BondType.DOUBLE, Chem.BondType.TRIPLE, Chem.BondType.AROMATIC]\n",
    "# --- Add a 5th \"No Bond\" class for the Generator ---\n",
    "BOND_FEAT_DIM_GENERATOR = len(BOND_CLASSES_RDKIT) + 1 # Now 5\n",
    "NO_BOND_IDX = len(BOND_CLASSES_RDKIT) # Index 4\n",
    "\n",
    "# --- Discriminator only sees 4 bond types ---\n",
    "BOND_FEAT_DIM_DISCRIMINATOR = len(BOND_CLASSES_RDKIT) # Back to 4\n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "Z_DIM = 100         # Latent noise dimension\n",
    "EMBED_DIM = 128     # Hidden dimension for the Graph Transformer\n",
    "T_EMBED_DIM = 1024  # Target embedding dimension (from ProtT5)\n",
    "LAMBDA_GP = 10.0    # Gradient Penalty weight\n",
    "MAX_NODES = 30      # Max atoms in generated molecules (for Generator tensor shape)\n",
    "N_CRITIC = 3        # Discriminator training steps per Generator step\n",
    "EPOCHS = 100        # Run for 100 epochs\n",
    "BATCH_SIZE = 64     # Your increased batch size\n",
    "CPU_WORKERS = 4     \n",
    "\n",
    "# --- CUDA Check ---\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ CUDA is available! GPU will be used for training.\")\n",
    "    print(f\"PyTorch CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    print(\"❌ CUDA not found. Running on CPU.\")\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "# --- 2. Real Protein Embedding Generation (No changes) ---\n",
    "\n",
    "def load_target_sequence(fasta_path, uniprot_id):\n",
    "    # (No changes to this function)\n",
    "    print(f\"Opening gzipped FASTA file: {fasta_path}\")\n",
    "    try:\n",
    "        with gzip.open(fasta_path, \"rt\") as handle:\n",
    "            for record in SeqIO.parse(handle, \"fasta\"):\n",
    "                if uniprot_id in record.id or uniprot_id in record.description:\n",
    "                    return str(record.seq)\n",
    "            print(f\"Warning: Could not find sequence for {uniprot_id} in {fasta_path}\")\n",
    "            return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FATAL ERROR: FASTA file not found at {fasta_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL ERROR: Could not read FASTA file. Error: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_protein_embedding(sequence, device):\n",
    "    # (No changes to this function)\n",
    "    print(\"Loading ProtT5 model... (This may take a moment)\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "    model = T5EncoderModel.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc').to(device)\n",
    "    model.eval() \n",
    "    sequence_preprocessed = \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))\n",
    "    inputs = tokenizer(sequence_preprocessed, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = model(**inputs).last_hidden_state\n",
    "    protein_vec = embedding.mean(dim=1).squeeze(0)\n",
    "    print(f\"Generated protein embedding of shape: {protein_vec.shape}\")\n",
    "    return protein_vec\n",
    "\n",
    "# --- Generate the REAL Target Embedding ---\n",
    "target_seq = load_target_sequence(BLAST_FASTA_PATH, TARGET_UNIPROT_ID)\n",
    "if target_seq is None:\n",
    "    raise ValueError(f\"Target sequence for {TARGET_UNIPROT_ID} not found. Exiting.\")\n",
    "TARGET_EMBED = get_protein_embedding(target_seq, DEVICE)\n",
    "\n",
    "\n",
    "# --- 3. Data Pipeline (Molecules -> Graphs) (v3 Logic) ---\n",
    "\n",
    "def extract_potent_inhibitors(db_path, uniprot_id, potency_cutoff_nM=100):\n",
    "    # (No changes to this function)\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        sql_query = f\"\"\"\n",
    "        SELECT DISTINCT cs.canonical_smiles\n",
    "        FROM activities acts\n",
    "        JOIN assays a ON acts.assay_id = a.assay_id\n",
    "        JOIN target_dictionary td ON a.tid = td.tid\n",
    "        JOIN target_components tc ON td.tid = tc.tid\n",
    "        JOIN component_sequences cseq ON tc.component_id = cseq.component_id\n",
    "        JOIN compound_structures cs ON acts.molregno = cs.molregno\n",
    "        WHERE\n",
    "            cseq.accession = '{uniprot_id}' AND\n",
    "            acts.standard_type = 'IC50' AND\n",
    "            acts.standard_units = 'nM' AND\n",
    "            acts.standard_relation = '=' AND\n",
    "            acts.standard_value <= {potency_cutoff_nM}\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(sql_query, conn)\n",
    "        conn.close()\n",
    "        print(f\"Found {len(df)} potent inhibitors for UniProt ID {uniprot_id}.\")\n",
    "        return df['canonical_smiles'].unique().tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during database query: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_atom_features(atom):\n",
    "    \"\"\"Creates a one-hot vector for the atom type.\"\"\"\n",
    "    atom_num = atom.GetAtomicNum()\n",
    "    if atom_num not in ATOM_CLASSES_MAP:\n",
    "        return None # Atom is not in our allowed list\n",
    "        \n",
    "    atom_index = ATOM_CLASSES_MAP[atom_num]\n",
    "    atom_one_hot = torch.zeros(ATOM_FEAT_DIM, dtype=torch.float)\n",
    "    atom_one_hot[atom_index] = 1.0\n",
    "    return atom_one_hot\n",
    "\n",
    "def smiles_to_graph(smiles, target_embed):\n",
    "    \"\"\"Converts SMILES to a PyG Data object.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol: return None\n",
    "    if mol.GetNumAtoms() > MAX_NODES: return None\n",
    "\n",
    "    atom_features_list = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        features = get_atom_features(atom)\n",
    "        if features is None: # Skip molecule if it contains an invalid atom\n",
    "            return None\n",
    "        atom_features_list.append(features)\n",
    "\n",
    "    if not atom_features_list:\n",
    "        return None\n",
    "    x = torch.stack(atom_features_list)\n",
    "    \n",
    "    edge_indices, edge_attrs = [], []\n",
    "    # --- Use the 4-class bond list ---\n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        # Create a 4-dim one-hot vector\n",
    "        bond_type_oh = [int(bond.GetBondType() == t) for t in BOND_CLASSES_RDKIT]\n",
    "        \n",
    "        # --- Ensure bond type is one we recognize ---\n",
    "        if sum(bond_type_oh) == 1: #i.e., it's S, D, T, or Aromatic\n",
    "            edge_indices.extend([[i, j], [j, i]])\n",
    "            edge_attrs.extend([bond_type_oh, bond_type_oh])\n",
    "\n",
    "    # --- Return None if molecule has no *recognized* bonds ---\n",
    "    if not edge_indices: \n",
    "        return None\n",
    "    \n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.float) # Shape: [num_bonds, 4]\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, \n",
    "                target_embed=target_embed.unsqueeze(0))\n",
    "    return data\n",
    "\n",
    "# --- Data Pipeline Execution ---\n",
    "inhibitor_smiles = extract_potent_inhibitors(CHEMPL_DB_PATH, TARGET_UNIPROT_ID)\n",
    "real_data_list = [smiles_to_graph(s, TARGET_EMBED.cpu()) for s in inhibitor_smiles]\n",
    "real_data_list = [d for d in real_data_list if d is not None]\n",
    "\n",
    "if not real_data_list:\n",
    "    print(f\"FATAL: No valid inhibitor data found (or all were filtered out). Check ATOM_CLASSES and BOND_CLASSES.\")\n",
    "    exit()\n",
    "\n",
    "# --- USE THE STANDARD PyG DATALOADER ---\n",
    "real_loader = PyGDataLoader(\n",
    "    real_data_list, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=CPU_WORKERS, \n",
    "    pin_memory=True \n",
    ")\n",
    "print(f\"Prepared {len(real_data_list)} real graph samples for training.\")\n",
    "\n",
    "\n",
    "# --- 4. Model Architecture (v3 Logic) ---\n",
    "\n",
    "# --- 4.1. Relational Graph Transformer Layer (No changes) ---\n",
    "class RelationalGraphTransformerLayer(MessagePassing):\n",
    "    # (No changes to this class)\n",
    "    def __init__(self, in_channels, out_channels, edge_dim, heads=4, dropout=0.1, **kwargs):\n",
    "        super().__init__(aggr='add', node_dim=0, **kwargs)\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.lin_q = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_k = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_v = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_edge = nn.Linear(edge_dim, heads * out_channels, bias=False) \n",
    "        self.att_coeff = nn.Parameter(torch.Tensor(1, heads, 2 * out_channels)) \n",
    "        self.lin_out = nn.Linear(heads * out_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.xavier_uniform_(self.att_coeff)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        Q = self.lin_q(x).view(-1, self.heads, self.out_channels)\n",
    "        K = self.lin_k(x).view(-1, self.heads, self.out_channels)\n",
    "        V = self.lin_v(x).view(-1, self.heads, self.out_channels)\n",
    "        E = self.lin_edge(edge_attr).view(-1, self.heads, self.out_channels)\n",
    "        out = self.propagate(edge_index, Q=Q, K=K, V=V, E=E)\n",
    "        out = out.view(-1, self.heads * self.out_channels)\n",
    "        out = self.lin_out(out)\n",
    "        return out\n",
    "\n",
    "    def message(self, Q_i, K_j, V_j, E_k, index):\n",
    "        try: is_empty = (E_k.size(0) == 0)\n",
    "        except AttributeError: is_empty = True\n",
    "        if is_empty: E_bias = torch.zeros(Q_i.size(0), self.heads, 1, device=self.lin_q.weight.device)\n",
    "        else: E_bias = E_k.mean(dim=-1, keepdim=True) \n",
    "        QK_cat = torch.cat([Q_i, K_j], dim=-1)\n",
    "        e_ij = (QK_cat * self.att_coeff).sum(dim=-1, keepdim=True)\n",
    "        e_ij = e_ij + E_bias\n",
    "        e_ij = F.leaky_relu(e_ij)\n",
    "        alpha = softmax(e_ij, index)\n",
    "        alpha = self.dropout(alpha)\n",
    "        return V_j * alpha.view(-1, self.heads, 1)\n",
    "\n",
    "# --- 4.2. Discriminator (v3 Logic) ---\n",
    "class Discriminator(nn.Module):\n",
    "    # --- edge_dim is BOND_FEAT_DIM_DISCRIMINATOR (4) ---\n",
    "    def __init__(self, node_features, edge_dim, t_embed_dim, embed_dim, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_d = node_features if i == 0 else embed_dim\n",
    "            # --- This layer now expects edge_dim = 4 ---\n",
    "            self.layers.append(RelationalGraphTransformerLayer(in_d, embed_dim, edge_dim))\n",
    "        self.lin_final = nn.Linear(embed_dim + t_embed_dim, 1)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        t_embed = data.target_embed\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        graph_embed = global_mean_pool(x, batch)\n",
    "        if t_embed.dim() > 2: t_embed = t_embed.squeeze(1) \n",
    "        final_input = torch.cat([graph_embed, t_embed], dim=1)\n",
    "        return self.lin_final(final_input).squeeze(1)\n",
    "\n",
    "# --- 4.3. Generator (v3 Logic) ---\n",
    "class Generator(nn.Module):\n",
    "    # --- bond_features is BOND_FEAT_DIM_GENERATOR (5) ---\n",
    "    def __init__(self, z_dim, t_embed_dim, node_features, bond_features, max_nodes=MAX_NODES):\n",
    "        super().__init__()\n",
    "        self.max_nodes = max_nodes\n",
    "        self.node_features = node_features\n",
    "        self.bond_features = bond_features\n",
    "        self.lin_x = nn.Sequential(nn.Linear(z_dim + t_embed_dim, 256), nn.ReLU(), nn.Linear(256, max_nodes * node_features))\n",
    "        # --- This layer now outputs 5 features per bond ---\n",
    "        self.lin_adj = nn.Sequential(nn.Linear(z_dim + t_embed_dim, 256), nn.ReLU(), nn.Linear(256, max_nodes * max_nodes * bond_features))\n",
    "\n",
    "    def forward(self, z, t_embed):\n",
    "        zt = torch.cat([z, t_embed], dim=1)\n",
    "        x_fake_logits = self.lin_x(zt).view(-1, self.max_nodes, self.node_features)\n",
    "        adj_fake_logits = self.lin_adj(zt).view(-1, self.max_nodes, self.max_nodes, self.bond_features)\n",
    "        return x_fake_logits, adj_fake_logits\n",
    "\n",
    "# --- Model Initialization ---\n",
    "print(\"Initializing models...\")\n",
    "# --- Pass the correct dimensions ---\n",
    "generator = Generator(Z_DIM, T_EMBED_DIM, ATOM_FEAT_DIM, BOND_FEAT_DIM_GENERATOR).to(DEVICE)\n",
    "discriminator = Discriminator(ATOM_FEAT_DIM, BOND_FEAT_DIM_DISCRIMINATOR, T_EMBED_DIM, EMBED_DIM).to(DEVICE)\n",
    "\n",
    "# --- !!!!!!!!!!!!!!!!!!!!!!!!!!! ---\n",
    "# --- 1st CHANGE: Lowered Learning Rate ---\n",
    "# --- !!!!!!!!!!!!!!!!!!!!!!!!!!! ---\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=1e-5, betas=(0.5, 0.9)) # Was 1e-4\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=5e-6, betas=(0.5, 0.9)) # <-- REDUCE D's LR (e.g., by 2-5x)\n",
    "\n",
    "\n",
    "# --- 5. Training Utilities (v3 Logic) ---\n",
    "\n",
    "# --- 5.1. Sparse Graph Conversion ---\n",
    "def convert_fake_to_SPARSE_data_vectorized(x_fake_logits, adj_fake_logits, t_embed_batch, device, gumbel=False, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Converts Generator output (logits) to a BATCH of sparse PyG Data objects.\n",
    "    \"\"\"\n",
    "    batch_size = x_fake_logits.size(0)\n",
    "    data_list = []\n",
    "\n",
    "    # 1. Sample nodes (still vectorized)\n",
    "    if gumbel:\n",
    "        x_fake_tensor = F.gumbel_softmax(x_fake_logits, tau=temperature, hard=True)\n",
    "    else:\n",
    "        x_indices = torch.argmax(x_fake_logits, dim=-1)\n",
    "        x_fake_tensor = F.one_hot(x_indices, num_classes=ATOM_FEAT_DIM).float()\n",
    "\n",
    "    # 2. Sample bonds (still vectorized)\n",
    "    if gumbel:\n",
    "        adj_fake_tensor = F.gumbel_softmax(adj_fake_logits, tau=temperature, hard=True)\n",
    "    else:\n",
    "        adj_indices = torch.argmax(adj_fake_logits, dim=-1)\n",
    "        adj_fake_tensor = F.one_hot(adj_indices, num_classes=BOND_FEAT_DIM_GENERATOR).float()\n",
    "    \n",
    "    # 3. Loop over batch to build sparse graphs\n",
    "    for i in range(batch_size):\n",
    "        x = x_fake_tensor[i] # Shape [N, ATOM_FEAT_DIM]\n",
    "        adj_full = adj_fake_tensor[i] # Shape [N, N, BOND_FEAT_DIM_GENERATOR]\n",
    "        \n",
    "        edge_indices = []\n",
    "        edge_attrs = []\n",
    "\n",
    "        # Iterate over upper triangle\n",
    "        for j in range(MAX_NODES):\n",
    "            for k in range(j + 1, MAX_NODES):\n",
    "                bond_logits = adj_full[j, k] # Shape [5]\n",
    "                bond_type_idx = torch.argmax(bond_logits).item()\n",
    "                \n",
    "                # --- Check if it's NOT a \"No Bond\" ---\n",
    "                if bond_type_idx != NO_BOND_IDX:\n",
    "                    # Add this edge\n",
    "                    edge_indices.extend([[j, k], [k, j]])\n",
    "                    \n",
    "                    # --- Create 4-dim one-hot vector ---\n",
    "                    bond_attr = torch.zeros(BOND_FEAT_DIM_DISCRIMINATOR, device=device)\n",
    "                    # Handle case where index might be out of bounds if something is wrong\n",
    "                    if 0 <= bond_type_idx < BOND_FEAT_DIM_DISCRIMINATOR:\n",
    "                         bond_attr[bond_type_idx] = 1.0\n",
    "                    edge_attrs.extend([bond_attr, bond_attr])\n",
    "\n",
    "        if not edge_indices:\n",
    "            # No bonds were formed, create a dummy to avoid errors\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long, device=device)\n",
    "            edge_attr = torch.empty((0, BOND_FEAT_DIM_DISCRIMINATOR), dtype=torch.float, device=device)\n",
    "        else:\n",
    "            edge_index = torch.tensor(edge_indices, dtype=torch.long, device=device).t().contiguous()\n",
    "            edge_attr = torch.stack(edge_attrs)\n",
    "\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            target_embed=t_embed_batch[i].unsqueeze(0) # Embed for this single graph\n",
    "        )\n",
    "        data_list.append(data)\n",
    "\n",
    "    # 4. Re-batch the sparse graphs\n",
    "    \n",
    "    # Use a loader to properly collate the list of Data objects into a Batch object\n",
    "    temp_loader = PyGDataLoader(data_list, batch_size=batch_size, shuffle=False)\n",
    "    batch = next(iter(temp_loader))\n",
    "    \n",
    "    # Squeeze the target_embed back to [B, T_EMBED_DIM]\n",
    "    batch.target_embed = batch.target_embed.squeeze(1)\n",
    "    \n",
    "    return batch.to(device)\n",
    "\n",
    "\n",
    "# --- 5.2. WGAN-GP Gradient Penalty (v3 Logic) ---\n",
    "def calculate_gradient_penalty(discriminator, real_data, fake_data, lambda_gp, device):\n",
    "    \"\"\"\n",
    "    Interpolates on the GLOBAL graph embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Get graph embeddings first ---\n",
    "    discriminator.eval() # Freeze discriminator for this part\n",
    "    \n",
    "    real_x, real_edge_index, real_edge_attr, real_batch = real_data.x, real_data.edge_index, real_data.edge_attr, real_data.batch\n",
    "    real_t_embed = real_data.target_embed\n",
    "    for layer in discriminator.layers:\n",
    "        real_x = layer(real_x, real_edge_index, real_edge_attr)\n",
    "        real_x = F.relu(real_x)\n",
    "    real_graph_embed = global_mean_pool(real_x, real_batch)\n",
    "    \n",
    "    fake_x, fake_edge_index, fake_edge_attr, fake_batch = fake_data.x, fake_data.edge_index, fake_data.edge_attr, fake_data.batch\n",
    "    # --- Handle case where fake batch might be empty ---\n",
    "    if fake_data.num_graphs == 0:\n",
    "        discriminator.train()\n",
    "        # Return a 0 penalty if there's nothing to compare\n",
    "        return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "        \n",
    "    fake_t_embed = fake_data.target_embed\n",
    "    for layer in discriminator.layers:\n",
    "        fake_x = layer(fake_x, fake_edge_index, fake_edge_attr)\n",
    "        fake_x = F.relu(fake_x)\n",
    "    fake_graph_embed = global_mean_pool(fake_x, fake_batch)\n",
    "    \n",
    "    discriminator.train() # Unfreeze\n",
    "    \n",
    "    # Match batch sizes if they differ\n",
    "    batch_size = min(real_graph_embed.size(0), fake_graph_embed.size(0))\n",
    "    if batch_size == 0:\n",
    "        return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "        \n",
    "    real_graph_embed = real_graph_embed[:batch_size]\n",
    "    fake_graph_embed = fake_graph_embed[:batch_size]\n",
    "    real_t_embed = real_t_embed[:batch_size]\n",
    "\n",
    "    # --- Interpolate on graph_embed ---\n",
    "    alpha = torch.rand(batch_size, 1).to(device)\n",
    "    interpolated_embed = (alpha * real_graph_embed) + ((1 - alpha) * fake_graph_embed)\n",
    "    interpolated_embed.requires_grad_(True)\n",
    "    \n",
    "    # --- Combine with target and pass to *final layer only* ---\n",
    "    final_input = torch.cat([interpolated_embed, real_t_embed], dim=1)\n",
    "    disc_interpolates = discriminator.lin_final(final_input).squeeze(1)\n",
    "\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=disc_interpolates, inputs=interpolated_embed, # Grad w.r.t. interpolated_embed\n",
    "        grad_outputs=torch.ones_like(disc_interpolates),\n",
    "        create_graph=True, retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = lambda_gp * ((gradient_norm - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty\n",
    "\n",
    "# --- 6. Main Training Loop (v3 Logic + STABILIZATION) ---\n",
    "# --- 6. Main Training Loop (v3 Logic + STABILIZATION) ---\n",
    "def run_wgan_gp_training(generator, discriminator, data_loader, epochs, n_critic):\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    # --- ADDED: History lists to store epoch averages ---\n",
    "    d_loss_history = []\n",
    "    g_loss_history = []\n",
    "    d_real_history = []\n",
    "    d_fake_history = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # --- Reset sums for each epoch ---\n",
    "        g_loss_sum, d_loss_sum = 0, 0\n",
    "        d_real_sum, d_fake_sum = 0, 0\n",
    "        \n",
    "        progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "        \n",
    "        for batch_idx, real_data in enumerate(progress_bar):\n",
    "            real_data = real_data.to(DEVICE)\n",
    "            \n",
    "            if real_data.num_graphs < 2: # Need at least 2 for GP\n",
    "                continue\n",
    "                \n",
    "            batch_size = real_data.num_graphs\n",
    "            target_embed_batch = real_data.target_embed\n",
    "\n",
    "            # 1. Train Discriminator (n_critic steps)\n",
    "            for _ in range(n_critic):\n",
    "                optimizer_D.zero_grad()\n",
    "                \n",
    "                d_real = discriminator(real_data).mean()\n",
    "                \n",
    "                z = torch.randn(batch_size, Z_DIM).to(DEVICE)\n",
    "                x_fake_logits, adj_fake_logits = generator(z, target_embed_batch)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    fake_data = convert_fake_to_SPARSE_data_vectorized(\n",
    "                        x_fake_logits.detach(), adj_fake_logits.detach(), \n",
    "                        target_embed_batch, DEVICE, gumbel=False\n",
    "                    )\n",
    "                \n",
    "                if fake_data.num_graphs == 0:\n",
    "                    continue\n",
    "                \n",
    "                d_fake = discriminator(fake_data).mean()\n",
    "                gp = calculate_gradient_penalty(discriminator, real_data, fake_data, LAMBDA_GP, DEVICE)\n",
    "                d_loss = - (d_real - d_fake) + gp\n",
    "                d_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(discriminator.parameters(), 1.0)\n",
    "                optimizer_D.step()\n",
    "                \n",
    "                d_loss_sum += d_loss.item()\n",
    "                d_real_sum += d_real.item() \n",
    "                d_fake_sum += d_fake.item()\n",
    "            \n",
    "            # 2. Train Generator (1 step)\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size, Z_DIM).to(DEVICE)\n",
    "            x_fake_logits, adj_fake_logits = generator(z, target_embed_batch)\n",
    "            \n",
    "            fake_data = convert_fake_to_SPARSE_data_vectorized(\n",
    "                x_fake_logits, adj_fake_logits, \n",
    "                target_embed_batch, DEVICE, gumbel=True\n",
    "            )\n",
    "            \n",
    "            if fake_data.num_graphs == 0:\n",
    "                continue\n",
    "\n",
    "            g_loss = - discriminator(fake_data).mean()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            g_loss_sum += g_loss.item()\n",
    "            \n",
    "            # --- Update postfix (calculates average for batches *so far*) ---\n",
    "            num_batches = batch_idx + 1\n",
    "            progress_bar.set_postfix(\n",
    "                D_Loss=f\"{(d_loss_sum / num_batches / n_critic):.4f}\", \n",
    "                G_Loss=f\"{(g_loss_sum / num_batches):.4f}\",\n",
    "                D_Real=f\"{(d_real_sum / num_batches / n_critic):.4f}\",\n",
    "                D_Fake=f\"{(d_fake_sum / num_batches / n_critic):.4f}\"\n",
    "            )\n",
    "        \n",
    "        # --- ADDED: Store the final average for the completed epoch ---\n",
    "        num_batches_total = len(data_loader)\n",
    "        d_loss_history.append(d_loss_sum / num_batches_total / n_critic)\n",
    "        g_loss_history.append(g_loss_sum / num_batches_total)\n",
    "        d_real_history.append(d_real_sum / num_batches_total / n_critic)\n",
    "        d_fake_history.append(d_fake_sum / num_batches_total / n_critic)\n",
    "\n",
    "    # --- ADDED: Return the histories ---\n",
    "    return d_loss_history, g_loss_history, d_real_history, d_fake_history\n",
    "# --- 7. Generation & SMILES Conversion (v3 Logic) ---\n",
    "\n",
    "def tensors_to_smiles(x_fake_one_hot, adj_fake_logits):\n",
    "    \"\"\"\n",
    "    Converts raw generator tensor output (one-hot nodes) into SMILES strings.\n",
    "    \"\"\"\n",
    "    # --- Get atomic number indices from one-hot nodes ---\n",
    "    x_fake_indices = torch.argmax(x_fake_one_hot, dim=-1).cpu().detach()\n",
    "    adj_fake_logits = adj_fake_logits.cpu().detach()\n",
    "    \n",
    "    # --- Get bond indices from 5-class logits ---\n",
    "    adj_bond_type_idx = torch.argmax(adj_fake_logits, dim=-1)\n",
    "    \n",
    "    batch_size = x_fake_indices.size(0)\n",
    "    generated_smiles = []\n",
    "    generated_mols = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        mol = Chem.RWMol()\n",
    "        atom_map = {} # Map from tensor index (0..MAX_NODES-1) to RDKit atom index\n",
    "        \n",
    "        # 1. Add atoms\n",
    "        for j in range(MAX_NODES):\n",
    "            atom_idx = x_fake_indices[i, j].item()\n",
    "            atom_num = ATOM_CLASSES[atom_idx]\n",
    "            \n",
    "            atom = Chem.Atom(atom_num)\n",
    "            rdkit_idx = mol.AddAtom(atom)\n",
    "            atom_map[j] = rdkit_idx\n",
    "                \n",
    "        # 2. Add bonds\n",
    "        for j in range(MAX_NODES):\n",
    "            for k in range(j + 1, MAX_NODES):\n",
    "                # --- Get bond type from 5-class indices ---\n",
    "                bond_type_idx = adj_bond_type_idx[i, j, k].item()\n",
    "                \n",
    "                # --- Add bond IF NOT \"No Bond\" ---\n",
    "                if bond_type_idx != NO_BOND_IDX:\n",
    "                    # --- Check if bond_type_idx is valid for RDKit bonds ---\n",
    "                    if 0 <= bond_type_idx < len(BOND_CLASSES_RDKIT):\n",
    "                        bond_type = BOND_CLASSES_RDKIT[bond_type_idx]\n",
    "                        mol.AddBond(atom_map[j], atom_map[k], bond_type)\n",
    "        \n",
    "        # 3. Sanitize and Convert\n",
    "        try:\n",
    "            Chem.SanitizeMol(mol)\n",
    "            smi = Chem.MolToSmiles(mol)\n",
    "            \n",
    "            # --- Filter out disconnected fragments ---\n",
    "            if '.' in smi:\n",
    "                generated_smiles.append(None) # Invalid fragment\n",
    "                generated_mols.append(None)\n",
    "            else:\n",
    "                generated_smiles.append(smi)\n",
    "                generated_mols.append(mol)\n",
    "        except Exception as e:\n",
    "            # print(f\"RDKit Error: {e}\") # Uncomment for debugging\n",
    "            generated_smiles.append(None) # Invalid molecule\n",
    "            generated_mols.append(None)\n",
    "\n",
    "    valid_smiles = [s for s in generated_smiles if s is not None]\n",
    "    valid_mols = [m for m in generated_mols if m is not None]\n",
    "    \n",
    "    return valid_smiles, valid_mols, generated_smiles\n",
    "\n",
    "# --- 8. Performance Metrics & Plotting (v3 Logic) ---\n",
    "\n",
    "def calculate_and_plot_metrics(generator, target_embed, real_smiles_list, num_to_generate, device):\n",
    "    \"\"\"\n",
    "    Generates molecules and calculates Validity, Uniqueness, Novelty,\n",
    "    and plots property distributions.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Generation & Evaluation ---\")\n",
    "    warnings.filterwarnings('ignore', '.*Implicit valence.*') # Suppress RDKit warnings\n",
    "    \n",
    "    generator.eval() # Set generator to evaluation mode\n",
    "    \n",
    "    real_mols = [Chem.MolFromSmiles(s) for s in real_smiles_list]\n",
    "    real_mols = [m for m in real_mols if m is not None]\n",
    "    real_smiles_set = set(real_smiles_list)\n",
    "    \n",
    "    all_valid_smiles = []\n",
    "    all_valid_mols = []\n",
    "    total_attempts = 0 # Track total attempts\n",
    "\n",
    "    print(f\"Generating {num_to_generate} *valid* molecules for evaluation...\")\n",
    "    with torch.no_grad():\n",
    "        while len(all_valid_smiles) < num_to_generate:\n",
    "            batch_size = BATCH_SIZE\n",
    "            total_attempts += batch_size\n",
    "\n",
    "            z = torch.randn(batch_size, Z_DIM).to(device)\n",
    "            t_embed_batch = target_embed.unsqueeze(0).repeat(batch_size, 1)\n",
    "            \n",
    "            x_fake_logits, adj_fake_logits = generator(z, t_embed_batch)\n",
    "            \n",
    "            # --- Use argmax (not Gumbel) for final generation ---\n",
    "            x_indices = torch.argmax(x_fake_logits, dim=-1)\n",
    "            x_fake_one_hot = F.one_hot(x_indices, num_classes=ATOM_FEAT_DIM).float()\n",
    "            \n",
    "            # --- Pass 5-dim bond logits to smiles converter ---\n",
    "            valid_smiles, valid_mols, _ = tensors_to_smiles(x_fake_one_hot, adj_fake_logits)\n",
    "            \n",
    "            all_valid_smiles.extend(valid_smiles)\n",
    "            all_valid_mols.extend(valid_mols)\n",
    "            \n",
    "            print(f\"Generated: {len(all_valid_smiles)}/{num_to_generate} valid molecules...\", end='\\r')\n",
    "            \n",
    "            if total_attempts > num_to_generate * 50 and not all_valid_smiles:\n",
    "                 print(\"\\nError: Generated too many molecules with 0 validity. Stopping.\")\n",
    "                 break\n",
    "            if total_attempts > num_to_generate * 10 and len(all_valid_smiles) < num_to_generate: \n",
    "                 print(f\"\\nWarning: Low validity. Stopping generation at {len(all_valid_smiles)} molecules.\")\n",
    "                 break\n",
    "\n",
    "    print(\"\\nGeneration complete. Calculating metrics...\")\n",
    "    \n",
    "    # --- 1. Calculate Metrics ---\n",
    "    \n",
    "    if total_attempts == 0: total_attempts = 1\n",
    "    validity = len(all_valid_smiles) / total_attempts\n",
    "    \n",
    "    if len(all_valid_smiles) > 0:\n",
    "        uniqueness = len(set(all_valid_smiles)) / len(all_valid_smiles)\n",
    "    else:\n",
    "        uniqueness = 0.0\n",
    "        \n",
    "    if len(all_valid_smiles) > 0:\n",
    "        unique_valid_smiles = set(all_valid_smiles)\n",
    "        novel_smiles = unique_valid_smiles - real_smiles_set\n",
    "        novelty = len(novel_smiles) / len(unique_valid_smiles)\n",
    "    else:\n",
    "        novelty = 0.0\n",
    "\n",
    "    print(\"\\n--- Generative Performance Metrics ---\")\n",
    "    print(f\"Total Attempts: {total_attempts}\")\n",
    "    print(f\"Total Valid Generated: {len(all_valid_smiles)}\")\n",
    "    print(f\"✅ Validity:     {validity * 100:.2f}%\")\n",
    "    print(f\"🧬 Uniqueness:   {uniqueness * 100:.2f}%\")\n",
    "    print(f\"⭐ Novelty:      {novelty * 100:.2f}%\")\n",
    "    print(\"----------------------------------------\")\n",
    "    \n",
    "    if not all_valid_mols:\n",
    "        print(\"No valid molecules generated. Skipping plots.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Calculate Properties ---\n",
    "    props_real = {\n",
    "        'MolWt': [Descriptors.MolWt(m) for m in real_mols],\n",
    "        'LogP': [Descriptors.MolLogP(m) for m in real_mols],\n",
    "        'QED': [QED.qed(m) for m in real_mols]\n",
    "    }\n",
    "    \n",
    "    props_fake = {\n",
    "        'MolWt': [Descriptors.MolWt(m) for m in all_valid_mols],\n",
    "        'LogP': [Descriptors.MolLogP(m) for m in all_valid_mols],\n",
    "        'QED': [QED.qed(m) for m in all_valid_mols]\n",
    "    }\n",
    "\n",
    "    # --- 3. Plot Distributions ---\n",
    "    print(\"Generating property distribution plots...\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    plot_titles = ['Molecular Weight (MolWt)', 'LogP', 'Quantitative Esimation of Drug-likeness (QED)']\n",
    "    prop_keys = ['MolWt', 'LogP', 'QED']\n",
    "    \n",
    "    for ax, title, key in zip(axes, plot_titles, prop_keys):\n",
    "        ax.hist(props_real[key], bins=50, alpha=0.7, label='Real (Training)', color='blue', density=True)\n",
    "        ax.hist(props_fake[key], bins=50, alpha=0.7, label='Generated (Fake)', color='red', density=True)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(\"Value\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        ax.legend()\n",
    "        \n",
    "    plt.suptitle(f\"Property Distributions (Real vs. Generated) for {TARGET_UNIPROT_ID}\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(f\"property_plots_{TARGET_UNIPROT_ID}_v3.1.png\")\n",
    "    print(f\"Plots saved to property_plots_{TARGET_UNIPROT_ID}_v3.1.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    warnings.filterwarnings('default', '.*Implicit valence.*') # Restore warnings\n",
    "\n",
    "# --- NEW FUNCTION (insert before Section 9) ---\n",
    "def plot_training_losses(d_loss_hist, g_loss_hist, d_real_hist, d_fake_hist, target_id):\n",
    "    \"\"\"\n",
    "    Plots the training history of WGAN-GP losses and scores.\n",
    "    \"\"\"\n",
    "    print(\"Generating training loss plots...\")\n",
    "    epochs_range = range(1, len(g_loss_hist) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "    \n",
    "    # --- Plot 1: G_Loss vs D_Loss ---\n",
    "    ax1.plot(epochs_range, g_loss_hist, label='Generator Loss (G_Loss)', color='blue')\n",
    "    ax1.plot(epochs_range, d_loss_hist, label='Discriminator Loss (D_Loss)', color='red')\n",
    "    ax1.set_title(f\"Generator & Discriminator Losses for {target_id}\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # --- Plot 2: D(Real) vs D(Fake) ---\n",
    "    ax2.plot(epochs_range, d_real_hist, label='Avg. D(Real) Score', color='green')\n",
    "    ax2.plot(epochs_range, d_fake_hist, label='Avg. D(Fake) Score', color='orange')\n",
    "    ax2.set_title(f\"Critic Scores for {target_id}\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Score\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"training_loss_plots_{target_id}_v3.1.png\")\n",
    "    print(f\"Loss plots saved to training_loss_plots_{target_id}_v3.1.png\")\n",
    "    plt.show()\n",
    "# --- 9. --- Main Execution (Train & Evaluate) ---\n",
    "\n",
    "# --- 9. --- Main Execution (Train & Evaluate) ---\n",
    "\n",
    "# --- Execute Training ---\n",
    "print(\"\\n--- Starting WGAN-GP Training (v3.1 - Sparse & Stabilized) ---\")\n",
    "# --- MODIFIED: Capture the returned histories ---\n",
    "d_loss_hist, g_loss_hist, d_real_hist, d_fake_hist = run_wgan_gp_training(\n",
    "    generator, \n",
    "    discriminator, \n",
    "    real_loader, \n",
    "    EPOCHS, \n",
    "    N_CRITIC\n",
    ") \n",
    "print(\"\\nTraining completed.\")\n",
    "\n",
    "# --- ADDED: Call the new plotting function ---\n",
    "plot_training_losses(d_loss_hist, g_loss_hist, d_real_hist, d_fake_hist, TARGET_UNIPROT_ID)\n",
    "\n",
    "# --- Execute Evaluation ---\n",
    "num_to_eval = len(real_data_list) # --- Use count of *filtered* real data ---\n",
    "calculate_and_plot_metrics(generator, TARGET_EMBED, inhibitor_smiles, num_to_eval, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "509663f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd_loss_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- ADDED: Call the new plotting function ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m plot_training_losses(\u001b[43md_loss_hist\u001b[49m, g_loss_hist, d_real_hist, d_fake_hist, TARGET_UNIPROT_ID)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# --- Execute Evaluation ---\u001b[39;00m\n\u001b[32m      5\u001b[39m num_to_eval = \u001b[38;5;28mlen\u001b[39m(real_data_list) \u001b[38;5;66;03m# --- Use count of *filtered* real data ---\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'd_loss_hist' is not defined"
     ]
    }
   ],
   "source": [
    "# --- ADDED: Call the new plotting function ---\n",
    "plot_training_losses(d_loss_hist, g_loss_hist, d_real_hist, d_fake_hist, TARGET_UNIPROT_ID)\n",
    "\n",
    "# --- Execute Evaluation ---\n",
    "num_to_eval = len(real_data_list) # --- Use count of *filtered* real data ---\n",
    "calculate_and_plot_metrics(generator, TARGET_EMBED, inhibitor_smiles, num_to_eval, DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
