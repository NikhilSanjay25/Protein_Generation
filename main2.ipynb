{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1da76c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA is available! GPU will be used for training.\n",
      "PyTorch CUDA Version: 12.1\n",
      "GPU Name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Using device: cuda\n",
      "Opening gzipped FASTA file: DL_ENDSEM__DATASET/chembl_35_blast.fa.gz\n",
      "Loading ProtT5 model... (This may take a moment)\n",
      "Generated protein embedding of shape: torch.Size([1024])\n",
      "Found 3989 potent inhibitors for UniProt ID P00533.\n",
      "Prepared 1334 real graph samples for training.\n",
      "Initializing models...\n",
      "\n",
      "--- Starting WGAN-GP Training ---\n",
      "--- Epoch 1, Processing Batch 1/11 ---\n",
      "--- Epoch 1, Processing Batch 2/11 ---\n",
      "--- Epoch 1, Processing Batch 3/11 ---\n",
      "--- Epoch 1, Processing Batch 4/11 ---\n",
      "--- Epoch 1, Processing Batch 5/11 ---\n",
      "--- Epoch 1, Processing Batch 6/11 ---\n",
      "--- Epoch 1, Processing Batch 7/11 ---\n",
      "--- Epoch 1, Processing Batch 8/11 ---\n",
      "--- Epoch 1, Processing Batch 9/11 ---\n",
      "--- Epoch 1, Processing Batch 10/11 ---\n",
      "--- Epoch 1, Processing Batch 11/11 ---\n",
      "Epoch 1/10 | D Loss: -12.3766 | G Loss: -0.0959\n",
      "--- Epoch 2, Processing Batch 1/11 ---\n",
      "--- Epoch 2, Processing Batch 2/11 ---\n",
      "--- Epoch 2, Processing Batch 3/11 ---\n",
      "--- Epoch 2, Processing Batch 4/11 ---\n",
      "--- Epoch 2, Processing Batch 5/11 ---\n",
      "--- Epoch 2, Processing Batch 6/11 ---\n",
      "--- Epoch 2, Processing Batch 7/11 ---\n",
      "--- Epoch 2, Processing Batch 8/11 ---\n",
      "--- Epoch 2, Processing Batch 9/11 ---\n",
      "--- Epoch 2, Processing Batch 10/11 ---\n",
      "--- Epoch 2, Processing Batch 11/11 ---\n",
      "Epoch 2/10 | D Loss: -436.7272 | G Loss: -9.3535\n",
      "--- Epoch 3, Processing Batch 1/11 ---\n",
      "--- Epoch 3, Processing Batch 2/11 ---\n",
      "--- Epoch 3, Processing Batch 3/11 ---\n",
      "--- Epoch 3, Processing Batch 4/11 ---\n",
      "--- Epoch 3, Processing Batch 5/11 ---\n",
      "--- Epoch 3, Processing Batch 6/11 ---\n",
      "--- Epoch 3, Processing Batch 7/11 ---\n",
      "--- Epoch 3, Processing Batch 8/11 ---\n",
      "--- Epoch 3, Processing Batch 9/11 ---\n",
      "--- Epoch 3, Processing Batch 10/11 ---\n",
      "--- Epoch 3, Processing Batch 11/11 ---\n",
      "Epoch 3/10 | D Loss: -758.1954 | G Loss: -38.0152\n",
      "--- Epoch 4, Processing Batch 1/11 ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 512\u001b[39m\n\u001b[32m    510\u001b[39m \u001b[38;5;66;03m# --- Execute Training ---\u001b[39;00m\n\u001b[32m    511\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Starting WGAN-GP Training ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m \u001b[43mrun_wgan_gp_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_CRITIC\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m    513\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining completed.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 468\u001b[39m, in \u001b[36mrun_wgan_gp_training\u001b[39m\u001b[34m(generator, discriminator, data_loader, epochs, n_critic)\u001b[39m\n\u001b[32m    465\u001b[39m x_fake, adj_fake_logits = generator(z, target_embed_batch)\n\u001b[32m    467\u001b[39m \u001b[38;5;66;03m# --- D-step uses DISCRETE (non-differentiable) sampling ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m fake_data_list = \u001b[43mconvert_fake_to_data_discrete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_fake\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_fake_logits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_embed_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fake_data_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    472\u001b[39m \u001b[38;5;66;03m# Convert list of Data objects back into a batched Data object\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 377\u001b[39m, in \u001b[36mconvert_fake_to_data_discrete\u001b[39m\u001b[34m(x_fake_tensor, adj_fake_logits, t_embed_batch, device)\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# Hard .argmax() sampling\u001b[39;00m\n\u001b[32m    373\u001b[39m bond_type_index = adj_i_logits[r, c].argmax().item()\n\u001b[32m    374\u001b[39m bond_one_hot = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbond_type_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBOND_FEAT_DIM\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m edge_indices.append([r, c])\n\u001b[32m    380\u001b[39m edge_attrs_list.append(bond_one_hot)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.utils import softmax\n",
    "from rdkit import Chem\n",
    "from Bio import SeqIO\n",
    "import random\n",
    "import re\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import gzip  # <-- FIX 1: ADDED THIS IMPORT\n",
    "\n",
    "# --- 1. System & Configuration ---\n",
    "\n",
    "# --- Configuration (UPDATE THESE PATHS & ID) ---\n",
    "CHEMPL_DB_PATH = 'DL_ENDSEM__DATASET/chembl_35/chembl_35_sqlite/chembl_35.db'\n",
    "BLAST_FASTA_PATH = 'DL_ENDSEM__DATASET/chembl_35_blast.fa.gz'\n",
    "TARGET_UNIPROT_ID = \"P00533\" # Example: EGFR Kinase\n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "Z_DIM = 100         # Latent noise dimension\n",
    "ATOM_FEAT_DIM = 9   # Atom feature size\n",
    "BOND_FEAT_DIM = 4   # Bond feature size (Single, Double, Triple, Aromatic)\n",
    "EMBED_DIM = 128     # Hidden dimension for the Graph Transformer\n",
    "T_EMBED_DIM = 1024  # Target embedding dimension (from ProtT5)\n",
    "LAMBDA_GP = 10.0    # Gradient Penalty weight\n",
    "MAX_NODES = 30      # Max atoms in generated molecules (for Generator tensor shape)\n",
    "N_CRITIC = 5        # Discriminator training steps per Generator step\n",
    "EPOCHS = 10       # Total epochs\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# --- CUDA Check ---\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ CUDA is available! GPU will be used for training.\")\n",
    "    print(f\"PyTorch CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    print(\"❌ CUDA not found. Running on CPU.\")\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "# --- 2. Real Protein Embedding Generation ---\n",
    "\n",
    "# --- FIX 2: REPLACED THIS FUNCTION ---\n",
    "def load_target_sequence(fasta_path, uniprot_id):\n",
    "    \"\"\"Loads a protein sequence from a gzipped FASTA file.\"\"\"\n",
    "    print(f\"Opening gzipped FASTA file: {fasta_path}\")\n",
    "    try:\n",
    "        # 'rt' = read in text mode\n",
    "        with gzip.open(fasta_path, \"rt\") as handle:\n",
    "            for record in SeqIO.parse(handle, \"fasta\"):\n",
    "                if uniprot_id in record.id or uniprot_id in record.description:\n",
    "                    return str(record.seq)\n",
    "        print(f\"Warning: Could not find sequence for {uniprot_id} in {fasta_path}\")\n",
    "        return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FATAL ERROR: FASTA file not found at {fasta_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL ERROR: Could not read FASTA file. Error: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_protein_embedding(sequence, device):\n",
    "    \"\"\"\n",
    "    Generates a protein embedding using the pre-trained ProtT5 model.\n",
    "    This replaces the \"mock\" random embedding.\n",
    "    \"\"\"\n",
    "    print(\"Loading ProtT5 model... (This may take a moment)\")\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "    model = T5EncoderModel.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc').to(device)\n",
    "    model.eval() # Set to evaluation mode\n",
    "\n",
    "    # Pre-process sequence: add spaces between AAs and handle rare AAs\n",
    "    sequence_preprocessed = \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))\n",
    "    \n",
    "    inputs = tokenizer(sequence_preprocessed, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = model(**inputs).last_hidden_state\n",
    "    \n",
    "    # Average pool the embedding to get a single vector [1, 1024]\n",
    "    protein_vec = embedding.mean(dim=1).squeeze(0) # Squeeze to [1024]\n",
    "    print(f\"Generated protein embedding of shape: {protein_vec.shape}\")\n",
    "    return protein_vec\n",
    "\n",
    "# --- Generate the REAL Target Embedding ---\n",
    "target_seq = load_target_sequence(BLAST_FASTA_PATH, TARGET_UNIPROT_ID)\n",
    "if target_seq is None:\n",
    "    raise ValueError(f\"Target sequence for {TARGET_UNIPROT_ID} not found. Exiting.\")\n",
    "\n",
    "TARGET_EMBED = get_protein_embedding(target_seq, DEVICE)\n",
    "\n",
    "\n",
    "# --- 3. Data Pipeline (Molecules -> Graphs) ---\n",
    "\n",
    "def extract_potent_inhibitors(db_path, uniprot_id, potency_cutoff_nM=100):\n",
    "    \"\"\"\n",
    "    Connects to ChEMBL DB and extracts SMILES for potent inhibitors of a given Uniprot ID.\n",
    "    (Using your robust SQL query)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        sql_query = f\"\"\"\n",
    "        SELECT DISTINCT\n",
    "            cs.canonical_smiles\n",
    "        FROM\n",
    "            activities acts\n",
    "        JOIN assays a ON acts.assay_id = a.assay_id\n",
    "        JOIN target_dictionary td ON a.tid = td.tid\n",
    "        JOIN target_components tc ON td.tid = tc.tid\n",
    "        JOIN component_sequences cseq ON tc.component_id = cseq.component_id\n",
    "        JOIN compound_structures cs ON acts.molregno = cs.molregno\n",
    "        WHERE\n",
    "            cseq.accession = '{uniprot_id}' AND\n",
    "            acts.standard_type = 'IC50' AND\n",
    "            acts.standard_units = 'nM' AND\n",
    "            acts.standard_relation = '=' AND\n",
    "            acts.standard_value <= {potency_cutoff_nM}\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(sql_query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"Found {len(df)} potent inhibitors for UniProt ID {uniprot_id}.\")\n",
    "        return df['canonical_smiles'].unique().tolist()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during database query. This is likely due to missing tables or a critical file path issue.\")\n",
    "        raise RuntimeError(f\"Database Error: {e}. Please ensure the file is the full ChEMBL SQLite dump.\") from e\n",
    "\n",
    "def get_atom_features(atom):\n",
    "    \"\"\"Creates the 9-dimensional atom feature vector.\"\"\"\n",
    "    return [\n",
    "        atom.GetAtomicNum(), atom.GetDegree(), atom.GetFormalCharge(),\n",
    "        int(atom.GetIsAromatic()),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP2),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP3),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP3D),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP3D2)\n",
    "    ]\n",
    "\n",
    "def smiles_to_graph(smiles, target_embed):\n",
    "    \"\"\"Converts SMILES to a PyG Data object.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol: return None\n",
    "    if mol.GetNumAtoms() > MAX_NODES: return None\n",
    "\n",
    "    x = torch.tensor([get_atom_features(atom) for atom in mol.GetAtoms()], dtype=torch.float).to(DEVICE)\n",
    "    \n",
    "    edge_indices, edge_attrs = [], []\n",
    "    bond_types = [Chem.BondType.SINGLE, Chem.BondType.DOUBLE, Chem.BondType.TRIPLE, Chem.BondType.AROMATIC]\n",
    "    \n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        bond_type_oh = [int(bond.GetBondType() == t) for t in bond_types]\n",
    "        edge_indices.extend([[i, j], [j, i]])\n",
    "        edge_attrs.extend([bond_type_oh, bond_type_oh])\n",
    "\n",
    "    if not edge_indices: return None\n",
    "    \n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous().to(DEVICE)\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, \n",
    "                target_embed=target_embed.unsqueeze(0))\n",
    "    return data\n",
    "\n",
    "# --- Data Pipeline Execution ---\n",
    "inhibitor_smiles = extract_potent_inhibitors(CHEMPL_DB_PATH, TARGET_UNIPROT_ID)\n",
    "real_data_list = [smiles_to_graph(s, TARGET_EMBED) for s in inhibitor_smiles]\n",
    "real_data_list = [d for d in real_data_list if d is not None]\n",
    "\n",
    "if not real_data_list:\n",
    "    print(\"FATAL: No valid inhibitor data found. The model cannot be trained.\")\n",
    "    exit()\n",
    "\n",
    "real_loader = DataLoader(real_data_list, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(f\"Prepared {len(real_data_list)} real graph samples for training.\")\n",
    "\n",
    "\n",
    "# --- 4. Model Architecture ---\n",
    "\n",
    "# --- 4.1. Relational Graph Transformer Layer (No Changes) ---\n",
    "class RelationalGraphTransformerLayer(MessagePassing):\n",
    "    \"\"\"Graph Transformer Layer with explicit edge/bond feature integration.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, edge_dim, heads=4, dropout=0.1, **kwargs):\n",
    "        super().__init__(aggr='add', node_dim=0, **kwargs)\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        \n",
    "        self.lin_q = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_k = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_v = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_edge = nn.Linear(edge_dim, heads * out_channels, bias=False) \n",
    "        self.att_coeff = nn.Parameter(torch.Tensor(1, heads, 2 * out_channels)) \n",
    "        self.lin_out = nn.Linear(heads * out_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize attention coefficients (Xavier initialization)\n",
    "        nn.init.xavier_uniform_(self.att_coeff)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        Q = self.lin_q(x).view(-1, self.heads, self.out_channels)\n",
    "        K = self.lin_k(x).view(-1, self.heads, self.out_channels)\n",
    "        V = self.lin_v(x).view(-1, self.heads, self.out_channels)\n",
    "        E = self.lin_edge(edge_attr).view(-1, self.heads, self.out_channels)\n",
    "\n",
    "        out = self.propagate(edge_index, Q=Q, K=K, V=V, E=E)\n",
    "        out = out.view(-1, self.heads * self.out_channels)\n",
    "        out = self.lin_out(out)\n",
    "        return out\n",
    "\n",
    "    def message(self, Q_i, K_j, V_j, E_k, index):\n",
    "        try:\n",
    "            is_empty = (E_k.size(0) == 0)\n",
    "        except AttributeError:\n",
    "            is_empty = True\n",
    "        \n",
    "        if is_empty:\n",
    "            E_bias = torch.zeros(Q_i.size(0), self.heads, 1, device=self.lin_q.weight.device)\n",
    "        else:\n",
    "            E_bias = E_k.mean(dim=-1, keepdim=True) \n",
    "\n",
    "        QK_cat = torch.cat([Q_i, K_j], dim=-1)\n",
    "        e_ij = (QK_cat * self.att_coeff).sum(dim=-1, keepdim=True)\n",
    "        e_ij = e_ij + E_bias\n",
    "        \n",
    "        e_ij = F.leaky_relu(e_ij)\n",
    "        alpha = softmax(e_ij, index)\n",
    "        alpha = self.dropout(alpha)\n",
    "        \n",
    "        return V_j * alpha.view(-1, self.heads, 1)\n",
    "\n",
    "# --- 4.2. Discriminator (No Changes) ---\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, node_features, edge_dim, t_embed_dim, embed_dim, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_d = node_features if i == 0 else embed_dim\n",
    "            self.layers.append(RelationalGraphTransformerLayer(in_d, embed_dim, edge_dim))\n",
    "        self.lin_final = nn.Linear(embed_dim + t_embed_dim, 1)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        t_embed = data.target_embed\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        graph_embed = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Ensure t_embed is [batch_size, t_embed_dim]\n",
    "        if t_embed.dim() > 2:\n",
    "            t_embed = t_embed.squeeze(1) \n",
    "            \n",
    "        final_input = torch.cat([graph_embed, t_embed], dim=1)\n",
    "        \n",
    "        return self.lin_final(final_input).squeeze(1)\n",
    "\n",
    "# --- 4.3. Generator (FIXED: Outputs Logits) ---\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, t_embed_dim, node_features, bond_features, max_nodes=MAX_NODES):\n",
    "        super().__init__()\n",
    "        self.max_nodes = max_nodes\n",
    "        self.node_features = node_features\n",
    "        self.bond_features = bond_features\n",
    "        \n",
    "        self.lin_x = nn.Sequential(\n",
    "            nn.Linear(z_dim + t_embed_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, max_nodes * node_features)\n",
    "        )\n",
    "        self.lin_adj = nn.Sequential(\n",
    "            nn.Linear(z_dim + t_embed_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, max_nodes * max_nodes * bond_features) \n",
    "        )\n",
    "\n",
    "    def forward(self, z, t_embed):\n",
    "        zt = torch.cat([z, t_embed], dim=1)\n",
    "        \n",
    "        x_fake = self.lin_x(zt).view(-1, self.max_nodes, self.node_features)\n",
    "        \n",
    "        # Output raw logits for the adjacency matrix (NO SOFTMAX)\n",
    "        adj_fake_logits = self.lin_adj(zt).view(-1, self.max_nodes, self.max_nodes, self.bond_features)\n",
    "        \n",
    "        # Note: We don't softmax x_fake as it contains continuous features (e.g., atomic num)\n",
    "        \n",
    "        return x_fake, adj_fake_logits\n",
    "\n",
    "# --- Model Initialization ---\n",
    "print(\"Initializing models...\")\n",
    "generator = Generator(Z_DIM, T_EMBED_DIM, ATOM_FEAT_DIM, BOND_FEAT_DIM).to(DEVICE)\n",
    "discriminator = Discriminator(ATOM_FEAT_DIM, BOND_FEAT_DIM, T_EMBED_DIM, EMBED_DIM).to(DEVICE)\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "\n",
    "# --- 5. Training Utilities (FIXED) ---\n",
    "\n",
    "# --- 5.1. Differentiable Graph Conversion (NEW) ---\n",
    "def convert_fake_to_data_differentiable(x_fake_tensor, adj_fake_logits, t_embed_batch, device, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Converts Generator output to PyG Data using Gumbel-Softmax for the G-step.\n",
    "    This IS differentiable, allowing the generator to learn bond formation.\n",
    "    \"\"\"\n",
    "    batch_size = x_fake_tensor.size(0)\n",
    "    data_list = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        num_nodes = MAX_NODES\n",
    "        x_i = x_fake_tensor[i, :, :]\n",
    "        adj_i_logits = adj_fake_logits[i, :, :, :]\n",
    "        \n",
    "        edge_indices, edge_attrs_gumbel = [], []\n",
    "        \n",
    "        for r in range(num_nodes):\n",
    "            for c in range(num_nodes):\n",
    "                if r == c: continue\n",
    "                \n",
    "                # Gumbel-Softmax Sampling (Differentiable \"argmax\")\n",
    "                bond_probs = F.gumbel_softmax(\n",
    "                    adj_i_logits[r, c], \n",
    "                    tau=temperature, \n",
    "                    hard=True\n",
    "                )\n",
    "                \n",
    "                edge_indices.append([r, c])\n",
    "                edge_attrs_gumbel.append(bond_probs)\n",
    "\n",
    "        if not edge_indices: continue\n",
    "\n",
    "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous().to(device)\n",
    "        edge_attr = torch.stack(edge_attrs_gumbel).to(device)\n",
    "\n",
    "        data_list.append(Data(x=x_i, edge_index=edge_index, edge_attr=edge_attr, \n",
    "                              target_embed=t_embed_batch[i].unsqueeze(0)))\n",
    "    \n",
    "    if not data_list: return None\n",
    "    # We return the .dataset attribute for direct use in GP calculation\n",
    "    return DataLoader(data_list, batch_size=batch_size).dataset\n",
    "\n",
    "# --- 5.2. Discrete Graph Conversion (NEW) ---\n",
    "def convert_fake_to_data_discrete(x_fake_tensor, adj_fake_logits, t_embed_batch, device):\n",
    "    \"\"\"\n",
    "    Converts Generator output to PyG Data using .argmax() for the D-step.\n",
    "    This is NOT differentiable and is used when we don't need grads (faster).\n",
    "    \"\"\"\n",
    "    batch_size = x_fake_tensor.size(0)\n",
    "    data_list = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        num_nodes = MAX_NODES\n",
    "        x_i = x_fake_tensor[i, :, :].detach() # Detach all inputs\n",
    "        adj_i_logits = adj_fake_logits[i, :, :, :].detach()\n",
    "        \n",
    "        edge_indices, edge_attrs_list = [], []\n",
    "        \n",
    "        for r in range(num_nodes):\n",
    "            for c in range(num_nodes):\n",
    "                if r == c: continue\n",
    "                \n",
    "                # Hard .argmax() sampling\n",
    "                bond_type_index = adj_i_logits[r, c].argmax().item()\n",
    "                bond_one_hot = F.one_hot(\n",
    "                    torch.tensor(bond_type_index, device=device), \n",
    "                    num_classes=BOND_FEAT_DIM\n",
    "                ).float()\n",
    "                \n",
    "                edge_indices.append([r, c])\n",
    "                edge_attrs_list.append(bond_one_hot)\n",
    "\n",
    "        if not edge_indices: continue\n",
    "        \n",
    "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous().to(device)\n",
    "        edge_attr = torch.stack(edge_attrs_list).to(device)\n",
    "\n",
    "        data_list.append(Data(x=x_i, edge_index=edge_index, edge_attr=edge_attr, \n",
    "                              target_embed=t_embed_batch[i].unsqueeze(0)))\n",
    "    \n",
    "    if not data_list: return None\n",
    "    return DataLoader(data_list, batch_size=batch_size).dataset\n",
    "\n",
    "# --- 5.3. WGAN-GP Gradient Penalty (Preserved User's Fix) ---\n",
    "def calculate_gradient_penalty(discriminator, real_data, fake_data, lambda_gp, device):\n",
    "    \"\"\"Calculates the Gradient Penalty on interpolated node features (X).\"\"\"\n",
    "    \n",
    "    real_x = real_data.x.detach()\n",
    "    fake_x = fake_data.x.detach()\n",
    "    real_x_size = real_x.size(0)\n",
    "    \n",
    "    # --- FIX: Match fake_data.x size to real_data.x size for interpolation ---\n",
    "    if fake_x.size(0) > real_x_size:\n",
    "        fake_x = fake_x[:real_x_size]\n",
    "    elif fake_x.size(0) < real_x_size:\n",
    "        padding = torch.zeros(real_x_size - fake_x.size(0), fake_x.size(1), device=device)\n",
    "        fake_x = torch.cat([fake_x, padding], dim=0)\n",
    "\n",
    "    # 1. Linear Interpolation\n",
    "    alpha = torch.rand(real_x_size, 1).to(device) \n",
    "    interpolated_x = (alpha * real_x) + ((1 - alpha) * fake_x)\n",
    "    interpolated_x.requires_grad_(True)\n",
    "\n",
    "    # 2. Create interpolated Data object\n",
    "    # We use the real data's structure (edges, batch) as a template\n",
    "    interpolated_data = Data(x=interpolated_x, \n",
    "                             edge_index=real_data.edge_index, \n",
    "                             edge_attr=real_data.edge_attr, \n",
    "                             batch=real_data.batch, \n",
    "                             target_embed=real_data.target_embed)\n",
    "\n",
    "    disc_interpolates = discriminator(interpolated_data)\n",
    "    \n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=disc_interpolates, inputs=interpolated_x,\n",
    "        grad_outputs=torch.ones_like(disc_interpolates),\n",
    "        create_graph=True, retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = lambda_gp * ((gradient_norm - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty\n",
    "\n",
    "# --- 6. Main Training Loop (FIXED) ---\n",
    "def run_wgan_gp_training(generator, discriminator, data_loader, epochs, n_critic):\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        g_loss_sum, d_loss_sum = 0, 0\n",
    "        \n",
    "        for batch_idx, real_data in enumerate(data_loader):\n",
    "            # --- THIS IS THE LINE YOU REQUESTED ---\n",
    "            print(f\"--- Epoch {epoch}, Processing Batch {batch_idx+1}/{len(data_loader)} ---\")\n",
    "\n",
    "            real_data = real_data.to(DEVICE)\n",
    "            batch_size = real_data.num_graphs\n",
    "            \n",
    "            # Ensure target_embed_batch is [batch_size, T_EMBED_DIM]\n",
    "            target_embed_batch = real_data.target_embed\n",
    "            if target_embed_batch.dim() > 2:\n",
    "                target_embed_batch = target_embed_batch.view(batch_size, T_EMBED_DIM)\n",
    "\n",
    "            # 1. Train Discriminator (n_critic steps)\n",
    "            for _ in range(n_critic):\n",
    "                optimizer_D.zero_grad()\n",
    "                \n",
    "                # Real Loss\n",
    "                d_real = discriminator(real_data).mean()\n",
    "                \n",
    "                # Fake Loss\n",
    "                z = torch.randn(batch_size, Z_DIM).to(DEVICE)\n",
    "                x_fake, adj_fake_logits = generator(z, target_embed_batch)\n",
    "                \n",
    "                # --- D-step uses DISCRETE (non-differentiable) sampling ---\n",
    "                fake_data_list = convert_fake_to_data_discrete(\n",
    "                    x_fake.detach(), adj_fake_logits.detach(), target_embed_batch, DEVICE\n",
    "                )\n",
    "                if fake_data_list is None: continue\n",
    "                # Convert list of Data objects back into a batched Data object\n",
    "                fake_data_loader = DataLoader(fake_data_list, batch_size=batch_size)\n",
    "                fake_data = next(iter(fake_data_loader)).to(DEVICE)\n",
    "\n",
    "                d_fake = discriminator(fake_data).mean()\n",
    "                \n",
    "                # Gradient Penalty\n",
    "                gp = calculate_gradient_penalty(discriminator, real_data, fake_data, LAMBDA_GP, DEVICE)\n",
    "                \n",
    "                d_loss = - (d_real - d_fake) + gp\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "                d_loss_sum += d_loss.item()\n",
    "            \n",
    "            # 2. Train Generator (1 step)\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size, Z_DIM).to(DEVICE)\n",
    "            x_fake, adj_fake_logits = generator(z, target_embed_batch)\n",
    "            \n",
    "            # --- G-step uses DIFFERENTIABLE Gumbel-Softmax sampling ---\n",
    "            fake_data_list = convert_fake_to_data_differentiable(\n",
    "                x_fake, adj_fake_logits, target_embed_batch, DEVICE\n",
    "            )\n",
    "            if fake_data_list is None: continue\n",
    "            fake_data_loader = DataLoader(fake_data_list, batch_size=batch_size)\n",
    "            fake_data = next(iter(fake_data_loader)).to(DEVICE)\n",
    "            \n",
    "            # Generator Loss\n",
    "            g_loss = - discriminator(fake_data).mean()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            g_loss_sum += g_loss.item()\n",
    "            \n",
    "        avg_d_loss = d_loss_sum / len(data_loader) / n_critic\n",
    "        avg_g_loss = g_loss_sum / len(data_loader)\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} | D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f}\")\n",
    "\n",
    "# --- Execute Training ---\n",
    "print(\"\\n--- Starting WGAN-GP Training ---\")\n",
    "run_wgan_gp_training(generator, discriminator, real_loader, EPOCHS, N_CRITIC) \n",
    "print(\"\\nTraining completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce7cf618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA is available! GPU will be used for training.\n",
      "PyTorch CUDA Version: 12.1\n",
      "GPU Name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Using device: cuda\n",
      "Opening gzipped FASTA file: DL_ENDSEM__DATASET/chembl_35_blast.fa.gz\n",
      "Loading ProtT5 model... (This may take a moment)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated protein embedding of shape: torch.Size([1024])\n",
      "Found 3989 potent inhibitors for UniProt ID P00533.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikhi\\OneDrive\\Attachments\\Desktop\\Git_Repos\\NNDL_endsem\\venv\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 1334 real graph samples for training.\n",
      "Initializing models...\n",
      "\n",
      "--- Starting WGAN-GP Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/21 [00:00<?, ?it/s]c:\\Users\\nikhi\\OneDrive\\Attachments\\Desktop\\Git_Repos\\NNDL_endsem\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ..\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Epoch 1/10: 100%|██████████| 21/21 [00:21<00:00,  1.04s/it, D_Loss=-202.8050, G_Loss=-3.8393]\n",
      "Epoch 2/10: 100%|██████████| 21/21 [00:19<00:00,  1.06it/s, D_Loss=-761.2867, G_Loss=-47.0787]\n",
      "Epoch 3/10: 100%|██████████| 21/21 [00:27<00:00,  1.32s/it, D_Loss=-836.8307, G_Loss=-103.6194]\n",
      "Epoch 4/10: 100%|██████████| 21/21 [00:28<00:00,  1.37s/it, D_Loss=-772.8533, G_Loss=-158.7001]\n",
      "Epoch 5/10: 100%|██████████| 21/21 [00:28<00:00,  1.37s/it, D_Loss=-700.1403, G_Loss=-210.0796]\n",
      "Epoch 6/10: 100%|██████████| 21/21 [00:20<00:00,  1.01it/s, D_Loss=-626.3723, G_Loss=-258.0512]\n",
      "Epoch 7/10: 100%|██████████| 21/21 [00:26<00:00,  1.26s/it, D_Loss=-543.0943, G_Loss=-306.2921]\n",
      "Epoch 8/10: 100%|██████████| 21/21 [00:28<00:00,  1.37s/it, D_Loss=-462.6541, G_Loss=-334.1303]\n",
      "Epoch 9/10: 100%|██████████| 21/21 [00:27<00:00,  1.33s/it, D_Loss=-396.6316, G_Loss=-353.0944]\n",
      "Epoch 10/10: 100%|██████████| 21/21 [00:20<00:00,  1.04it/s, D_Loss=-327.0524, G_Loss=-381.8533]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.utils import softmax\n",
    "from rdkit import Chem\n",
    "from Bio import SeqIO\n",
    "import random\n",
    "import re\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import gzip\n",
    "\n",
    "# --- 1. System & Configuration ---\n",
    "\n",
    "# --- Configuration (UPDATE THESE PATHS & ID) ---\n",
    "CHEMPL_DB_PATH = 'DL_ENDSEM__DATASET/chembl_35/chembl_35_sqlite/chembl_35.db'\n",
    "BLAST_FASTA_PATH = 'DL_ENDSEM__DATASET/chembl_35_blast.fa.gz'\n",
    "TARGET_UNIPROT_ID = \"P00533\" # Example: EGFR Kinase\n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "Z_DIM = 100         # Latent noise dimension\n",
    "ATOM_FEAT_DIM = 9   # Atom feature size\n",
    "BOND_FEAT_DIM = 4   # Bond feature size (Single, Double, Triple, Aromatic)\n",
    "EMBED_DIM = 128     # Hidden dimension for the Graph Transformer\n",
    "T_EMBED_DIM = 1024  # Target embedding dimension (from ProtT5)\n",
    "LAMBDA_GP = 10.0    # Gradient Penalty weight\n",
    "MAX_NODES = 30      # Max atoms in generated molecules (for Generator tensor shape)\n",
    "N_CRITIC = 5        # Discriminator training steps per Generator step\n",
    "EPOCHS = 10         # Total epochs\n",
    "BATCH_SIZE = 64    # Your increased batch size\n",
    "\n",
    "# --- OPTIMIZATION 1: Set num_workers based on your 16-core CPU ---\n",
    "# Use ~half your cores to pre-fetch real data\n",
    "CPU_WORKERS = 4  \n",
    "\n",
    "# --- CUDA Check ---\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ CUDA is available! GPU will be used for training.\")\n",
    "    print(f\"PyTorch CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    print(\"❌ CUDA not found. Running on CPU.\")\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "# --- 2. Real Protein Embedding Generation (No changes) ---\n",
    "\n",
    "def load_target_sequence(fasta_path, uniprot_id):\n",
    "    \"\"\"Loads a protein sequence from a gzipped FASTA file.\"\"\"\n",
    "    print(f\"Opening gzipped FASTA file: {fasta_path}\")\n",
    "    try:\n",
    "        with gzip.open(fasta_path, \"rt\") as handle:\n",
    "            for record in SeqIO.parse(handle, \"fasta\"):\n",
    "                if uniprot_id in record.id or uniprot_id in record.description:\n",
    "                    return str(record.seq)\n",
    "        print(f\"Warning: Could not find sequence for {uniprot_id} in {fasta_path}\")\n",
    "        return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FATAL ERROR: FASTA file not found at {fasta_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL ERROR: Could not read FASTA file. Error: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_protein_embedding(sequence, device):\n",
    "    \"\"\"Generates a protein embedding using the pre-trained ProtT5 model.\"\"\"\n",
    "    print(\"Loading ProtT5 model... (This may take a moment)\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "    model = T5EncoderModel.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc').to(device)\n",
    "    model.eval() \n",
    "\n",
    "    sequence_preprocessed = \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))\n",
    "    inputs = tokenizer(sequence_preprocessed, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = model(**inputs).last_hidden_state\n",
    "    \n",
    "    protein_vec = embedding.mean(dim=1).squeeze(0)\n",
    "    print(f\"Generated protein embedding of shape: {protein_vec.shape}\")\n",
    "    return protein_vec\n",
    "\n",
    "# --- Generate the REAL Target Embedding ---\n",
    "target_seq = load_target_sequence(BLAST_FASTA_PATH, TARGET_UNIPROT_ID)\n",
    "if target_seq is None:\n",
    "    raise ValueError(f\"Target sequence for {TARGET_UNIPROT_ID} not found. Exiting.\")\n",
    "TARGET_EMBED = get_protein_embedding(target_seq, DEVICE)\n",
    "\n",
    "\n",
    "# --- 3. Data Pipeline (Molecules -> Graphs) (No changes) ---\n",
    "\n",
    "def extract_potent_inhibitors(db_path, uniprot_id, potency_cutoff_nM=100):\n",
    "    \"\"\"Connects to ChEMBL DB and extracts SMILES for potent inhibitors.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        sql_query = f\"\"\"\n",
    "        SELECT DISTINCT cs.canonical_smiles\n",
    "        FROM activities acts\n",
    "        JOIN assays a ON acts.assay_id = a.assay_id\n",
    "        JOIN target_dictionary td ON a.tid = td.tid\n",
    "        JOIN target_components tc ON td.tid = tc.tid\n",
    "        JOIN component_sequences cseq ON tc.component_id = cseq.component_id\n",
    "        JOIN compound_structures cs ON acts.molregno = cs.molregno\n",
    "        WHERE\n",
    "            cseq.accession = '{uniprot_id}' AND\n",
    "            acts.standard_type = 'IC50' AND\n",
    "            acts.standard_units = 'nM' AND\n",
    "            acts.standard_relation = '=' AND\n",
    "            acts.standard_value <= {potency_cutoff_nM}\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(sql_query, conn)\n",
    "        conn.close()\n",
    "        print(f\"Found {len(df)} potent inhibitors for UniProt ID {uniprot_id}.\")\n",
    "        return df['canonical_smiles'].unique().tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during database query: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_atom_features(atom):\n",
    "    \"\"\"Creates the 9-dimensional atom feature vector.\"\"\"\n",
    "    return [\n",
    "        atom.GetAtomicNum(), atom.GetDegree(), atom.GetFormalCharge(),\n",
    "        int(atom.GetIsAromatic()),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP2),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP3),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP3D),\n",
    "        int(atom.GetHybridization() == Chem.HybridizationType.SP3D2)\n",
    "    ]\n",
    "\n",
    "def smiles_to_graph(smiles, target_embed):\n",
    "    \"\"\"Converts SMILES to a PyG Data object.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol: return None\n",
    "    if mol.GetNumAtoms() > MAX_NODES: return None\n",
    "\n",
    "    x = torch.tensor([get_atom_features(atom) for atom in mol.GetAtoms()], dtype=torch.float)\n",
    "    \n",
    "    edge_indices, edge_attrs = [], []\n",
    "    bond_types = [Chem.BondType.SINGLE, Chem.BondType.DOUBLE, Chem.BondType.TRIPLE, Chem.BondType.AROMATIC]\n",
    "    \n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        bond_type_oh = [int(bond.GetBondType() == t) for t in bond_types]\n",
    "        edge_indices.extend([[i, j], [j, i]])\n",
    "        edge_attrs.extend([bond_type_oh, bond_type_oh])\n",
    "\n",
    "    if not edge_indices: return None\n",
    "    \n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "\n",
    "    # Note: We send to DEVICE in the data loader, not here.\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, \n",
    "                target_embed=target_embed.unsqueeze(0))\n",
    "    return data\n",
    "\n",
    "# --- Data Pipeline Execution ---\n",
    "inhibitor_smiles = extract_potent_inhibitors(CHEMPL_DB_PATH, TARGET_UNIPROT_ID)\n",
    "# Note: We keep data on CPU first, for num_workers to be efficient.\n",
    "real_data_list = [smiles_to_graph(s, TARGET_EMBED.cpu()) for s in inhibitor_smiles]\n",
    "real_data_list = [d for d in real_data_list if d is not None]\n",
    "\n",
    "if not real_data_list:\n",
    "    print(\"FATAL: No valid inhibitor data found. The model cannot be trained.\")\n",
    "    exit()\n",
    "\n",
    "# --- OPTIMIZATION 2: Added num_workers and pin_memory ---\n",
    "real_loader = DataLoader(\n",
    "    real_data_list, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=CPU_WORKERS, \n",
    "    pin_memory=True  # Speeds up CPU-to-GPU data transfer\n",
    ")\n",
    "print(f\"Prepared {len(real_data_list)} real graph samples for training.\")\n",
    "\n",
    "\n",
    "# --- 4. Model Architecture (No changes) ---\n",
    "\n",
    "# --- 4.1. Relational Graph Transformer Layer ---\n",
    "class RelationalGraphTransformerLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, edge_dim, heads=4, dropout=0.1, **kwargs):\n",
    "        super().__init__(aggr='add', node_dim=0, **kwargs)\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.lin_q = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_k = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_v = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_edge = nn.Linear(edge_dim, heads * out_channels, bias=False) \n",
    "        self.att_coeff = nn.Parameter(torch.Tensor(1, heads, 2 * out_channels)) \n",
    "        self.lin_out = nn.Linear(heads * out_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.xavier_uniform_(self.att_coeff)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        Q = self.lin_q(x).view(-1, self.heads, self.out_channels)\n",
    "        K = self.lin_k(x).view(-1, self.heads, self.out_channels)\n",
    "        V = self.lin_v(x).view(-1, self.heads, self.out_channels)\n",
    "        E = self.lin_edge(edge_attr).view(-1, self.heads, self.out_channels)\n",
    "        out = self.propagate(edge_index, Q=Q, K=K, V=V, E=E)\n",
    "        out = out.view(-1, self.heads * self.out_channels)\n",
    "        out = self.lin_out(out)\n",
    "        return out\n",
    "\n",
    "    def message(self, Q_i, K_j, V_j, E_k, index):\n",
    "        try: is_empty = (E_k.size(0) == 0)\n",
    "        except AttributeError: is_empty = True\n",
    "        \n",
    "        if is_empty: E_bias = torch.zeros(Q_i.size(0), self.heads, 1, device=self.lin_q.weight.device)\n",
    "        else: E_bias = E_k.mean(dim=-1, keepdim=True) \n",
    "\n",
    "        QK_cat = torch.cat([Q_i, K_j], dim=-1)\n",
    "        e_ij = (QK_cat * self.att_coeff).sum(dim=-1, keepdim=True)\n",
    "        e_ij = e_ij + E_bias\n",
    "        e_ij = F.leaky_relu(e_ij)\n",
    "        alpha = softmax(e_ij, index)\n",
    "        alpha = self.dropout(alpha)\n",
    "        return V_j * alpha.view(-1, self.heads, 1)\n",
    "\n",
    "# --- 4.2. Discriminator ---\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, node_features, edge_dim, t_embed_dim, embed_dim, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_d = node_features if i == 0 else embed_dim\n",
    "            self.layers.append(RelationalGraphTransformerLayer(in_d, embed_dim, edge_dim))\n",
    "        self.lin_final = nn.Linear(embed_dim + t_embed_dim, 1)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        t_embed = data.target_embed\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        graph_embed = global_mean_pool(x, batch)\n",
    "        if t_embed.dim() > 2: t_embed = t_embed.squeeze(1) \n",
    "        final_input = torch.cat([graph_embed, t_embed], dim=1)\n",
    "        return self.lin_final(final_input).squeeze(1)\n",
    "\n",
    "# --- 4.3. Generator ---\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, t_embed_dim, node_features, bond_features, max_nodes=MAX_NODES):\n",
    "        super().__init__()\n",
    "        self.max_nodes = max_nodes\n",
    "        self.node_features = node_features\n",
    "        self.bond_features = bond_features\n",
    "        self.lin_x = nn.Sequential(nn.Linear(z_dim + t_embed_dim, 256), nn.ReLU(), nn.Linear(256, max_nodes * node_features))\n",
    "        self.lin_adj = nn.Sequential(nn.Linear(z_dim + t_embed_dim, 256), nn.ReLU(), nn.Linear(256, max_nodes * max_nodes * bond_features))\n",
    "\n",
    "    def forward(self, z, t_embed):\n",
    "        zt = torch.cat([z, t_embed], dim=1)\n",
    "        x_fake = self.lin_x(zt).view(-1, self.max_nodes, self.node_features)\n",
    "        adj_fake_logits = self.lin_adj(zt).view(-1, self.max_nodes, self.max_nodes, self.bond_features)\n",
    "        return x_fake, adj_fake_logits\n",
    "\n",
    "# --- Model Initialization ---\n",
    "print(\"Initializing models...\")\n",
    "generator = Generator(Z_DIM, T_EMBED_DIM, ATOM_FEAT_DIM, BOND_FEAT_DIM).to(DEVICE)\n",
    "discriminator = Discriminator(ATOM_FEAT_DIM, BOND_FEAT_DIM, T_EMBED_DIM, EMBED_DIM).to(DEVICE)\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "\n",
    "# --- 5. Training Utilities (FIXED & VECTORIZED) ---\n",
    "\n",
    "# --- OPTIMIZATION 3: Create a helper for the dense edge_index template ---\n",
    "# We compute this once on CPU and move it to GPU,\n",
    "# so it's not recomputed every batch.\n",
    "N = MAX_NODES\n",
    "# Create a [N, N] adjacency matrix with 1s everywhere except the diagonal\n",
    "adj_template = (torch.ones(N, N) - torch.eye(N)).bool()\n",
    "# Convert to sparse edge_index format [2, N*(N-1)]\n",
    "EDGE_INDEX_TEMPLATE = adj_template.nonzero(as_tuple=False).t().contiguous().to(DEVICE)\n",
    "# This template has N*(N-1) edges\n",
    "NUM_DENSE_EDGES = EDGE_INDEX_TEMPLATE.size(1)\n",
    "\n",
    "# --- OPTIMIZATION 4: Vectorized Fake Graph Generation ---\n",
    "# This one function replaces both old convert_fake_to_data functions\n",
    "# It's fully vectorized and runs on the GPU. No Python loops!\n",
    "\n",
    "def convert_fake_to_data_vectorized(x_fake_tensor, adj_fake_logits, t_embed_batch, device, gumbel=False, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Converts Generator output to a single batched PyG Data object *on the GPU*.\n",
    "    This is the core optimization, replacing the slow Python loops.\n",
    "    \"\"\"\n",
    "    batch_size, num_nodes, _ = x_fake_tensor.shape\n",
    "    \n",
    "    # 1. Create Batched Node Features (x)\n",
    "    # Reshape [B, N, F] -> [B*N, F]\n",
    "    x_batched = x_fake_tensor.reshape(batch_size * num_nodes, -1)\n",
    "    \n",
    "    # 2. Create Batched Batch Index (batch)\n",
    "    # Create [0, 0, ..., 1, 1, ..., B-1, B-1]\n",
    "    batch_vec = torch.arange(batch_size, device=device).repeat_interleave(num_nodes)\n",
    "    \n",
    "    # 3. Create Batched Edge Index (edge_index)\n",
    "    # Repeat the [2, N*(N-1)] template B times\n",
    "    edge_index_batched = EDGE_INDEX_TEMPLATE.repeat(1, batch_size)\n",
    "    # Create offsets: [0, 0, ..., N, N, ..., 2N, 2N, ...]\n",
    "    offset = torch.arange(0, batch_size * num_nodes, num_nodes, device=device).repeat_interleave(NUM_DENSE_EDGES)\n",
    "    # Add offsets to create the full [2, B*N*(N-1)] edge_index\n",
    "    edge_index_batched = edge_index_batched + offset\n",
    "    \n",
    "    # 4. Create Batched Edge Attributes (edge_attr)\n",
    "    # We need to gather the logits corresponding to our new edge_index\n",
    "    # Get batch, row, and column indices from the batched edge_index\n",
    "    batch_indices = edge_index_batched[0] // num_nodes\n",
    "    row_indices = edge_index_batched[0] % num_nodes\n",
    "    col_indices = edge_index_batched[1] % num_nodes\n",
    "    \n",
    "    # Gather the [B*N*(N-1), Bonds] logits\n",
    "    adj_logits_flat = adj_fake_logits[batch_indices, row_indices, col_indices]\n",
    "    \n",
    "    # 5. Sample edge attributes\n",
    "    if gumbel:\n",
    "        # Differentiable Gumbel-Softmax for Generator step\n",
    "        edge_attr_batched = F.gumbel_softmax(adj_logits_flat, tau=temperature, hard=True)\n",
    "    else:\n",
    "        # Non-differentiable argmax for Discriminator step\n",
    "        bond_indices = torch.argmax(adj_logits_flat, dim=-1)\n",
    "        edge_attr_batched = F.one_hot(bond_indices, num_classes=BOND_FEAT_DIM).float()\n",
    "        \n",
    "    # 6. Create the single, batched Data object\n",
    "    fake_data = Data(\n",
    "        x=x_batched,\n",
    "        edge_index=edge_index_batched,\n",
    "        edge_attr=edge_attr_batched,\n",
    "        batch=batch_vec,\n",
    "        target_embed=t_embed_batch  # Already [B, T_EMBED_DIM]\n",
    "    )\n",
    "    return fake_data\n",
    "\n",
    "\n",
    "# --- 5.3. WGAN-GP Gradient Penalty (No changes) ---\n",
    "def calculate_gradient_penalty(discriminator, real_data, fake_data, lambda_gp, device):\n",
    "    \"\"\"Calculates the Gradient Penalty on interpolated node features (X).\"\"\"\n",
    "    real_x = real_data.x.detach()\n",
    "    fake_x = fake_data.x.detach()\n",
    "    real_x_size = real_x.size(0)\n",
    "    \n",
    "    if fake_x.size(0) > real_x_size:\n",
    "        fake_x = fake_x[:real_x_size]\n",
    "    elif fake_x.size(0) < real_x_size:\n",
    "        padding = torch.zeros(real_x_size - fake_x.size(0), fake_x.size(1), device=device)\n",
    "        fake_x = torch.cat([fake_x, padding], dim=0)\n",
    "\n",
    "    alpha = torch.rand(real_x_size, 1).to(device) \n",
    "    interpolated_x = (alpha * real_x) + ((1 - alpha) * fake_x)\n",
    "    interpolated_x.requires_grad_(True)\n",
    "\n",
    "    interpolated_data = Data(x=interpolated_x, \n",
    "                             edge_index=real_data.edge_index, \n",
    "                             edge_attr=real_data.edge_attr, \n",
    "                             batch=real_data.batch, \n",
    "                             target_embed=real_data.target_embed)\n",
    "\n",
    "    disc_interpolates = discriminator(interpolated_data)\n",
    "    \n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=disc_interpolates, inputs=interpolated_x,\n",
    "        grad_outputs=torch.ones_like(disc_interpolates),\n",
    "        create_graph=True, retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = lambda_gp * ((gradient_norm - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty\n",
    "\n",
    "# --- 6. Main Training Loop (FIXED & OPTIMIZED) ---\n",
    "def run_wgan_gp_training(generator, discriminator, data_loader, epochs, n_critic):\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        g_loss_sum, d_loss_sum = 0, 0\n",
    "        \n",
    "        # Use a for loop that automatically prints progress\n",
    "        from tqdm import tqdm\n",
    "        progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "        \n",
    "        for batch_idx, real_data in enumerate(progress_bar):\n",
    "            # OPTIMIZATION: Move real_data to GPU here\n",
    "            real_data = real_data.to(DEVICE)\n",
    "            batch_size = real_data.num_graphs\n",
    "            \n",
    "            # target_embed is already [B, T_EMBED_DIM] from the loader\n",
    "            target_embed_batch = real_data.target_embed\n",
    "\n",
    "            # 1. Train Discriminator (n_critic steps)\n",
    "            for _ in range(n_critic):\n",
    "                optimizer_D.zero_grad()\n",
    "                \n",
    "                # Real Loss\n",
    "                d_real = discriminator(real_data).mean()\n",
    "                \n",
    "                # Fake Loss\n",
    "                z = torch.randn(batch_size, Z_DIM).to(DEVICE)\n",
    "                x_fake, adj_fake_logits = generator(z, target_embed_batch)\n",
    "                \n",
    "                # --- D-step: Use VECTORIZED function (non-differentiable) ---\n",
    "                with torch.no_grad(): # Ensure no grads are computed here\n",
    "                    fake_data = convert_fake_to_data_vectorized(\n",
    "                        x_fake.detach(), adj_fake_logits.detach(), target_embed_batch, DEVICE, gumbel=False\n",
    "                    )\n",
    "                \n",
    "                d_fake = discriminator(fake_data).mean()\n",
    "                \n",
    "                # Gradient Penalty\n",
    "                gp = calculate_gradient_penalty(discriminator, real_data, fake_data, LAMBDA_GP, DEVICE)\n",
    "                \n",
    "                d_loss = - (d_real - d_fake) + gp\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "                d_loss_sum += d_loss.item()\n",
    "            \n",
    "            # 2. Train Generator (1 step)\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size, Z_DIM).to(DEVICE)\n",
    "            x_fake, adj_fake_logits = generator(z, target_embed_batch)\n",
    "            \n",
    "            # --- G-step: Use VECTORIZED function (DIFFERENTIABLE) ---\n",
    "            fake_data = convert_fake_to_data_vectorized(\n",
    "                x_fake, adj_fake_logits, target_embed_batch, DEVICE, gumbel=True\n",
    "            )\n",
    "            \n",
    "            # Generator Loss\n",
    "            g_loss = - discriminator(fake_data).mean()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            g_loss_sum += g_loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(\n",
    "                D_Loss=f\"{(d_loss_sum / (batch_idx+1) / n_critic):.4f}\", \n",
    "                G_Loss=f\"{(g_loss_sum / (batch_idx+1)):.4f}\"\n",
    "            )\n",
    "            \n",
    "        avg_d_loss = d_loss_sum / len(data_loader) / n_critic\n",
    "        avg_g_loss = g_loss_sum / len(data_loader)\n",
    "        # TQDM handles the epoch printout\n",
    "        # print(f\"Epoch {epoch}/{EPOCHS} | D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f}\")\n",
    "\n",
    "# --- Execute Training ---\n",
    "print(\"\\n--- Starting WGAN-GP Training ---\")\n",
    "# Add 'tqdm' to your environment: pip install tqdm\n",
    "run_wgan_gp_training(generator, discriminator, real_loader, EPOCHS, N_CRITIC) \n",
    "print(\"\\nTraining completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f6f301d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📊 DATASET SUMMARY\n",
      "============================================================\n",
      "✅ Valid molecules: 1334\n",
      "📦 Batch size: 64\n",
      "🔄 Batches per epoch: 21\n",
      "🧬 Atoms per molecule: 25.7 (avg)\n",
      "🔗 Bonds per molecule: 56.4 (avg)\n",
      "⚙️  Target protein: P00533\n",
      "🎯 Potency cutoff: ≤ 100 nM\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 📊 DATASET MONITORING\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📊 DATASET SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✅ Valid molecules: {len(real_data_list)}\")\n",
    "print(f\"📦 Batch size: {BATCH_SIZE}\")\n",
    "print(f\"🔄 Batches per epoch: {len(real_loader)}\")\n",
    "print(f\"🧬 Atoms per molecule: {sum(d.x.size(0) for d in real_data_list)/len(real_data_list):.1f} (avg)\")\n",
    "print(f\"🔗 Bonds per molecule: {sum(d.edge_index.size(1) for d in real_data_list)/len(real_data_list):.1f} (avg)\")\n",
    "print(f\"⚙️  Target protein: {TARGET_UNIPROT_ID}\")\n",
    "print(f\"🎯 Potency cutoff: ≤ 100 nM\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "# ============================================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
